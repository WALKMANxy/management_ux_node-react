{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openpyxl  # noqa: F401\n",
    "\n",
    "\n",
    "def is_unnamed(header):\n",
    "    return str(header).startswith(\"Unnamed\")\n",
    "\n",
    "\n",
    "def validate_first_sheet(df):\n",
    "    return (\n",
    "        is_unnamed(df.columns[1])\n",
    "        and is_unnamed(df.columns[2])\n",
    "        and is_unnamed(df.columns[3])\n",
    "        and is_unnamed(df.columns[4])\n",
    "        and is_unnamed(df.columns[5])\n",
    "        and not pd.isna(df.columns[0])\n",
    "        and not pd.isna(df.columns[6])\n",
    "    )\n",
    "\n",
    "\n",
    "def validate_other_sheet(df):\n",
    "    return (\n",
    "        df.shape[1] == 6\n",
    "        and not is_unnamed(df.columns[1])\n",
    "        and not is_unnamed(df.columns[2])\n",
    "        and not is_unnamed(df.columns[3])\n",
    "        and not is_unnamed(df.columns[4])\n",
    "        and is_unnamed(df.columns[0])\n",
    "    )\n",
    "\n",
    "\n",
    "def process_files(articles_file_path, output_file_path):\n",
    "    # Load the Excel file\n",
    "    xls = pd.ExcelFile(articles_file_path)\n",
    "\n",
    "    # Validate and load relevant sheets\n",
    "    relevant_sheets = []\n",
    "    first_sheet_validated = False\n",
    "\n",
    "    for sheet_name in xls.sheet_names:\n",
    "        df = pd.read_excel(xls, sheet_name=sheet_name, header=0)\n",
    "        if not first_sheet_validated:\n",
    "            if validate_first_sheet(df):\n",
    "                relevant_sheets.append((sheet_name, df))\n",
    "                first_sheet_validated = True\n",
    "            else:\n",
    "                raise ValueError(\"First sheet is not valid\")\n",
    "        else:\n",
    "            if validate_other_sheet(df):\n",
    "                relevant_sheets.append((sheet_name, df))\n",
    "\n",
    "    if not relevant_sheets:\n",
    "        raise ValueError(\"No relevant sheets found in the Excel file\")\n",
    "\n",
    "    # Process the first sheet\n",
    "    first_sheet_df = relevant_sheets[0][1].drop(columns=[\"mgs210\"])\n",
    "\n",
    "    # Align and merge other relevant sheets\n",
    "    aligned_sheets = [first_sheet_df]\n",
    "    for sheet_name, df in relevant_sheets[1:]:\n",
    "        df.columns = first_sheet_df.columns\n",
    "        aligned_sheets.append(df)\n",
    "\n",
    "    # Concatenate all aligned sheets\n",
    "    df_combined = pd.concat(aligned_sheets, ignore_index=True)\n",
    "\n",
    "    # Drop rows where the third column (index 2) is empty or \".\"\n",
    "    df_combined = df_combined[~df_combined.iloc[:, 2].isin([\"\", \".\"])]\n",
    "\n",
    "    # Drop the \"STAMPA LISTINI\" column\n",
    "    df_combined = df_combined.drop(columns=[\"STAMPA LISTINI\"])\n",
    "\n",
    "    # Print the number of columns for debugging\n",
    "    print(\"Number of columns in df_combined:\", len(df_combined.columns))\n",
    "    print(\"Column names in df_combined:\", df_combined.columns.tolist())\n",
    "\n",
    "    # Rename the columns\n",
    "    df_combined.columns = [\n",
    "        \"CODICE PRODOTTO\",\n",
    "        \"BRAND\",\n",
    "        \"DESCRIZIONE\",\n",
    "        \"GIACENZA\",\n",
    "        \"PRZ. ULT. ACQ.\",\n",
    "    ]\n",
    "\n",
    "    # Add new columns\n",
    "    df_combined[\"CODICE OE\"] = pd.NA\n",
    "    df_combined[\"CODICI CROSS\"] = pd.NA\n",
    "    df_combined[\"LINK IMMAGINE\"] = pd.NA\n",
    "    df_combined[\"CATEGORIA\"] = pd.NA\n",
    "    df_combined[\"SCHEDA TECNICA\"] = pd.NA\n",
    "    df_combined[\"SCHEDA DI SICUREZZA\"] = pd.NA\n",
    "    df_combined[\"CONFEZIONE\"] = pd.NA\n",
    "    df_combined[\"QUANTITÀ MINIMA\"] = pd.NA\n",
    "    df_combined[\"META.LUNGHEZZA\"] = pd.NA\n",
    "    df_combined[\"META.LARGHEZZA\"] = pd.NA\n",
    "    df_combined[\"META.PROFONDITA'\"] = pd.NA\n",
    "    df_combined[\"META. ...\"] = pd.NA\n",
    "\n",
    "    # Reorder the columns\n",
    "    df_combined = df_combined[\n",
    "        [\n",
    "            \"CODICE PRODOTTO\",\n",
    "            \"CODICE OE\",\n",
    "            \"CODICI CROSS\",\n",
    "            \"BRAND\",\n",
    "            \"DESCRIZIONE\",\n",
    "            \"LINK IMMAGINE\",\n",
    "            \"CATEGORIA\",\n",
    "            \"PRZ. ULT. ACQ.\",\n",
    "            \"GIACENZA\",\n",
    "            \"SCHEDA TECNICA\",\n",
    "            \"SCHEDA DI SICUREZZA\",\n",
    "            \"CONFEZIONE\",\n",
    "            \"QUANTITÀ MINIMA\",\n",
    "            \"META.LUNGHEZZA\",\n",
    "            \"META.LARGHEZZA\",\n",
    "            \"META.PROFONDITA'\",\n",
    "            \"META. ...\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    # Convert 'GIACENZA' and 'PRZ. ULT. ACQ.' columns to numeric types\n",
    "    df_combined[\"GIACENZA\"] = pd.to_numeric(\n",
    "        df_combined[\"GIACENZA\"].str.replace(\",\", \".\"), errors=\"coerce\"\n",
    "    )\n",
    "    df_combined[\"PRZ. ULT. ACQ.\"] = pd.to_numeric(\n",
    "        df_combined[\"PRZ. ULT. ACQ.\"].str.replace(\",\", \".\"), errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    # Clean the data\n",
    "    df_combined = df_combined[df_combined[\"GIACENZA\"] > 0]\n",
    "    df_combined = df_combined[df_combined[\"PRZ. ULT. ACQ.\"].notna()]\n",
    "\n",
    "    # Save the cleaned data to a CSV file\n",
    "    df_combined.to_csv(output_file_path, index=False)\n",
    "\n",
    "\n",
    "# Placeholders for file paths\n",
    "articles_file_path = \"Z:/My Drive/rcs/oem cross/Articles26062024.xls\"\n",
    "output_file_path = \"Z:/My Drive/rcs/oem cross/cleaned_Articles26062024.csv\"\n",
    "\n",
    "process_files(articles_file_path, output_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#MERGE OE CODES, BY APPENDING THEM#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def aggregate_oem_numbers(oems_df):\n",
    "    # Ensure 'oem_number' and 'article_alt' columns are treated as strings\n",
    "    oems_df[\"oem_number\"] = oems_df[\"oem_number\"].astype(str)\n",
    "    oems_df[\"article_alt\"] = oems_df[\"article_alt\"].astype(str)\n",
    "\n",
    "    # Aggregate OEM numbers for each article_alt\n",
    "    oems_agg = (\n",
    "        oems_df.groupby(\"article_alt\")[\"oem_number\"]\n",
    "        .apply(lambda x: \" | \".join(x))\n",
    "        .reset_index()\n",
    "    )\n",
    "    return oems_agg\n",
    "\n",
    "\n",
    "def append_oem_numbers(df_cleaned, oems_agg):\n",
    "    # Ensure 'CODICE PRODOTTO' is treated as a string\n",
    "    df_cleaned[\"CODICE PRODOTTO\"] = df_cleaned[\"CODICE PRODOTTO\"].astype(str)\n",
    "\n",
    "    # Strip leading/trailing spaces and convert to uppercase for both columns\n",
    "    df_cleaned[\"CODICE PRODOTTO\"] = (\n",
    "        df_cleaned[\"CODICE PRODOTTO\"].str.strip().str.upper()\n",
    "    )\n",
    "    oems_agg[\"article_alt\"] = oems_agg[\"article_alt\"].str.strip().str.upper()\n",
    "\n",
    "    # Merge the cleaned dataframe with the aggregated OEM numbers\n",
    "    merged_df = pd.merge(\n",
    "        df_cleaned,\n",
    "        oems_agg,\n",
    "        left_on=\"CODICE PRODOTTO\",\n",
    "        right_on=\"article_alt\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    # Rename the column for clarity\n",
    "    merged_df.rename(columns={\"oem_number\": \"CODICE OE\"}, inplace=True)\n",
    "\n",
    "    # Drop the article_alt column as it's no longer needed\n",
    "    merged_df.drop(columns=[\"article_alt\"], inplace=True)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def process_and_merge_files(cleaned_csv_path, oems_file_path, output_file_path):\n",
    "    # Load the cleaned articles CSV file\n",
    "    df_cleaned = pd.read_csv(cleaned_csv_path)\n",
    "\n",
    "    # Load the OEM CSV file with specified delimiter and as string type\n",
    "    oems_df = pd.read_csv(oems_file_path, delimiter=\";\", dtype=str)\n",
    "\n",
    "    # Aggregate the OEM numbers\n",
    "    oems_agg = aggregate_oem_numbers(oems_df)\n",
    "\n",
    "    # Append OEM numbers to the cleaned dataframe\n",
    "    df_result = append_oem_numbers(df_cleaned, oems_agg)\n",
    "\n",
    "    # Save the final dataframe to a CSV file\n",
    "    df_result.to_csv(output_file_path, index=False)\n",
    "\n",
    "    # Print some sample values for debugging\n",
    "    print(\"Sample CODICE PRODOTTO values:\")\n",
    "    print(df_cleaned[\"CODICE PRODOTTO\"].head(10))  # Print more rows for inspection\n",
    "    print(\"Sample article_alt values:\")\n",
    "    print(oems_agg[\"article_alt\"].head(10))  # Print more rows for inspection\n",
    "    print(\"Sample merged CODICE OE values:\")\n",
    "    print(\n",
    "        df_result[[\"CODICE PRODOTTO\", \"CODICE OE\"]].head(10)\n",
    "    )  # Print more rows for inspection\n",
    "\n",
    "    # Print unique values in both columns for further inspection\n",
    "    print(\"Unique CODICE PRODOTTO values:\")\n",
    "    print(df_cleaned[\"CODICE PRODOTTO\"].unique()[:10])  # Print more unique values\n",
    "    print(\"Unique article_alt values:\")\n",
    "    print(oems_agg[\"article_alt\"].unique()[:10])  # Print more unique values\n",
    "\n",
    "\n",
    "# Placeholders for file paths\n",
    "articles_file_path = \"Z:/My Drive/rcs/oem cross/cleaned_Articles26062024.csv\"\n",
    "oems_file_path = \"Z:/My Drive/rcs/oem cross/oems.csv\"\n",
    "output_file_path = \"Z:/My Drive/rcs/oem cross/OEMERGED_cleaned_Articles26062024.csv\"\n",
    "\n",
    "# Run the process\n",
    "process_and_merge_files(articles_file_path, oems_file_path, output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#SPLIT CSV FROM DT#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def split_csv(input_csv_path, output_folder, output_csv_prefix, rows_per_file=1048575):\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Count the total number of rows in the original CSV file\n",
    "    total_rows = (\n",
    "        sum(1 for row in open(input_csv_path, \"r\")) - 1\n",
    "    )  # Subtract 1 for header row\n",
    "\n",
    "    # Read the large CSV file in chunks\n",
    "    chunk_iter = pd.read_csv(\n",
    "        input_csv_path, chunksize=rows_per_file, delimiter=\";\", dtype=str\n",
    "    )\n",
    "\n",
    "    # Initialize file counter\n",
    "    file_number = 1\n",
    "    total_split_rows = 0\n",
    "\n",
    "    for chunk in tqdm(chunk_iter, desc=\"Processing chunks\"):\n",
    "        # Drop columns at indices 5, 6, 7, and 8\n",
    "        chunk.drop(chunk.columns[[0, 4, 5, 6, 7]], axis=1, inplace=True)\n",
    "\n",
    "        # Count the rows in the current chunk\n",
    "        chunk_rows = chunk.shape[0]\n",
    "        total_split_rows += chunk_rows\n",
    "\n",
    "        # Generate output file name with folder path\n",
    "        output_csv_path = os.path.join(\n",
    "            output_folder, f\"{output_csv_prefix}_{file_number}.csv\"\n",
    "        )\n",
    "\n",
    "        # Write chunk to a new CSV file\n",
    "        chunk.to_csv(output_csv_path, index=False)\n",
    "\n",
    "        # Increment file counter\n",
    "        file_number += 1\n",
    "\n",
    "    # Compare the total rows\n",
    "    comparison_result = \"Match\" if total_rows == total_split_rows else \"Mismatch\"\n",
    "\n",
    "    # Write the results to a text file\n",
    "    with open(os.path.join(output_folder, \"row_count_comparison.txt\"), \"w\") as f:\n",
    "        f.write(f\"Total rows in original CSV: {total_rows}\\n\")\n",
    "        f.write(f\"Total rows in split CSVs: {total_split_rows}\\n\")\n",
    "        f.write(f\"Comparison result: {comparison_result}\\n\")\n",
    "\n",
    "\n",
    "# Placeholders for file paths\n",
    "input_csv_path = \"Z:/My Drive/rcs/oem cross/oems.csv\"\n",
    "output_folder = \"Z:/My Drive/rcs/oem cross/oemssplit/\"\n",
    "output_csv_prefix = \"oemsDT\"\n",
    "\n",
    "# Run the split function\n",
    "split_csv(input_csv_path, output_folder, output_csv_prefix, rows_per_file=1048575)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#SPLIT CSV FROM DC, BUT BRANDS UNIFIED WITH PRODUCT IDS#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def split_csv(\n",
    "    input_csv_path,\n",
    "    output_folder,\n",
    "    output_csv_prefix,\n",
    "    rows_per_file=1048575,\n",
    "    encoding=\"utf-8\",\n",
    "):\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Count the total number of rows in the original CSV file\n",
    "    with open(input_csv_path, \"r\", encoding=encoding) as f:\n",
    "        total_rows = sum(1 for row in f) - 1  # Subtract 1 for header row\n",
    "\n",
    "    # Read the large CSV file in chunks\n",
    "    chunk_iter = pd.read_csv(\n",
    "        input_csv_path,\n",
    "        chunksize=rows_per_file,\n",
    "        delimiter=\";\",\n",
    "        dtype=str,\n",
    "        encoding=encoding,\n",
    "    )\n",
    "\n",
    "    # Initialize file counter\n",
    "    file_number = 1\n",
    "    total_split_rows = 0\n",
    "\n",
    "    for chunk in tqdm(chunk_iter, desc=\"Processing chunks\"):\n",
    "        # Drop columns at indices 5, 6, 7, and 8\n",
    "        chunk.drop(chunk.columns[[0, 4, 5, 6, 7]], axis=1, inplace=True)\n",
    "\n",
    "        # Count the rows in the current chunk\n",
    "        chunk_rows = chunk.shape[0]\n",
    "        total_split_rows += chunk_rows\n",
    "\n",
    "        # Generate output file name with folder path\n",
    "        output_csv_path = os.path.join(\n",
    "            output_folder, f\"{output_csv_prefix}_{file_number}.csv\"\n",
    "        )\n",
    "\n",
    "        # Write chunk to a new CSV file\n",
    "        chunk.to_csv(output_csv_path, index=False)\n",
    "\n",
    "        # Increment file counter\n",
    "        file_number += 1\n",
    "\n",
    "    # Compare the total rows\n",
    "    comparison_result = \"Match\" if total_rows == total_split_rows else \"Mismatch\"\n",
    "\n",
    "    # Write the results to a text file\n",
    "    with open(os.path.join(output_folder, \"row_count_comparison.txt\"), \"w\") as f:\n",
    "        f.write(f\"Total rows in original CSV: {total_rows}\\n\")\n",
    "        f.write(f\"Total rows in split CSVs: {total_split_rows}\\n\")\n",
    "        f.write(f\"Comparison result: {comparison_result}\\n\")\n",
    "\n",
    "\n",
    "# Placeholders for file paths\n",
    "input_csv_path = \"Z:/My Drive/rcs/oem cross/oems_id.csv\"\n",
    "output_folder = \"Z:/My Drive/rcs/oem cross/oemssplit/\"\n",
    "output_csv_prefix = \"oemsDC\"\n",
    "\n",
    "# Run the split function\n",
    "split_csv(\n",
    "    input_csv_path,\n",
    "    output_folder,\n",
    "    output_csv_prefix,\n",
    "    rows_per_file=1048575,\n",
    "    encoding=\"utf-8\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#IDENTIFY BRANDS WITH MULTIPLE SPACES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def identify_brands_with_spaces(\n",
    "    input_folder, output_folder, brands_with_spaces_file, chunksize=100000\n",
    "):\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Initialize list for brands with spaces\n",
    "    brands_with_spaces = []\n",
    "\n",
    "    # List all CSV files in the input folder\n",
    "    csv_files = [file for file in os.listdir(input_folder) if file.endswith(\".csv\")]\n",
    "\n",
    "    # Iterate over all files in the input folder\n",
    "    for file_name in tqdm(csv_files, desc=\"Identifying brands with spaces\"):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "\n",
    "        # Process the file in chunks\n",
    "        for chunk in pd.read_csv(file_path, dtype=str, chunksize=chunksize):\n",
    "            # Ensure all values in the first column are strings\n",
    "            chunk.iloc[:, 0] = chunk.iloc[:, 0].astype(str)\n",
    "\n",
    "            # Strip leading/trailing spaces and identify rows with multiple spaces\n",
    "            chunk[\"cleaned\"] = chunk.iloc[:, 0].str.strip()\n",
    "            mask = chunk[\"cleaned\"].str.count(\" \") > 1\n",
    "            brands_with_spaces.extend(chunk[mask].values)\n",
    "\n",
    "    # Save brands with spaces to a separate CSV file\n",
    "    brands_with_spaces_df = pd.DataFrame(brands_with_spaces, columns=chunk.columns)\n",
    "    brands_with_spaces_df.to_csv(\n",
    "        os.path.join(output_folder, brands_with_spaces_file), index=False\n",
    "    )\n",
    "\n",
    "    print(f\"Identified and saved brands with spaces to {brands_with_spaces_file}\")\n",
    "\n",
    "\n",
    "# Placeholders for file paths\n",
    "input_folder = \"Z:/My Drive/rcs/oem cross/oemssplit/\"\n",
    "output_folder = \"Z:/My Drive/rcs/oem cross/oemssplit/\"\n",
    "brands_with_spaces_file = \"brandswithspaces.csv\"\n",
    "\n",
    "# Identify brands with spaces and save them to a separate file\n",
    "identify_brands_with_spaces(input_folder, output_folder, brands_with_spaces_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#IDENTIFY AND FIX BRANDS WITH SPACES#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def apply_custom_rules(brand_product):\n",
    "    if brand_product.startswith(\"SKF\"):\n",
    "        parts = brand_product.split(\" \", 2)\n",
    "        return f\"{parts[0]} {parts[1]}{parts[2].replace(' ', '', 1)}\"\n",
    "\n",
    "    if brand_product.startswith(\"DIESEL POWER\"):\n",
    "        return brand_product.replace(\" \", \"-\", 1)\n",
    "\n",
    "    if brand_product.startswith(\"OPEN PARTS\"):\n",
    "        return brand_product.replace(\" \", \"-\", 1)\n",
    "\n",
    "    if brand_product.startswith(\"DT MT \"):\n",
    "        return brand_product.replace(\" \", \"-\", 1)\n",
    "\n",
    "    if brand_product.startswith(\"INCAR \"):\n",
    "        parts = brand_product.split(\" \", 2)\n",
    "        return f\"{parts[0]} {parts[1]}{parts[2].replace(' ', '', 1)}\"\n",
    "\n",
    "    if brand_product.startswith(\"MERITOR\"):\n",
    "        parts = brand_product.split(\" \", 3)\n",
    "        return (\n",
    "            f\"{parts[0]} {parts[1]}{parts[2].replace(' ', '', 1).replace(' ', '', 1)}\"\n",
    "        )\n",
    "\n",
    "    if brand_product.startswith(\"OE GERMANY\"):\n",
    "        parts = brand_product.split(\" \", 2)\n",
    "        return f\"{parts[0]} {parts[1]}{parts[2].replace(' ', '', 1)}\"\n",
    "\n",
    "    if brand_product.startswith(\"RCS\"):\n",
    "        parts = brand_product.split(\" \")\n",
    "        if len(parts) == 2:\n",
    "            return f\"{parts[0]} {parts[1]}\"\n",
    "        elif len(parts) == 3:\n",
    "            return f\"{parts[0]} {parts[1]}{parts[2].replace(' ', '', 1)}\"\n",
    "        elif len(parts) >= 4:\n",
    "            modified_part = parts[2].replace(\" \", \"\", 1).replace(\" \", \"\", 1)\n",
    "            combined_part = f\"{parts[1]}{modified_part}{parts[3]}\"\n",
    "            return f\"{parts[0]} {combined_part} {' '.join(parts[4:])}\"\n",
    "\n",
    "    if brand_product.startswith(\"SEM LASTIK\"):\n",
    "        return brand_product.replace(\" \", \"-\", 1)\n",
    "\n",
    "    if brand_product.startswith(\"XXL TRUCK\"):\n",
    "        return brand_product.replace(\" \", \"-\", 1)\n",
    "\n",
    "    if brand_product.startswith(\"XXL  \"):\n",
    "        return brand_product.replace(\" \", \"\", 1)\n",
    "\n",
    "    return brand_product\n",
    "\n",
    "\n",
    "def identify_and_apply_rules(\n",
    "    input_folder, output_folder, brands_with_spaces_file, chunksize=100000\n",
    "):\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Initialize list for brands with spaces\n",
    "    brands_with_spaces = []\n",
    "\n",
    "    # List all CSV files in the input folder\n",
    "    csv_files = [file for file in os.listdir(input_folder) if file.endswith(\".csv\")]\n",
    "\n",
    "    # Iterate over all files in the input folder\n",
    "    for file_name in tqdm(csv_files, desc=\"Processing files\"):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "\n",
    "        # Process the file in chunks\n",
    "        for chunk in pd.read_csv(file_path, dtype=str, chunksize=chunksize):\n",
    "            # Ensure all values in the first column are strings\n",
    "            chunk.iloc[:, 0] = chunk.iloc[:, 0].astype(str)\n",
    "\n",
    "            # Strip leading/trailing spaces\n",
    "            chunk.iloc[:, 0] = chunk.iloc[:, 0].str.strip()\n",
    "\n",
    "            # Drop rows that start with \"DT CAT\" or \"DT SA-FLYER EN\"\n",
    "            chunk = chunk[\n",
    "                ~chunk.iloc[:, 0].str.startswith((\"DT CAT\", \"DT SA-FLYER EN\"))\n",
    "            ]\n",
    "\n",
    "            # Identify rows with multiple spaces\n",
    "            mask = chunk.iloc[:, 0].str.count(\" \") > 1\n",
    "\n",
    "            # Apply custom rules to the rows identified\n",
    "            for index in chunk[mask].index:\n",
    "                brand_product = chunk.at[index, chunk.columns[0]]\n",
    "                modified_brand_product = apply_custom_rules(brand_product)\n",
    "                chunk.at[index, chunk.columns[0]] = modified_brand_product\n",
    "\n",
    "            brands_with_spaces.extend(chunk[mask].values)\n",
    "\n",
    "    # Save brands with spaces to a separate CSV file\n",
    "    brands_with_spaces_df = pd.DataFrame(brands_with_spaces, columns=chunk.columns)\n",
    "    brands_with_spaces_df.to_csv(\n",
    "        os.path.join(output_folder, brands_with_spaces_file), index=False\n",
    "    )\n",
    "\n",
    "    print(f\"Processed and saved brands with spaces to {brands_with_spaces_file}\")\n",
    "\n",
    "\n",
    "# Placeholders for file paths\n",
    "input_folder = \"Z:/My Drive/rcs/oem cross/oemssplit/\"\n",
    "output_folder = \"Z:/My Drive/rcs/oem cross/oemssplit/\"\n",
    "brands_with_spaces_file = \"brandswithspaces.csv\"\n",
    "\n",
    "# Identify brands with spaces, apply rules, and save them to a separate file\n",
    "identify_and_apply_rules(input_folder, output_folder, brands_with_spaces_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DIVIDE IDS AND BRANDS#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def apply_custom_rules(brand_product):\n",
    "    if brand_product.startswith(\"SKF\"):\n",
    "        parts = brand_product.split(\" \", 2)\n",
    "        return f\"{parts[0]} {parts[1]}{parts[2].replace(' ', '', 1)}\"\n",
    "\n",
    "    if brand_product.startswith(\"DIESEL POWER\"):\n",
    "        return brand_product.replace(\" \", \"-\", 1)\n",
    "\n",
    "    if brand_product.startswith(\"OPEN PARTS\"):\n",
    "        return brand_product.replace(\" \", \"-\", 1)\n",
    "\n",
    "    if brand_product.startswith(\"DT MT \"):\n",
    "        return brand_product.replace(\" \", \"-\", 1)\n",
    "\n",
    "    if brand_product.startswith(\"INCAR \"):\n",
    "        parts = brand_product.split(\" \", 2)\n",
    "        return f\"{parts[0]} {parts[1]}{parts[2].replace(' ', '', 1)}\"\n",
    "\n",
    "    if brand_product.startswith(\"MERITOR\"):\n",
    "        parts = brand_product.split(\" \", 3)\n",
    "        return (\n",
    "            f\"{parts[0]} {parts[1]}{parts[2].replace(' ', '', 1).replace(' ', '', 1)}\"\n",
    "        )\n",
    "\n",
    "    if brand_product.startswith(\"OE GERMANY\"):\n",
    "        parts = brand_product.split(\" \", 2)\n",
    "        return f\"{parts[0]} {parts[1]}{parts[2].replace(' ', '', 1)}\"\n",
    "\n",
    "    if brand_product.startswith(\"RCS\"):\n",
    "        parts = brand_product.split(\" \")\n",
    "        if len(parts) == 2:\n",
    "            return f\"{parts[0]} {parts[1]}\"\n",
    "        elif len(parts) == 3:\n",
    "            return f\"{parts[0]} {parts[1]}{parts[2].replace(' ', '', 1)}\"\n",
    "        elif len(parts) >= 4:\n",
    "            modified_part = parts[2].replace(\" \", \"\", 1).replace(\" \", \"\", 1)\n",
    "            combined_part = f\"{parts[1]}{modified_part}{parts[3]}\"\n",
    "            return f\"{parts[0]} {combined_part} {' '.join(parts[4:])}\"\n",
    "\n",
    "    if brand_product.startswith(\"SEM LASTIK\"):\n",
    "        return brand_product.replace(\" \", \"-\", 1)\n",
    "\n",
    "    if brand_product.startswith(\"XXL TRUCK\"):\n",
    "        return brand_product.replace(\" \", \"-\", 1)\n",
    "\n",
    "    if brand_product.startswith(\"XXL  \"):\n",
    "        return brand_product.replace(\" \", \"\", 1)\n",
    "\n",
    "    return brand_product\n",
    "\n",
    "\n",
    "def process_csv_files(input_folder, output_folder, chunksize=100000):\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # List all CSV files in the input folder\n",
    "    csv_files = [file for file in os.listdir(input_folder) if file.endswith(\".csv\")]\n",
    "\n",
    "    # Iterate over all files in the input folder\n",
    "    for file_name in tqdm(csv_files, desc=\"Processing files\"):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        output_file_path = os.path.join(output_folder, file_name)\n",
    "\n",
    "        processed_chunks = []\n",
    "\n",
    "        # Process the file in chunks\n",
    "        for chunk in pd.read_csv(file_path, dtype=str, chunksize=chunksize):\n",
    "            # Ensure all values in the first column are strings\n",
    "            chunk.iloc[:, 0] = chunk.iloc[:, 0].astype(str)\n",
    "\n",
    "            # Strip leading/trailing spaces\n",
    "            chunk.iloc[:, 0] = chunk.iloc[:, 0].str.strip()\n",
    "\n",
    "            # Drop rows that start with \"DT CAT\" or \"DT SA-FLYER EN\"\n",
    "            chunk = chunk[\n",
    "                ~chunk.iloc[:, 0].str.startswith((\"DT CAT\", \"DT SA-FLYER EN\"))\n",
    "            ]\n",
    "\n",
    "            # Identify rows with multiple spaces\n",
    "            mask = chunk.iloc[:, 0].str.count(\" \") > 1\n",
    "\n",
    "            # Apply custom rules to the rows identified\n",
    "            for index in chunk[mask].index:\n",
    "                brand_product = chunk.at[index, chunk.columns[0]]\n",
    "                modified_brand_product = apply_custom_rules(brand_product)\n",
    "                chunk.at[index, chunk.columns[0]] = modified_brand_product\n",
    "\n",
    "            # Separate brand and product ID into new columns\n",
    "            chunk[\"article_alt_brands\"] = chunk.iloc[:, 0].str.extract(r\"(^[^\\s]+)\")\n",
    "            chunk[\"article_alt\"] = chunk.iloc[:, 0].str.extract(r\"^[^\\s]+\\s+(.+)\")\n",
    "            chunk[\"article_altc\"] = chunk[\"article_alt\"]\n",
    "            chunk = chunk.drop(columns=[\"article_alt\"])\n",
    "\n",
    "            # Remove any spaces from the oem_number column\n",
    "            chunk[\"oem_number\"] = chunk[\"oem_number\"].str.replace(\" \", \"\")\n",
    "\n",
    "            # Reorder columns to the required order\n",
    "            chunk = chunk[\n",
    "                [\"article_altc\", \"article_alt_brands\", \"manufacturer\", \"oem_number\"]\n",
    "            ]\n",
    "\n",
    "            processed_chunks.append(chunk)\n",
    "\n",
    "        # Concatenate all processed chunks and save to a new CSV file\n",
    "        processed_df = pd.concat(processed_chunks)\n",
    "        processed_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(f\"Processed and saved files to {output_folder}\")\n",
    "\n",
    "\n",
    "# Placeholders for file paths\n",
    "input_folder = \"Z:/My Drive/rcs/oem cross/oemssplit/\"\n",
    "output_folder = \"Z:/My Drive/rcs/oem cross/oemssplit/processed/\"\n",
    "\n",
    "# Process all CSV files to separate brands and product IDs\n",
    "process_csv_files(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#FIX BRANDS NAMING#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define file paths\n",
    "oems_folder = \"Z:/My Drive/rcs/oem cross/oemssplit/processed/\"\n",
    "output_folder = \"Z:/My Drive/rcs/oem cross/oemssplit/processed/tulero/\"\n",
    "output_file = os.path.join(output_folder, \"unique_brands.csv\")\n",
    "\n",
    "# Ensure the output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Initialize a DataFrame to hold all OEM brands\n",
    "all_oem_brands = pd.DataFrame(columns=[\"article_alt_brands\"])\n",
    "\n",
    "# List all OEM files in the processed folder\n",
    "oems_files = [file for file in os.listdir(oems_folder) if file.endswith(\".csv\")]\n",
    "\n",
    "# Iterate over all OEM files and collect brands\n",
    "for file_name in tqdm(oems_files, desc=\"Collecting OEM brands\"):\n",
    "    file_path = os.path.join(oems_folder, file_name)\n",
    "    oems_df = pd.read_csv(file_path, dtype=str)\n",
    "    if \"article_alt_brands\" in oems_df.columns:\n",
    "        all_oem_brands = pd.concat([all_oem_brands, oems_df[[\"article_alt_brands\"]]])\n",
    "\n",
    "# Drop duplicates to ensure unique brands\n",
    "unique_brands = all_oem_brands.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Save the result to a CSV file\n",
    "unique_brands.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Unique brands saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "brands_to_check_file = (\n",
    "    \"Z:/My Drive/rcs/oem cross/oemssplit/processed/tulero/articlesbrands.csv\"\n",
    ")\n",
    "existing_brands_file = (\n",
    "    \"Z:/My Drive/rcs/oem cross/oemssplit/processed/tulero/unique_brands.csv\"\n",
    ")\n",
    "output_file = (\n",
    "    \"Z:/My Drive/rcs/oem cross/oemssplit/processed/tulero/unique_brands_result.csv\"\n",
    ")\n",
    "\n",
    "# Load the CSV files\n",
    "brands_to_check_df = pd.read_csv(brands_to_check_file, dtype=str)\n",
    "existing_brands_df = pd.read_csv(existing_brands_file, dtype=str)\n",
    "\n",
    "# Ensure the brand columns are stripped of leading/trailing spaces\n",
    "brands_to_check_df[\"Brand\"] = brands_to_check_df[\"Brand\"].str.strip()\n",
    "existing_brands_df[\"Brands\"] = existing_brands_df[\"Brands\"].str.strip()\n",
    "\n",
    "# Create a set of existing brands for fast lookup\n",
    "existing_brands_set = set(existing_brands_df[\"Brands\"])\n",
    "\n",
    "# Check for matches and add a new column with the matched brand or \"No Match\"\n",
    "brands_to_check_df[\"Match\"] = brands_to_check_df[\"Brand\"].apply(\n",
    "    lambda x: x if x in existing_brands_set else \"No Match\"\n",
    ")\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "brands_to_check_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Output with matches saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DO THE THING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define file paths\n",
    "cleaned_articles_file = \"Z:/My Drive/rcs/oem cross/cleaned_Articles26062024.csv\"\n",
    "old_oems_folder = \"Z:/My Drive/rcs/oem cross/oemssplit/processed/\"\n",
    "output_folder = \"Z:/My Drive/rcs/oem cross/oemssplit/processed/tulero/\"\n",
    "log_file = os.path.join(output_folder, \"match_log.txt\")\n",
    "\n",
    "# Ensure the output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Load the cleaned articles file\n",
    "cleaned_df = pd.read_csv(cleaned_articles_file, dtype=str)\n",
    "\n",
    "# Ensure all values in the relevant columns are strings and strip any leading/trailing spaces\n",
    "cleaned_df[\"CODICE PRODOTTO\"] = cleaned_df[\"CODICE PRODOTTO\"].astype(str).str.strip()\n",
    "cleaned_df[\"CODICE OE\"] = cleaned_df[\"CODICE OE\"].astype(str).str.strip()\n",
    "cleaned_df[\"BRAND\"] = cleaned_df[\"BRAND\"].astype(str).str.strip()\n",
    "\n",
    "# List all old CSV files in the processed folder\n",
    "old_oems_files = [\n",
    "    file\n",
    "    for file in os.listdir(old_oems_folder)\n",
    "    if file.startswith(\"oemsDC\") and file.endswith(\".csv\")\n",
    "]\n",
    "\n",
    "# Initialize a DataFrame to hold all OEM mappings\n",
    "all_oem_mappings = pd.DataFrame()\n",
    "\n",
    "# Iterate over all old oems files and build the oem_number mappings\n",
    "for file_name in tqdm(old_oems_files, desc=\"Building old OEM mappings\"):\n",
    "    file_path = os.path.join(old_oems_folder, file_name)\n",
    "    oems_df = pd.read_csv(file_path, dtype=str)\n",
    "    oems_df[\"article_altc\"] = oems_df[\"article_altc\"].astype(str).str.strip()\n",
    "    oems_df[\"oem_number\"] = (\n",
    "        oems_df[\"oem_number\"].astype(str).str.strip().str.replace(\" \", \"\")\n",
    "    )\n",
    "    oems_df[\"article_alt_brands\"] = (\n",
    "        oems_df[\"article_alt_brands\"].astype(str).str.strip()\n",
    "    )\n",
    "    oems_df[\"brand_prefix\"] = oems_df[\"article_alt_brands\"].str[:5]\n",
    "    all_oem_mappings = pd.concat(\n",
    "        [all_oem_mappings, oems_df[[\"article_altc\", \"oem_number\", \"brand_prefix\"]]]\n",
    "    )\n",
    "\n",
    "# Create a lookup dictionary\n",
    "oem_lookup = (\n",
    "    all_oem_mappings.groupby([\"article_altc\", \"brand_prefix\"])[\"oem_number\"]\n",
    "    .apply(list)\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "\n",
    "# Function to get OEM number\n",
    "def get_oem_number(row):\n",
    "    key = (row[\"CODICE PRODOTTO\"], row[\"BRAND\"][:5])\n",
    "    return \" | \".join(oem_lookup[key]) if key in oem_lookup else \"Unknown OE\"\n",
    "\n",
    "\n",
    "# Update the CODICE OE column in the cleaned articles file\n",
    "tqdm.pandas(desc=\"Updating CODICE OE with old OEMs\")\n",
    "cleaned_df[\"CODICE OE\"] = cleaned_df.progress_apply(get_oem_number, axis=1)\n",
    "\n",
    "# Increase the values in \"PRZ. ULT. ACQ.\" by 25% and rename the column to \"PREZZO\"\n",
    "cleaned_df[\"PRZ. ULT. ACQ.\"] = pd.to_numeric(\n",
    "    cleaned_df[\"PRZ. ULT. ACQ.\"], errors=\"coerce\"\n",
    ")\n",
    "cleaned_df[\"PREZZO\"] = cleaned_df[\"PRZ. ULT. ACQ.\"].apply(\n",
    "    lambda x: round(x * 1.25, 2) if pd.notnull(x) else x\n",
    ")\n",
    "cleaned_df = cleaned_df.drop(columns=[\"PRZ. ULT. ACQ.\"])\n",
    "\n",
    "# Reorder the columns so that PREZZO is between GIACENZA and SCHEDA TECNICA\n",
    "columns_order = [\n",
    "    \"CODICE PRODOTTO\",\n",
    "    \"CODICE OE\",\n",
    "    \"CODICI CROSS\",\n",
    "    \"BRAND\",\n",
    "    \"DESCRIZIONE\",\n",
    "    \"LINK IMMAGINE\",\n",
    "    \"CATEGORIA\",\n",
    "    \"GIACENZA\",\n",
    "    \"PREZZO\",\n",
    "    \"SCHEDA TECNICA\",\n",
    "    \"SCHEDA DI SICUREZZA\",\n",
    "    \"CONFEZIONE\",\n",
    "    \"QUANTITÀ MINIMA\",\n",
    "    \"META.LUNGHEZZA\",\n",
    "    \"META.LARGHEZZA\",\n",
    "    \"META.PROFONDITA'\",\n",
    "    \"META. ...\",\n",
    "]\n",
    "cleaned_df = cleaned_df[columns_order]\n",
    "\n",
    "# Initialize the CODICI CROSS column\n",
    "cleaned_df[\"CODICI CROSS\"] = \"\"\n",
    "\n",
    "# Create a dictionary to hold cross references, excluding \"Unknown OE\"\n",
    "cross_references = (\n",
    "    cleaned_df[cleaned_df[\"CODICE OE\"] != \"Unknown OE\"]\n",
    "    .groupby(\"CODICE OE\")[\"CODICE PRODOTTO\"]\n",
    "    .apply(list)\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# Create a Series to store cross codes\n",
    "cross_codes_series = pd.Series(index=cleaned_df.index, dtype=str)\n",
    "\n",
    "# Apply the function to update the CODICI CROSS column, ignoring \"Unknown OE\"\n",
    "for codice_oe, prodotti in tqdm(cross_references.items(), desc=\"Updating CODICI CROSS\"):\n",
    "    cross_codes = {\n",
    "        prodotto: \" | \".join([code for code in prodotti if code != prodotto])\n",
    "        for prodotto in prodotti\n",
    "    }\n",
    "    cross_codes_series.update(pd.Series(cross_codes))\n",
    "\n",
    "cleaned_df[\"CODICI CROSS\"] = cleaned_df.index.map(cross_codes_series).fillna(\"\")\n",
    "\n",
    "# Precompute padded_oe column\n",
    "cleaned_df[\"padded_oe\"] = \" \" + cleaned_df[\"CODICE OE\"].str.strip() + \" \"\n",
    "\n",
    "\n",
    "# Handle Unknown OE: Check if CODICE PRODOTTO is present in other CODICE OE\n",
    "def find_additional_cross_codes(codice_prodotto, padded_oe):\n",
    "    matches = cleaned_df[cleaned_df[\"CODICE OE\"] != \"Unknown OE\"]\n",
    "    exact_matches = matches[\n",
    "        matches[\"padded_oe\"].str.contains(f\" {codice_prodotto} \", regex=False)\n",
    "    ]\n",
    "    if not exact_matches.empty:\n",
    "        return \" | \".join(exact_matches[\"CODICE PRODOTTO\"].unique())\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "# Apply the function to update the CODICI CROSS column for Unknown OE\n",
    "unknown_oe_mask = cleaned_df[\"CODICE OE\"] == \"Unknown OE\"\n",
    "cleaned_df.loc[unknown_oe_mask, \"CODICI CROSS\"] = cleaned_df.loc[\n",
    "    unknown_oe_mask, \"CODICE PRODOTTO\"\n",
    "].progress_apply(\n",
    "    lambda codice_prodotto: find_additional_cross_codes(\n",
    "        codice_prodotto, cleaned_df[\"padded_oe\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Save the updated cleaned articles file\n",
    "output_file_path = os.path.join(output_folder, \"tulerodataset26062024.csv\")\n",
    "cleaned_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Write the match statistics to a log file\n",
    "successful_matches = cleaned_df[\"CODICE OE\"].apply(lambda x: x != \"Unknown OE\").sum()\n",
    "unsuccessful_matches = cleaned_df[\"CODICE OE\"].apply(lambda x: x == \"Unknown OE\").sum()\n",
    "\n",
    "with open(log_file, \"w\") as log:\n",
    "    log.write(f\"Total successful matches: {successful_matches}\\n\")\n",
    "    log.write(f\"Total unsuccessful matches: {unsuccessful_matches}\\n\")\n",
    "\n",
    "print(f\"Updated CODICE OE and PREZZO in {output_file_path}\")\n",
    "print(f\"Match statistics written to {log_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#SAMPLE FILE#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample file with unique BRAND rows saved to Z:/My Drive/rcs/Web/Tulero/output/sample_final_output_with_brands.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define file paths\n",
    "input_file_path = \"Z:/My Drive/rcs/Web/Tulero/output/final_output_with_brands.csv\"\n",
    "output_file_path = (\n",
    "    \"Z:/My Drive/rcs/Web/Tulero/output/sample_final_output_with_brands.csv\"\n",
    ")\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(input_file_path, dtype=str)\n",
    "\n",
    "# Keep only the first occurrence of each unique BRAND\n",
    "df_unique_brands = df.drop_duplicates(subset=[\"BRAND\"], keep=\"first\")\n",
    "\n",
    "# Save the resulting dataframe to a new CSV file\n",
    "df_unique_brands.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Sample file with unique BRAND rows saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating sheets: 100%|██████████| 5/5 [00:00<00:00,  9.73it/s]\n",
      "Building old OEM mappings: 100%|██████████| 4/4 [00:02<00:00,  1.53it/s]\n",
      "Updating CODICE OE with old OEMs: 100%|██████████| 30376/30376 [00:00<00:00, 252486.45it/s]\n",
      "Updating CODICI CROSS: 100%|██████████| 16342/16342 [00:18<00:00, 894.97it/s]\n",
      "Updating CODICE OE with old OEMs: 100%|██████████| 13842/13842 [01:15<00:00, 182.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CODICE OE and PREZZO in Z:/My Drive/rcs/oem cross/oemssplit/processed/tulero/tulerodataset26062024.csv\n",
      "Match statistics written to Z:/My Drive/rcs/oem cross/oemssplit/processed/tulero/match_log.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating brands: 100%|██████████| 30376/30376 [00:00<00:00, 3038037.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brands updated in Z:/My Drive/rcs/oem cross/oemssplit/processed/tulero/tulerodataset_with_brands26062024.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openpyxl  # noqa: F401\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def is_unnamed(header):\n",
    "    return str(header).startswith(\"Unnamed\")\n",
    "\n",
    "\n",
    "def validate_first_sheet(df):\n",
    "    return (\n",
    "        is_unnamed(df.columns[1])\n",
    "        and is_unnamed(df.columns[2])\n",
    "        and is_unnamed(df.columns[3])\n",
    "        and is_unnamed(df.columns[4])\n",
    "        and is_unnamed(df.columns[5])\n",
    "        and not pd.isna(df.columns[0])\n",
    "        and not pd.isna(df.columns[6])\n",
    "    )\n",
    "\n",
    "\n",
    "def validate_other_sheet(df):\n",
    "    return (\n",
    "        df.shape[1] == 6\n",
    "        and not is_unnamed(df.columns[1])\n",
    "        and not is_unnamed(df.columns[2])\n",
    "        and not is_unnamed(df.columns[3])\n",
    "        and not is_unnamed(df.columns[4])\n",
    "        and is_unnamed(df.columns[0])\n",
    "    )\n",
    "\n",
    "\n",
    "def process_files(articles_file_path, output_file_path):\n",
    "    xls = pd.ExcelFile(articles_file_path)\n",
    "    relevant_sheets = []\n",
    "    first_sheet_validated = False\n",
    "\n",
    "    for sheet_name in tqdm(xls.sheet_names, desc=\"Validating sheets\"):\n",
    "        df = pd.read_excel(xls, sheet_name=sheet_name, header=0)\n",
    "        if not first_sheet_validated:\n",
    "            if validate_first_sheet(df):\n",
    "                relevant_sheets.append((sheet_name, df))\n",
    "                first_sheet_validated = True\n",
    "            else:\n",
    "                raise ValueError(\"First sheet is not valid\")\n",
    "        else:\n",
    "            if validate_other_sheet(df):\n",
    "                relevant_sheets.append((sheet_name, df))\n",
    "\n",
    "    if not relevant_sheets:\n",
    "        raise ValueError(\"No relevant sheets found in the Excel file\")\n",
    "\n",
    "    first_sheet_df = relevant_sheets[0][1].drop(columns=[\"mgs210\"])\n",
    "    aligned_sheets = [first_sheet_df]\n",
    "    for sheet_name, df in relevant_sheets[1:]:\n",
    "        df.columns = first_sheet_df.columns\n",
    "        aligned_sheets.append(df)\n",
    "\n",
    "    df_combined = pd.concat(aligned_sheets, ignore_index=True)\n",
    "    df_combined = df_combined[~df_combined.iloc[:, 2].isin([\"\", \".\"])]\n",
    "    df_combined = df_combined.drop(columns=[\"STAMPA LISTINI\"])\n",
    "\n",
    "    df_combined.columns = [\n",
    "        \"CODICE PRODOTTO\",\n",
    "        \"BRAND\",\n",
    "        \"DESCRIZIONE\",\n",
    "        \"GIACENZA\",\n",
    "        \"PRZ. ULT. ACQ.\",\n",
    "    ]\n",
    "\n",
    "    for col in [\n",
    "        \"CODICE OE\",\n",
    "        \"CODICI CROSS\",\n",
    "        \"LINK IMMAGINE\",\n",
    "        \"CATEGORIA\",\n",
    "        \"SCHEDA TECNICA\",\n",
    "        \"SCHEDA DI SICUREZZA\",\n",
    "        \"CONFEZIONE\",\n",
    "        \"QUANTITÀ MINIMA\",\n",
    "        \"META.LUNGHEZZA\",\n",
    "        \"META.LARGHEZZA\",\n",
    "        \"META.PROFONDITA'\",\n",
    "        \"META. ...\",\n",
    "    ]:\n",
    "        df_combined[col] = pd.NA\n",
    "\n",
    "    df_combined = df_combined[\n",
    "        [\n",
    "            \"CODICE PRODOTTO\",\n",
    "            \"CODICE OE\",\n",
    "            \"CODICI CROSS\",\n",
    "            \"BRAND\",\n",
    "            \"DESCRIZIONE\",\n",
    "            \"LINK IMMAGINE\",\n",
    "            \"CATEGORIA\",\n",
    "            \"PRZ. ULT. ACQ.\",\n",
    "            \"GIACENZA\",\n",
    "            \"SCHEDA TECNICA\",\n",
    "            \"SCHEDA DI SICUREZZA\",\n",
    "            \"CONFEZIONE\",\n",
    "            \"QUANTITÀ MINIMA\",\n",
    "            \"META.LUNGHEZZA\",\n",
    "            \"META.LARGHEZZA\",\n",
    "            \"META.PROFONDITA'\",\n",
    "            \"META. ...\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    df_combined[\"GIACENZA\"] = pd.to_numeric(\n",
    "        df_combined[\"GIACENZA\"].str.replace(\",\", \".\"), errors=\"coerce\"\n",
    "    )\n",
    "    df_combined[\"PRZ. ULT. ACQ.\"] = pd.to_numeric(\n",
    "        df_combined[\"PRZ. ULT. ACQ.\"].str.replace(\",\", \".\"), errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    df_combined = df_combined[df_combined[\"GIACENZA\"] > 0]\n",
    "    df_combined = df_combined[df_combined[\"PRZ. ULT. ACQ.\"].notna()]\n",
    "    df_combined.to_csv(output_file_path, index=False)\n",
    "\n",
    "\n",
    "def aggregate_oem_numbers(oems_df):\n",
    "    oems_df[\"oem_number\"] = oems_df[\"oem_number\"].astype(str)\n",
    "    oems_df[\"article_alt\"] = oems_df[\"article_alt\"].astype(str)\n",
    "    oems_agg = (\n",
    "        oems_df.groupby(\"article_alt\")[\"oem_number\"]\n",
    "        .apply(lambda x: \" | \".join(x))\n",
    "        .reset_index()\n",
    "    )\n",
    "    return oems_agg\n",
    "\n",
    "\n",
    "def append_oem_numbers(df_cleaned, oems_agg):\n",
    "    df_cleaned[\"CODICE PRODOTTO\"] = (\n",
    "        df_cleaned[\"CODICE PRODOTTO\"].astype(str).str.strip().str.upper()\n",
    "    )\n",
    "    oems_agg[\"article_alt\"] = oems_agg[\"article_alt\"].str.strip().str.upper()\n",
    "    merged_df = pd.merge(\n",
    "        df_cleaned,\n",
    "        oems_agg,\n",
    "        left_on=\"CODICE PRODOTTO\",\n",
    "        right_on=\"article_alt\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "    merged_df.rename(columns={\"oem_number\": \"CODICE OE\"}, inplace=True)\n",
    "    merged_df.drop(columns=[\"article_alt\"], inplace=True)\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def process_and_merge_files(cleaned_csv_path, oems_file_path, output_file_path):\n",
    "    df_cleaned = pd.read_csv(cleaned_csv_path)\n",
    "    oems_df = pd.read_csv(oems_file_path, delimiter=\";\", dtype=str)\n",
    "    oems_agg = aggregate_oem_numbers(oems_df)\n",
    "    df_result = append_oem_numbers(df_cleaned, oems_agg)\n",
    "    df_result.to_csv(output_file_path, index=False)\n",
    "\n",
    "\n",
    "def update_brands(output_file_path, brands_file_path, final_output_file_path):\n",
    "    # Load the output file\n",
    "    df_output = pd.read_csv(output_file_path, dtype=str)\n",
    "\n",
    "    # Load the brands file\n",
    "    brands_df = pd.read_csv(brands_file_path, dtype=str)\n",
    "\n",
    "    # Ensure all values in the relevant columns are strings and strip any leading/trailing spaces\n",
    "    df_output[\"BRAND\"] = df_output[\"BRAND\"].astype(str).str.strip()\n",
    "    brands_df[\"Brand\"] = brands_df[\"Brand\"].astype(str).str.strip()\n",
    "    brands_df[\"Match\"] = brands_df[\"Match\"].astype(str).str.strip()\n",
    "\n",
    "    # Create a lookup dictionary from the brands file\n",
    "    brand_lookup = dict(zip(brands_df[\"Brand\"], brands_df[\"Match\"]))\n",
    "\n",
    "    # Update the BRANDS in the output dataframe\n",
    "    tqdm.pandas(desc=\"Updating brands\")\n",
    "    df_output[\"BRAND\"] = df_output[\"BRAND\"].progress_apply(\n",
    "        lambda x: brand_lookup.get(x, x)\n",
    "    )\n",
    "\n",
    "    # Save the updated dataframe to a new CSV file\n",
    "    df_output.to_csv(final_output_file_path, index=False)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Define file paths\n",
    "    articles_file_path = \"Z:/My Drive/rcs/oem cross/Articles26062024.xls\"\n",
    "    cleaned_csv_path = \"Z:/My Drive/rcs/oem cross/cleaned_Articles26062024.csv\"\n",
    "    old_oems_folder = \"Z:/My Drive/rcs/oem cross/oemssplit/processed/\"\n",
    "    output_folder = \"Z:/My Drive/rcs/oem cross/oemssplit/processed/tulero/\"\n",
    "    final_output_file_path = os.path.join(output_folder, \"tulerodataset26062024.csv\")\n",
    "    final_output_with_brands_path = os.path.join(\n",
    "        output_folder, \"tulerodataset_with_brands26062024.csv\"\n",
    "    )\n",
    "    log_file = os.path.join(output_folder, \"match_log.txt\")\n",
    "    brands_file_path = \"Z:/My Drive/rcs/oem cross/BRANDS.csv\"\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Step 1: Clean dataset\n",
    "    process_files(articles_file_path, cleaned_csv_path)\n",
    "\n",
    "    # Step 2: Do the thing\n",
    "    cleaned_df = pd.read_csv(cleaned_csv_path, dtype=str)\n",
    "    cleaned_df[\"CODICE PRODOTTO\"] = (\n",
    "        cleaned_df[\"CODICE PRODOTTO\"].astype(str).str.strip()\n",
    "    )\n",
    "    cleaned_df[\"CODICE OE\"] = cleaned_df[\"CODICE OE\"].astype(str).str.strip()\n",
    "    cleaned_df[\"BRAND\"] = cleaned_df[\"BRAND\"].astype(str).str.strip()\n",
    "\n",
    "    old_oems_files = [\n",
    "        file\n",
    "        for file in os.listdir(old_oems_folder)\n",
    "        if file.startswith(\"oemsDC\") and file.endswith(\".csv\")\n",
    "    ]\n",
    "    all_oem_mappings = pd.DataFrame()\n",
    "\n",
    "    for file_name in tqdm(old_oems_files, desc=\"Building old OEM mappings\"):\n",
    "        file_path = os.path.join(old_oems_folder, file_name)\n",
    "        oems_df = pd.read_csv(file_path, dtype=str)\n",
    "        oems_df[\"article_altc\"] = oems_df[\"article_altc\"].astype(str).str.strip()\n",
    "        oems_df[\"oem_number\"] = (\n",
    "            oems_df[\"oem_number\"].astype(str).str.strip().str.replace(\" \", \"\")\n",
    "        )\n",
    "        oems_df[\"article_alt_brands\"] = (\n",
    "            oems_df[\"article_alt_brands\"].astype(str).str.strip()\n",
    "        )\n",
    "        oems_df[\"brand_prefix\"] = oems_df[\"article_alt_brands\"].str[:5]\n",
    "        all_oem_mappings = pd.concat(\n",
    "            [all_oem_mappings, oems_df[[\"article_altc\", \"oem_number\", \"brand_prefix\"]]]\n",
    "        )\n",
    "\n",
    "    oem_lookup = (\n",
    "        all_oem_mappings.groupby([\"article_altc\", \"brand_prefix\"])[\"oem_number\"]\n",
    "        .apply(list)\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "    def get_oem_number(row):\n",
    "        key = (row[\"CODICE PRODOTTO\"], row[\"BRAND\"][:5])\n",
    "        return \" | \".join(oem_lookup[key]) if key in oem_lookup else \"Unknown OE\"\n",
    "\n",
    "    tqdm.pandas(desc=\"Updating CODICE OE with old OEMs\")\n",
    "    cleaned_df[\"CODICE OE\"] = cleaned_df.progress_apply(get_oem_number, axis=1)\n",
    "\n",
    "    cleaned_df[\"PRZ. ULT. ACQ.\"] = pd.to_numeric(\n",
    "        cleaned_df[\"PRZ. ULT. ACQ.\"], errors=\"coerce\"\n",
    "    )\n",
    "    cleaned_df[\"PREZZO\"] = cleaned_df[\"PRZ. ULT. ACQ.\"].apply(\n",
    "        lambda x: round(x * 1.25, 2) if pd.notnull(x) else x\n",
    "    )\n",
    "    cleaned_df = cleaned_df.drop(columns=[\"PRZ. ULT. ACQ.\"])\n",
    "\n",
    "    columns_order = [\n",
    "        \"CODICE PRODOTTO\",\n",
    "        \"CODICE OE\",\n",
    "        \"CODICI CROSS\",\n",
    "        \"BRAND\",\n",
    "        \"DESCRIZIONE\",\n",
    "        \"LINK IMMAGINE\",\n",
    "        \"CATEGORIA\",\n",
    "        \"GIACENZA\",\n",
    "        \"PREZZO\",\n",
    "        \"SCHEDA TECNICA\",\n",
    "        \"SCHEDA DI SICUREZZA\",\n",
    "        \"CONFEZIONE\",\n",
    "        \"QUANTITÀ MINIMA\",\n",
    "        \"META.LUNGHEZZA\",\n",
    "        \"META.LARGHEZZA\",\n",
    "        \"META.PROFONDITA'\",\n",
    "        \"META. ...\",\n",
    "    ]\n",
    "    cleaned_df = cleaned_df[columns_order]\n",
    "    cleaned_df[\"CODICI CROSS\"] = \"\"\n",
    "\n",
    "    cross_references = (\n",
    "        cleaned_df[cleaned_df[\"CODICE OE\"] != \"Unknown OE\"]\n",
    "        .groupby(\"CODICE OE\")[\"CODICE PRODOTTO\"]\n",
    "        .apply(list)\n",
    "        .to_dict()\n",
    "    )\n",
    "    cross_codes_series = pd.Series(index=cleaned_df.index, dtype=str)\n",
    "\n",
    "    for codice_oe, prodotti in tqdm(\n",
    "        cross_references.items(), desc=\"Updating CODICI CROSS\"\n",
    "    ):\n",
    "        cross_codes = {\n",
    "            prodotto: \" | \".join([code for code in prodotti if code != prodotto])\n",
    "            for prodotto in prodotti\n",
    "        }\n",
    "        cross_codes_series.update(pd.Series(cross_codes))\n",
    "\n",
    "    cleaned_df[\"CODICI CROSS\"] = cleaned_df.index.map(cross_codes_series).fillna(\"\")\n",
    "    cleaned_df[\"padded_oe\"] = \" \" + cleaned_df[\"CODICE OE\"].str.strip() + \" \"\n",
    "\n",
    "    def find_additional_cross_codes(codice_prodotto, padded_oe):\n",
    "        matches = cleaned_df[cleaned_df[\"CODICE OE\"] != \"Unknown OE\"]\n",
    "        exact_matches = matches[\n",
    "            matches[\"padded_oe\"].str.contains(f\" {codice_prodotto} \", regex=False)\n",
    "        ]\n",
    "        if not exact_matches.empty:\n",
    "            return \" | \".join(exact_matches[\"CODICE PRODOTTO\"].unique())\n",
    "        return \"\"\n",
    "\n",
    "    unknown_oe_mask = cleaned_df[\"CODICE OE\"] == \"Unknown OE\"\n",
    "    cleaned_df.loc[unknown_oe_mask, \"CODICI CROSS\"] = cleaned_df.loc[\n",
    "        unknown_oe_mask, \"CODICE PRODOTTO\"\n",
    "    ].progress_apply(\n",
    "        lambda codice_prodotto: find_additional_cross_codes(\n",
    "            codice_prodotto, cleaned_df[\"padded_oe\"]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    cleaned_df.to_csv(final_output_file_path, index=False)\n",
    "\n",
    "    successful_matches = (\n",
    "        cleaned_df[\"CODICE OE\"].apply(lambda x: x != \"Unknown OE\").sum()\n",
    "    )\n",
    "    unsuccessful_matches = (\n",
    "        cleaned_df[\"CODICE OE\"].apply(lambda x: x == \"Unknown OE\").sum()\n",
    "    )\n",
    "\n",
    "    with open(log_file, \"w\") as log:\n",
    "        log.write(f\"Total successful matches: {successful_matches}\\n\")\n",
    "        log.write(f\"Total unsuccessful matches: {unsuccessful_matches}\\n\")\n",
    "\n",
    "    print(f\"Updated CODICE OE and PREZZO in {final_output_file_path}\")\n",
    "    print(f\"Match statistics written to {log_file}\")\n",
    "\n",
    "    # Step 4: Update brands\n",
    "    update_brands(\n",
    "        final_output_file_path, brands_file_path, final_output_with_brands_path\n",
    "    )\n",
    "    print(f\"Brands updated in {final_output_with_brands_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating sheets: 100%|██████████| 5/5 [00:00<00:00,  9.74it/s]\n",
      "Building old OEM mappings: 100%|██████████| 4/4 [00:02<00:00,  1.50it/s]\n",
      "Updating CODICE OE with old OEMs: 100%|██████████| 31915/31915 [00:00<00:00, 221370.17it/s]\n",
      "Generating cross codes: 100%|██████████| 31915/31915 [00:00<00:00, 66107.44it/s]\n",
      "Updating CODICI CROSS: 100%|██████████| 12660/12660 [00:16<00:00, 766.00it/s]\n",
      "Updating CODICE OE with old OEMs: 100%|██████████| 4991/4991 [00:26<00:00, 190.45it/s]\n",
      "Validating sheets: 100%|██████████| 5/5 [00:00<00:00, 10.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns before renaming: 5\n",
      "Column names before renaming: ['Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4', 'Unnamed: 5']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating brands: 100%|██████████| 31915/31915 [00:00<00:00, 3190133.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final file saved at Z:/My Drive/rcs/Web/Tulero/app/output/final_output_with_brands_path.csv\n",
      "Small items file saved at Z:/My Drive/rcs/Web/Tulero/app/output/output_csv_small_items_path.csv\n",
      "Large items file saved at Z:/My Drive/rcs/Web/Tulero/app/output/output_csv_large_items_path.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "IGNORED_BRANDS = [\n",
    "    \"AP\",\n",
    "    \"AREXONS\",\n",
    "    \"ASSO\",\n",
    "    \"ATE\",\n",
    "    \"ATECSO\",\n",
    "    \"AUTOCLIMA\",\n",
    "    \"BIRTH\",\n",
    "    \"BOSCH\",\n",
    "    \"BREMBO\",\n",
    "    \"BUGATTI\",\n",
    "    \"CASCO\",\n",
    "    \"CASTROL\",\n",
    "    \"CEI\",\n",
    "    \"CORTECO\",\n",
    "    \"COVIND\",\n",
    "    \"DAF\",\n",
    "    \"DAYCO\",\n",
    "    \"DELPHI\",\n",
    "    \"DENSO\",\n",
    "    \"DOLZ\",\n",
    "    \"ELRING\",\n",
    "    \"EMMERRE\",\n",
    "    \"ERA\",\n",
    "    \"EXIDE\",\n",
    "    \"FAG\",\n",
    "    \"FEBI\",\n",
    "    \"FERODO\",\n",
    "    \"FIAT\",\n",
    "    \"FORD\",\n",
    "    \"FRAP\",\n",
    "    \"FTE\",\n",
    "    \"GATES\",\n",
    "    \"HELLA\",\n",
    "    \"HOFFER\",\n",
    "    \"IMASAF\",\n",
    "    \"INA\",\n",
    "    \"ISUZU\",\n",
    "    \"IVECO\",\n",
    "    \"JAPANPARTS\",\n",
    "    \"JAPKO\",\n",
    "    \"KNECHT\",\n",
    "    \"KRIOS\",\n",
    "    \"LEMFORDER\",\n",
    "    \"LUK\",\n",
    "    \"MAHLE\",\n",
    "    \"MAN\",\n",
    "    \"METELLI\",\n",
    "    \"MEYLE\",\n",
    "    \"MOBIL\",\n",
    "    \"MONROE\",\n",
    "    \"MOOG\",\n",
    "    \"MULLER FILTER\",\n",
    "    \"NISSAN\",\n",
    "    \"NISSENS\",\n",
    "    \"NK\",\n",
    "    \"NRF\",\n",
    "    \"OLSA\",\n",
    "    \"OMP\",\n",
    "    \"PEUGEOT\",\n",
    "    \"PIAGGIO\",\n",
    "    \"PIERBURG\",\n",
    "    \"RAICAM\",\n",
    "    \"RENAULT\",\n",
    "    \"SACHS\",\n",
    "    \"SCANIA\",\n",
    "    \"SELENIA\",\n",
    "    \"SIDAT\",\n",
    "    \"SKF\",\n",
    "    \"TEXTAR\",\n",
    "    \"TRW\",\n",
    "    \"TUDOR\",\n",
    "    \"UFI\",\n",
    "    \"VALEO\",\n",
    "    \"VEMA\",\n",
    "    \"VITAL SUSPENSIONS\",\n",
    "    \"VOLVO\",\n",
    "    \"VOLKSWAGEN\",\n",
    "    \"ZETA-ERRE\",\n",
    "    \"ZF\",\n",
    "]\n",
    "\n",
    "\n",
    "def is_unnamed(header):\n",
    "    return str(header).startswith(\"Unnamed\")\n",
    "\n",
    "\n",
    "def validate_first_sheet(df):\n",
    "    return (\n",
    "        is_unnamed(df.columns[1])\n",
    "        and is_unnamed(df.columns[2])\n",
    "        and is_unnamed(df.columns[3])\n",
    "        and is_unnamed(df.columns[4])\n",
    "        and is_unnamed(df.columns[5])\n",
    "        and not pd.isna(df.columns[0])\n",
    "        and not pd.isna(df.columns[6])\n",
    "    )\n",
    "\n",
    "\n",
    "def validate_other_sheet(df):\n",
    "    return (\n",
    "        df.shape[1] == 6\n",
    "        and not is_unnamed(df.columns[1])\n",
    "        and not is_unnamed(df.columns[2])\n",
    "        and not is_unnamed(df.columns[3])\n",
    "        and not is_unnamed(df.columns[4])\n",
    "        and is_unnamed(df.columns[0])\n",
    "    )\n",
    "\n",
    "\n",
    "def get_oem_number(row, oem_lookup, IGNORED_BRANDS):\n",
    "    if row[\"BRAND\"] in IGNORED_BRANDS:\n",
    "        return \"\"  # Skip filling OE code for ignored brands\n",
    "    key = (row[\"CODICE PRODOTTO\"], row[\"BRAND\"][:5])\n",
    "    return \" | \".join(oem_lookup[key]) if key in oem_lookup else \"Unknown OE\"\n",
    "\n",
    "\n",
    "def find_additional_cross_codes(codice_prodotto, padded_oe, cleaned_df, IGNORED_BRANDS):\n",
    "    matches = cleaned_df[\n",
    "        (cleaned_df[\"CODICE OE\"] != \"Unknown OE\")\n",
    "        & (~cleaned_df[\"BRAND\"].isin(IGNORED_BRANDS))\n",
    "    ]\n",
    "    exact_matches = matches[\n",
    "        matches[\"padded_oe\"].str.contains(f\" {codice_prodotto} \", regex=False)\n",
    "    ]\n",
    "    if not exact_matches.empty:\n",
    "        return \" | \".join(exact_matches[\"CODICE PRODOTTO\"].unique())\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def update_brands(df_output, brands_file_path):\n",
    "    # Load the brands file\n",
    "    brands_df = pd.read_csv(brands_file_path, dtype=str)\n",
    "    brands_df[\"Brand\"] = brands_df[\"Brand\"].astype(str).str.strip()\n",
    "    brands_df[\"Match\"] = brands_df[\"Match\"].astype(str).str.strip()\n",
    "\n",
    "    # Create a lookup dictionary from the brands file\n",
    "    brand_lookup = dict(zip(brands_df[\"Brand\"], brands_df[\"Match\"]))\n",
    "\n",
    "    # Update the BRANDS in the output dataframe\n",
    "    tqdm.pandas(desc=\"Updating brands\")\n",
    "    df_output[\"BRAND\"] = df_output[\"BRAND\"].progress_apply(\n",
    "        lambda x: brand_lookup.get(x, x)\n",
    "    )\n",
    "    return df_output\n",
    "\n",
    "\n",
    "def optimized_cross_code_generation(cleaned_df, ignored_brands):\n",
    "    cross_references = {}\n",
    "    for _, row in tqdm(\n",
    "        cleaned_df.iterrows(), total=len(cleaned_df), desc=\"Generating cross codes\"\n",
    "    ):\n",
    "        codice_oe = row[\"CODICE OE\"]\n",
    "        codice_prodotto = row[\"CODICE PRODOTTO\"]\n",
    "        brand = row[\"BRAND\"]\n",
    "\n",
    "        if codice_oe != \"Unknown OE\" and brand not in ignored_brands:\n",
    "            if codice_oe not in cross_references:\n",
    "                cross_references[codice_oe] = []\n",
    "            cross_references[codice_oe].append(codice_prodotto)\n",
    "\n",
    "    cross_codes_series = pd.Series(index=cleaned_df.index, dtype=str)\n",
    "    for codice_oe, prodotti in tqdm(\n",
    "        cross_references.items(), desc=\"Updating CODICI CROSS\"\n",
    "    ):\n",
    "        cross_codes = {\n",
    "            prodotto: \" | \".join([code for code in prodotti if code != prodotto])\n",
    "            for prodotto in prodotti\n",
    "        }\n",
    "        cross_codes_series.update(pd.Series(cross_codes))\n",
    "\n",
    "    return cross_codes_series.fillna(\"\")\n",
    "\n",
    "\n",
    "def load_and_clean_warehouse_file(\n",
    "    warehouse_location_file, warehouse_unknown_location_path\n",
    "):\n",
    "    xls = pd.ExcelFile(warehouse_location_file)\n",
    "    relevant_sheets = []\n",
    "    first_sheet_validated = False\n",
    "\n",
    "    for sheet_name in tqdm(xls.sheet_names, desc=\"Validating sheets\"):\n",
    "        df = pd.read_excel(xls, sheet_name=sheet_name, header=0, dtype=str)\n",
    "        if not first_sheet_validated:\n",
    "            if validate_first_sheet(df):\n",
    "                relevant_sheets.append((sheet_name, df))\n",
    "                first_sheet_validated = True\n",
    "            else:\n",
    "                raise ValueError(\"First sheet is not valid\")\n",
    "        else:\n",
    "            if validate_other_sheet(df):\n",
    "                relevant_sheets.append((sheet_name, df))\n",
    "\n",
    "    if not relevant_sheets:\n",
    "        raise ValueError(\"No relevant sheets found in the Excel file\")\n",
    "\n",
    "    # Identify and drop the column that starts with 'mgs'\n",
    "    first_sheet_df = relevant_sheets[0][1]\n",
    "    mgs_column = [col for col in first_sheet_df.columns if col.startswith(\"mgs\")]\n",
    "    if mgs_column:\n",
    "        first_sheet_df = first_sheet_df.drop(columns=mgs_column)\n",
    "    else:\n",
    "        raise ValueError(\"No column starting with 'mgs' found\")\n",
    "\n",
    "    aligned_sheets = [first_sheet_df]\n",
    "    for sheet_name, df in relevant_sheets[1:]:\n",
    "        df.columns = first_sheet_df.columns\n",
    "        aligned_sheets.append(df)\n",
    "\n",
    "    df_combined = pd.concat(aligned_sheets, ignore_index=True)\n",
    "    df_combined = df_combined[~df_combined.iloc[:, 2].isin([\"\", \".\"])]\n",
    "\n",
    "    # Drop the 'STAMPA ANAGRAFICA ARTICOLI' column\n",
    "    df_combined = df_combined.drop(columns=[\"STAMPA ANAGRAFICA ARTICOLI\"])\n",
    "\n",
    "    # Check the number of columns before renaming\n",
    "    print(f\"Number of columns before renaming: {len(df_combined.columns)}\")\n",
    "    print(f\"Column names before renaming: {df_combined.columns.tolist()}\")\n",
    "\n",
    "    # Assign column names based on the number of columns\n",
    "    if len(df_combined.columns) == 5:\n",
    "        df_combined.columns = [\n",
    "            \"CODICE PRODOTTO\",\n",
    "            \"BRAND\",\n",
    "            \"DESCRIZIONE\",\n",
    "            \"UBICAZIONE\",\n",
    "            \"GIACENZA\",\n",
    "        ]\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected number of columns: {len(df_combined.columns)}\")\n",
    "\n",
    "    df_combined[\"UBICAZIONE\"] = (\n",
    "        df_combined[\"UBICAZIONE\"].astype(str).replace(\"\", \"Location Unknown\")\n",
    "    )\n",
    "\n",
    "    # Cleaning UBICAZIONE column\n",
    "    pattern = r\"^[A-Z]\\.\"  # Regex pattern to match a letter followed by a \".\"\n",
    "    valid_rows = df_combined[\"UBICAZIONE\"].str.match(pattern)\n",
    "    valid_df = df_combined[valid_rows]\n",
    "    invalid_df = df_combined[~valid_rows]\n",
    "\n",
    "    # Output the rows with unknown locations\n",
    "    invalid_df.to_csv(warehouse_unknown_location_path, index=False)\n",
    "\n",
    "    return valid_df\n",
    "\n",
    "\n",
    "def merge_with_warehouse_data(cleaned_df, warehouse_df):\n",
    "    # Merging based on CODICE PRODOTTO and BRAND\n",
    "    merged_df = pd.merge(\n",
    "        cleaned_df,\n",
    "        warehouse_df[[\"CODICE PRODOTTO\", \"BRAND\", \"UBICAZIONE\"]],\n",
    "        on=[\"CODICE PRODOTTO\", \"BRAND\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    # Fill NaN locations with empty string to avoid issues later\n",
    "    merged_df[\"UBICAZIONE\"] = merged_df[\"UBICAZIONE\"].fillna(\"\")\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def process_files(\n",
    "    articles_file_path,\n",
    "    output_file_path,\n",
    "    old_oems_folder,\n",
    "    brands_file_path,\n",
    "    final_output_with_brands_path,\n",
    "    update_progress=None,\n",
    "):\n",
    "    xls = pd.ExcelFile(articles_file_path)\n",
    "    relevant_sheets = []\n",
    "    first_sheet_validated = False\n",
    "\n",
    "    for sheet_name in tqdm(xls.sheet_names, desc=\"Validating sheets\"):\n",
    "        df = pd.read_excel(\n",
    "            xls, sheet_name=sheet_name, header=0, dtype=str\n",
    "        )  # Ensure all columns are read as strings\n",
    "        if not first_sheet_validated:\n",
    "            if validate_first_sheet(df):\n",
    "                relevant_sheets.append((sheet_name, df))\n",
    "                first_sheet_validated = True\n",
    "            else:\n",
    "                raise ValueError(\"First sheet is not valid\")\n",
    "        else:\n",
    "            if validate_other_sheet(df):\n",
    "                relevant_sheets.append((sheet_name, df))\n",
    "\n",
    "    if not relevant_sheets:\n",
    "        raise ValueError(\"No relevant sheets found in the Excel file\")\n",
    "\n",
    "    first_sheet_df = relevant_sheets[0][1].drop(columns=[\"mgs210\"])\n",
    "    aligned_sheets = [first_sheet_df]\n",
    "    for sheet_name, df in relevant_sheets[1:]:\n",
    "        df.columns = first_sheet_df.columns\n",
    "        aligned_sheets.append(df)\n",
    "\n",
    "    df_combined = pd.concat(aligned_sheets, ignore_index=True)\n",
    "    df_combined = df_combined[~df_combined.iloc[:, 2].isin([\"\", \".\"])]\n",
    "    df_combined = df_combined.drop(columns=[\"STAMPA LISTINI\"])\n",
    "\n",
    "    df_combined.columns = [\n",
    "        \"CODICE PRODOTTO\",\n",
    "        \"BRAND\",\n",
    "        \"DESCRIZIONE\",\n",
    "        \"GIACENZA\",\n",
    "        \"PRZ. ULT. ACQ.\",\n",
    "    ]\n",
    "\n",
    "    for col in [\n",
    "        \"CODICE OE\",\n",
    "        \"CODICI CROSS\",\n",
    "        \"LINK IMMAGINE\",\n",
    "        \"CATEGORIA\",\n",
    "        \"SCHEDA TECNICA\",\n",
    "        \"SCHEDA DI SICUREZZA\",\n",
    "        \"CONFEZIONE\",\n",
    "        \"QUANTITÀ MINIMA\",\n",
    "        \"META.LUNGHEZZA\",\n",
    "        \"META.LARGHEZZA\",\n",
    "        \"META.PROFONDITA'\",\n",
    "        \"META. ...\",\n",
    "    ]:\n",
    "        df_combined[col] = pd.NA\n",
    "\n",
    "    # Ensure specified columns are treated as strings\n",
    "    columns_to_ensure_as_strings = [\n",
    "        \"CODICE PRODOTTO\",\n",
    "        \"BRAND\",\n",
    "        \"DESCRIZIONE\",\n",
    "        \"CODICE OE\",\n",
    "        \"CODICI CROSS\",\n",
    "        \"LINK IMMAGINE\",\n",
    "        \"CATEGORIA\",\n",
    "        \"SCHEDA TECNICA\",\n",
    "        \"SCHEDA DI SICUREZZA\",\n",
    "        \"CONFEZIONE\",\n",
    "        \"META.LUNGHEZZA\",\n",
    "        \"META.LARGHEZZA\",\n",
    "        \"META.PROFONDITA'\",\n",
    "        \"META. ...\",\n",
    "    ]\n",
    "    df_combined[columns_to_ensure_as_strings] = df_combined[\n",
    "        columns_to_ensure_as_strings\n",
    "    ].astype(str)\n",
    "\n",
    "    df_combined[\"GIACENZA\"] = pd.to_numeric(\n",
    "        df_combined[\"GIACENZA\"].str.replace(\",\", \".\"), errors=\"coerce\"\n",
    "    )\n",
    "    df_combined[\"PRZ. ULT. ACQ.\"] = pd.to_numeric(\n",
    "        df_combined[\"PRZ. ULT. ACQ.\"].str.replace(\",\", \".\"), errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    df_combined = df_combined[df_combined[\"GIACENZA\"] > 0]\n",
    "    df_combined = df_combined[df_combined[\"PRZ. ULT. ACQ.\"].notna()]\n",
    "\n",
    "    # No need to save and reload, continue with the DataFrame in memory\n",
    "    cleaned_df = df_combined.copy()\n",
    "    cleaned_df[\"CODICE PRODOTTO\"] = cleaned_df[\"CODICE PRODOTTO\"].str.strip()\n",
    "    cleaned_df[\"CODICE OE\"] = cleaned_df[\"CODICE OE\"].str.strip()\n",
    "    cleaned_df[\"BRAND\"] = cleaned_df[\"BRAND\"].str.strip()\n",
    "\n",
    "    old_oems_files = [\n",
    "        file\n",
    "        for file in os.listdir(old_oems_folder)\n",
    "        if file.startswith(\"oemsDC\") and file.endswith(\".csv\")\n",
    "    ]\n",
    "    all_oem_mappings = pd.DataFrame()\n",
    "\n",
    "    for file_name in tqdm(old_oems_files, desc=\"Building old OEM mappings\"):\n",
    "        file_path = os.path.join(old_oems_folder, file_name)\n",
    "        oems_df = pd.read_csv(file_path, dtype=str)\n",
    "        oems_df[\"article_altc\"] = oems_df[\"article_altc\"].astype(str).str.strip()\n",
    "        oems_df[\"oem_number\"] = (\n",
    "            oems_df[\"oem_number\"].astype(str).str.strip().str.replace(\" \", \"\")\n",
    "        )\n",
    "        oems_df[\"article_alt_brands\"] = (\n",
    "            oems_df[\"article_alt_brands\"].astype(str).str.strip()\n",
    "        )\n",
    "        oems_df[\"brand_prefix\"] = oems_df[\"article_alt_brands\"].str[:5]\n",
    "        all_oem_mappings = pd.concat(\n",
    "            [all_oem_mappings, oems_df[[\"article_altc\", \"oem_number\", \"brand_prefix\"]]]\n",
    "        )\n",
    "\n",
    "    oem_lookup = (\n",
    "        all_oem_mappings.groupby([\"article_altc\", \"brand_prefix\"])[\"oem_number\"]\n",
    "        .apply(list)\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "    # Update CODICE OE using the get_oem_number function\n",
    "    tqdm.pandas(desc=\"Updating CODICE OE with old OEMs\")\n",
    "    cleaned_df[\"CODICE OE\"] = cleaned_df.progress_apply(\n",
    "        lambda row: get_oem_number(row, oem_lookup, IGNORED_BRANDS), axis=1\n",
    "    )\n",
    "\n",
    "    # Update PREZZO based on PRZ. ULT. ACQ.\n",
    "    cleaned_df[\"PRZ. ULT. ACQ.\"] = pd.to_numeric(\n",
    "        cleaned_df[\"PRZ. ULT. ACQ.\"], errors=\"coerce\"\n",
    "    )\n",
    "    cleaned_df[\"PREZZO\"] = cleaned_df[\"PRZ. ULT. ACQ.\"].apply(\n",
    "        lambda x: round(x * 1.25, 2) if pd.notnull(x) else x\n",
    "    )\n",
    "    cleaned_df = cleaned_df.drop(columns=[\"PRZ. ULT. ACQ.\"])\n",
    "\n",
    "    # Ensure specified columns are treated as strings before reordering\n",
    "    cleaned_df[columns_to_ensure_as_strings] = cleaned_df[\n",
    "        columns_to_ensure_as_strings\n",
    "    ].astype(str)\n",
    "\n",
    "    # Reorder columns\n",
    "    columns_order = [\n",
    "        \"CODICE PRODOTTO\",\n",
    "        \"CODICE OE\",\n",
    "        \"CODICI CROSS\",\n",
    "        \"BRAND\",\n",
    "        \"DESCRIZIONE\",\n",
    "        \"LINK IMMAGINE\",\n",
    "        \"CATEGORIA\",\n",
    "        \"GIACENZA\",\n",
    "        \"PREZZO\",\n",
    "        \"SCHEDA TECNICA\",\n",
    "        \"SCHEDA DI SICUREZZA\",\n",
    "        \"CONFEZIONE\",\n",
    "        \"QUANTITÀ MINIMA\",\n",
    "        \"META.LUNGHEZZA\",\n",
    "        \"META.LARGHEZZA\",\n",
    "        \"META.PROFONDITA'\",\n",
    "        \"META. ...\",\n",
    "    ]\n",
    "    cleaned_df = cleaned_df[columns_order]\n",
    "    cleaned_df[\"CODICI CROSS\"] = \"\"\n",
    "\n",
    "    # Apply optimized cross-code generation function\n",
    "    cleaned_df[\"CODICI CROSS\"] = optimized_cross_code_generation(\n",
    "        cleaned_df, IGNORED_BRANDS\n",
    "    )\n",
    "\n",
    "    # Handle cases where CODICE OE is unknown and brand is not ignored\n",
    "    cleaned_df[\"padded_oe\"] = \" \" + cleaned_df[\"CODICE OE\"].str.strip() + \" \"\n",
    "    unknown_oe_mask = (cleaned_df[\"CODICE OE\"] == \"Unknown OE\") & (\n",
    "        ~cleaned_df[\"BRAND\"].isin(IGNORED_BRANDS)\n",
    "    )\n",
    "    cleaned_df.loc[unknown_oe_mask, \"CODICI CROSS\"] = cleaned_df.loc[\n",
    "        unknown_oe_mask, \"CODICE PRODOTTO\"\n",
    "    ].progress_apply(\n",
    "        lambda codice_prodotto: find_additional_cross_codes(\n",
    "            codice_prodotto, cleaned_df[\"padded_oe\"], cleaned_df, IGNORED_BRANDS\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Drop the 'padded_oe' column\n",
    "    cleaned_df = cleaned_df.drop(columns=[\"padded_oe\"])\n",
    "\n",
    "    # Fill the \"CONFEZIONE\" column with \"1 pz\" and the \"QUANTITÀ MINIMA\" column with \"1\"\n",
    "    cleaned_df[\"CONFEZIONE\"] = \"1 pz\"\n",
    "    cleaned_df[\"QUANTITÀ MINIMA\"] = \"1\"\n",
    "\n",
    "    # Set CATEGORIA to \"Ricambio\"\n",
    "    cleaned_df[\"CATEGORIA\"] = \"Ricambio\"\n",
    "\n",
    "    # Ensure specified columns remain empty\n",
    "    columns_to_keep_empty = [\n",
    "        \"LINK IMMAGINE\",\n",
    "        \"SCHEDA TECNICA\",\n",
    "        \"SCHEDA DI SICUREZZA\",\n",
    "        \"META.LUNGHEZZA\",\n",
    "        \"META.LARGHEZZA\",\n",
    "        \"META.PROFONDITA'\",\n",
    "        \"META. ...\",\n",
    "    ]\n",
    "    cleaned_df[columns_to_keep_empty] = \"\"\n",
    "\n",
    "    return cleaned_df\n",
    "\n",
    "\n",
    "def process_files_with_location(\n",
    "    articles_file_path,\n",
    "    output_file_path,\n",
    "    old_oems_folder,\n",
    "    brands_file_path,\n",
    "    warehouse_location_file,\n",
    "    final_output_with_brands_path,\n",
    "    output_csv_small_items_path,\n",
    "    output_csv_large_items_path,\n",
    "    warehouse_unknown_location_path,\n",
    "    update_progress=None,\n",
    "):\n",
    "    # Load and clean the articles file (same as original)\n",
    "    cleaned_df = process_files(\n",
    "        articles_file_path,\n",
    "        output_file_path,\n",
    "        old_oems_folder,\n",
    "        brands_file_path,\n",
    "        final_output_with_brands_path,\n",
    "        update_progress,\n",
    "    )\n",
    "\n",
    "    # Load and clean the warehouse location file\n",
    "    warehouse_df = load_and_clean_warehouse_file(\n",
    "        warehouse_location_file, warehouse_unknown_location_path\n",
    "    )\n",
    "\n",
    "    # Merge the warehouse location data with the cleaned data\n",
    "    cleaned_df = merge_with_warehouse_data(cleaned_df, warehouse_df)\n",
    "\n",
    "    # Update brands as before\n",
    "    cleaned_df = update_brands(cleaned_df, brands_file_path)\n",
    "\n",
    "    # Final reordering of columns\n",
    "    columns_order = [\n",
    "        \"CODICE PRODOTTO\",\n",
    "        \"CODICE OE\",\n",
    "        \"CODICI CROSS\",\n",
    "        \"BRAND\",\n",
    "        \"DESCRIZIONE\",\n",
    "        \"LINK IMMAGINE\",\n",
    "        \"CATEGORIA\",\n",
    "        \"UBICAZIONE\",\n",
    "        \"GIACENZA\",\n",
    "        \"PREZZO\",\n",
    "        \"SCHEDA TECNICA\",\n",
    "        \"SCHEDA DI SICUREZZA\",\n",
    "        \"CONFEZIONE\",\n",
    "        \"QUANTITÀ MINIMA\",\n",
    "        \"META.LUNGHEZZA\",\n",
    "        \"META.LARGHEZZA\",\n",
    "        \"META.PROFONDITA'\",\n",
    "        \"META. ...\",\n",
    "    ]\n",
    "    cleaned_df = cleaned_df[columns_order]\n",
    "\n",
    "    # Save the final output\n",
    "    cleaned_df.to_csv(final_output_with_brands_path, index=False)\n",
    "\n",
    "    # Generate filtered outputs\n",
    "    small_items_df = cleaned_df[\n",
    "        cleaned_df[\"UBICAZIONE\"].str.startswith((\"A.\", \"B.\", \"C.\"))\n",
    "    ]\n",
    "    large_items_df = cleaned_df[\n",
    "        ~cleaned_df[\"UBICAZIONE\"].str.startswith((\"A.\", \"B.\", \"C.\"))\n",
    "    ]\n",
    "\n",
    "    small_items_df.to_csv(output_csv_small_items_path, index=False)\n",
    "    large_items_df.to_csv(output_csv_large_items_path, index=False)\n",
    "\n",
    "    print(f\"Final file saved at {final_output_with_brands_path}\")\n",
    "    print(f\"Small items file saved at {output_csv_small_items_path}\")\n",
    "    print(f\"Large items file saved at {output_csv_large_items_path}\")\n",
    "\n",
    "\n",
    "# Define paths for files\n",
    "articles_file_path = \"Z:/My Drive/rcs/Web/Tulero/app/data/mgs010_Stampa Anagrafica Articoli al 21-08 con upa e giac.xls\"\n",
    "old_oems_folder = \"Z:/My Drive/rcs/Web/Tulero/app/data/oems\"\n",
    "brands_file_path = \"Z:/My Drive/rcs/Web/Tulero/app/data/BRANDS.csv\"\n",
    "warehouse_location_file = \"Z:/My Drive/rcs/Web/Tulero/app/data/mgs010_Stampa Anagrafica Articoli al 21-08 con ubi e giac.xls\"\n",
    "final_output_with_brands_path = (\n",
    "    \"Z:/My Drive/rcs/Web/Tulero/app/output/final_output_with_brands_path.csv\"\n",
    ")\n",
    "output_csv_small_items_path = (\n",
    "    \"Z:/My Drive/rcs/Web/Tulero/app/output/output_csv_small_items_path.csv\"\n",
    ")\n",
    "output_csv_large_items_path = (\n",
    "    \"Z:/My Drive/rcs/Web/Tulero/app/output/output_csv_large_items_path.csv\"\n",
    ")\n",
    "warehouse_unknown_location_path = (\n",
    "    \"Z:/My Drive/rcs/Web/Tulero/app/output/warehouse_unknown_location_path.csv\"\n",
    ")\n",
    "\n",
    "\n",
    "# Main Processing Call\n",
    "process_files_with_location(\n",
    "    articles_file_path,\n",
    "    output_csv_small_items_path,\n",
    "    old_oems_folder,\n",
    "    brands_file_path,\n",
    "    warehouse_location_file,\n",
    "    final_output_with_brands_path,\n",
    "    output_csv_small_items_path,\n",
    "    output_csv_large_items_path,\n",
    "    warehouse_unknown_location_path,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating sheets: 100%|██████████| 5/5 [00:01<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns before renaming: 5\n",
      "Column names before renaming: ['Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4', 'Unnamed: 5']\n",
      "Unknown items saved to: Z:/My Drive/rcs/Web/Tulero/app/output/warehouse_unknown_location_path.csv\n",
      "Small items saved to: Z:/My Drive/rcs/Web/Tulero/app/output/output_csv_small_items_path.csv\n",
      "Large items saved to: Z:/My Drive/rcs/Web/Tulero/app/output/output_csv_large_items_path.csv\n",
      "Small Items:\n",
      "   CODICE PRODOTTO MARCA                     DESCRIZIONE    UBICAZIONE  \\\n",
      "3        0.001.003   RCS             TESTINA SOSP ANT SX  B.07.013.009   \n",
      "6        0.001.176   RCS        INTERRUTTORE VOLVO TRUCK  C.12.016.030   \n",
      "9        0.001.302   RCS  SENSORE GIRI MOTORE MITSUBISHI  C.33.004.035   \n",
      "12       0.002.005   RCS       ELETTROV TUBO GASOLIO 12V  B.06.034.004   \n",
      "13   0.002.005/24V   RCS       ELETTROV TUBO GASOLIO 24V  C.26.015.018   \n",
      "\n",
      "    GIACENZA  \n",
      "3        1.0  \n",
      "6        1.0  \n",
      "9        0.0  \n",
      "12       1.0  \n",
      "13       2.0  \n",
      "\n",
      "Large Items:\n",
      "    CODICE PRODOTTO MARCA                         DESCRIZIONE    UBICAZIONE  \\\n",
      "8         0.001.252   RCS         TESTA STERZO DAILY 35C9/C11        G01.17   \n",
      "11        0.001.312   RCS                   BARRA DI REAZIONE  F.01.016.005   \n",
      "49        0.003.853   RCS  MOT.AVV/TO HM95R3801SE ex 1327A431  F.01.021.007   \n",
      "94        0.006.304   RCS       POMPA INIEZIONE PERKINS DP210  E.01.002.002   \n",
      "102       0.009.030   RCS                        KIT FRIZIONE  E.20.020.P01   \n",
      "\n",
      "     GIACENZA  \n",
      "8         1.0  \n",
      "11        0.0  \n",
      "49        0.0  \n",
      "94        1.0  \n",
      "102       1.0  \n",
      "\n",
      "Unknown Items:\n",
      "   CODICE PRODOTTO  MARCA                     DESCRIZIONE        UBICAZIONE  \\\n",
      "1        KTBWP8860  DAYCO               KIT DISTRIBUZIONE  Location Unknown   \n",
      "4        0.001.102    RCS                      PLANETARIO  Location Unknown   \n",
      "5        0.001.152    RCS                  DIFFUSORE OM40  Location Unknown   \n",
      "7        0.001.182    RCS  INT. LUCE FRENO - RENAULT CLIO  Location Unknown   \n",
      "10       0.001.304    RCS             SEMIASSE FIAT DOBLO  Location Unknown   \n",
      "\n",
      "    GIACENZA  \n",
      "1        0.0  \n",
      "4        0.0  \n",
      "5        1.0  \n",
      "7        0.0  \n",
      "10       0.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def is_unnamed(header):\n",
    "    return str(header).startswith(\"Unnamed\")\n",
    "\n",
    "\n",
    "def validate_first_sheet(df):\n",
    "    return (\n",
    "        is_unnamed(df.columns[1])\n",
    "        and is_unnamed(df.columns[2])\n",
    "        and is_unnamed(df.columns[3])\n",
    "        and is_unnamed(df.columns[4])\n",
    "        and is_unnamed(df.columns[5])\n",
    "        and not pd.isna(df.columns[0])\n",
    "        and not pd.isna(df.columns[6])\n",
    "    )\n",
    "\n",
    "\n",
    "def validate_other_sheet(df):\n",
    "    return (\n",
    "        df.shape[1] == 6\n",
    "        and not is_unnamed(df.columns[1])\n",
    "        and not is_unnamed(df.columns[2])\n",
    "        and not is_unnamed(df.columns[3])\n",
    "        and not is_unnamed(df.columns[4])\n",
    "        and is_unnamed(df.columns[0])\n",
    "    )\n",
    "\n",
    "\n",
    "def load_and_clean_warehouse_file(\n",
    "    warehouse_location_file,\n",
    "    output_csv_small_items_path,\n",
    "    output_csv_large_items_path,\n",
    "    warehouse_unknown_location_path,\n",
    "):\n",
    "    xls = pd.ExcelFile(warehouse_location_file)\n",
    "    relevant_sheets = []\n",
    "    first_sheet_validated = False\n",
    "\n",
    "    for sheet_name in tqdm(xls.sheet_names, desc=\"Validating sheets\"):\n",
    "        df = pd.read_excel(xls, sheet_name=sheet_name, header=0, dtype=str)\n",
    "        if not first_sheet_validated:\n",
    "            if validate_first_sheet(df):\n",
    "                relevant_sheets.append((sheet_name, df))\n",
    "                first_sheet_validated = True\n",
    "            else:\n",
    "                raise ValueError(\"First sheet is not valid\")\n",
    "        else:\n",
    "            if validate_other_sheet(df):\n",
    "                relevant_sheets.append((sheet_name, df))\n",
    "\n",
    "    if not relevant_sheets:\n",
    "        raise ValueError(\"No relevant sheets found in the Excel file\")\n",
    "\n",
    "    # Identify and drop the column that starts with 'mgs'\n",
    "    first_sheet_df = relevant_sheets[0][1]\n",
    "    mgs_column = [col for col in first_sheet_df.columns if col.startswith(\"mgs\")]\n",
    "    if mgs_column:\n",
    "        first_sheet_df = first_sheet_df.drop(columns=mgs_column)\n",
    "    else:\n",
    "        raise ValueError(\"No column starting with 'mgs' found\")\n",
    "\n",
    "    aligned_sheets = [first_sheet_df]\n",
    "    for sheet_name, df in relevant_sheets[1:]:\n",
    "        df.columns = first_sheet_df.columns\n",
    "        aligned_sheets.append(df)\n",
    "\n",
    "    df_combined = pd.concat(aligned_sheets, ignore_index=True)\n",
    "    df_combined = df_combined[~df_combined.iloc[:, 2].isin([\"\", \".\"])]\n",
    "\n",
    "    # Print current csv.\n",
    "    df_combined.to_csv(cleaned_original_csv_path, index=False)\n",
    "\n",
    "    # Drop the 'STAMPA ANAGRAFICA ARTICOLI' column\n",
    "    df_combined = df_combined.drop(columns=[\"STAMPA ANAGRAFICA ARTICOLI\"])\n",
    "\n",
    "    # Check the number of columns before renaming\n",
    "    print(f\"Number of columns before renaming: {len(df_combined.columns)}\")\n",
    "    print(f\"Column names before renaming: {df_combined.columns.tolist()}\")\n",
    "\n",
    "    # Assign column names based on the number of columns\n",
    "    if len(df_combined.columns) == 5:\n",
    "        df_combined.columns = [\n",
    "            \"CODICE PRODOTTO\",\n",
    "            \"MARCA\",\n",
    "            \"DESCRIZIONE\",\n",
    "            \"UBICAZIONE\",\n",
    "            \"GIACENZA\",\n",
    "        ]\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected number of columns: {len(df_combined.columns)}\")\n",
    "\n",
    "    # Ensure specific columns are treated as strings\n",
    "    df_combined[\"CODICE PRODOTTO\"] = df_combined[\"CODICE PRODOTTO\"].astype(str)\n",
    "    df_combined[\"MARCA\"] = df_combined[\"MARCA\"].astype(str)\n",
    "    df_combined[\"DESCRIZIONE\"] = df_combined[\"DESCRIZIONE\"].astype(str)\n",
    "    # Replace NaN and empty strings in 'UBICAZIONE' with \"Location Unknown\"\n",
    "    df_combined[\"UBICAZIONE\"] = (\n",
    "        df_combined[\"UBICAZIONE\"]\n",
    "        .fillna(\"Location Unknown\")\n",
    "        .replace(\"\", \"Location Unknown\")\n",
    "    )\n",
    "\n",
    "    df_combined[\"GIACENZA\"] = pd.to_numeric(\n",
    "        df_combined[\"GIACENZA\"].str.replace(\",\", \".\"), errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    # Define patterns for valid small items and valid large items\n",
    "    small_item_patterns = [\n",
    "        r\"^[A-Ca-c](?:\\.|[0-9])\",  # Existing pattern for 'A.', 'B.', 'C.', etc.\n",
    "        r\"^MAG S\",  # Match 'MAG S'\n",
    "        r\"^SC[0-9]+\",  # Match 'SC' followed by a number\n",
    "        r\"^SCAT(?:[0-9]+|\\s)\",  # Match 'SCAT' followed by a number or space\n",
    "        r\"^SCATOLO(?:[0-9]+|\\s)\",  # Match 'SCATOLO' followed by a number or space\n",
    "        r\"^TOTEM LEMA$\",  # Match 'TOTEM LEMA'\n",
    "        r\"^V-[0-9]+\",  # Match 'V-' followed by a number\n",
    "    ]\n",
    "\n",
    "    large_item_patterns = [\n",
    "        r\"^[D-Zd-z](?:\\.|[0-9])\",\n",
    "        r\"^2\\.\",  # Match '2.' at the beginning\n",
    "        r\"COSPEL\",  # Match any string containing 'COSPEL'\n",
    "        r\"PIAZZALE\",  # Match any string containing 'COSPEL'\n",
    "        r\"MAG\\.ZF\",  # Match 'MAG.ZF'\n",
    "        r\"^MG\\.[0-9]+\",  # Match 'MG.' followed by a number\n",
    "        r\"^MG\\.[A-Za-z]\",  # Match 'MG.' followed by a letter\n",
    "        r\"^Mg\\.[A-Za-z]\",  # Match 'Mg.' followed by a letter\n",
    "        r\"^mg\\.[A-Za-z]\",  # Match 'mg.' followed by a letter\n",
    "        r\"^ZF\\.[A-Za-z]\",  # Match 'ZF.' followed by a letter\n",
    "    ]\n",
    "\n",
    "    # Combine small item patterns into a single regex pattern\n",
    "    valid_small_items_pattern = \"|\".join(small_item_patterns)\n",
    "    valid_small_items = df_combined[\"UBICAZIONE\"].str.match(valid_small_items_pattern)\n",
    "\n",
    "    # Combine large item patterns into a single regex pattern\n",
    "    valid_large_items_pattern = \"|\".join(large_item_patterns)\n",
    "    valid_large_items = ~valid_small_items & df_combined[\"UBICAZIONE\"].str.match(\n",
    "        valid_large_items_pattern\n",
    "    )\n",
    "\n",
    "    # Define unknown items as those that do not match any small or large item patterns\n",
    "    unknown_items = ~df_combined[\"UBICAZIONE\"].str.match(\n",
    "        f\"{valid_small_items_pattern}|{valid_large_items_pattern}\"\n",
    "    )\n",
    "\n",
    "    # Filter the DataFrame based on these patterns\n",
    "    small_items_df = df_combined[valid_small_items]\n",
    "    large_items_df = df_combined[valid_large_items]\n",
    "    unknown_df = df_combined[unknown_items]\n",
    "\n",
    "    # Identify and drop rows where the headers are repeated in the data\n",
    "    unknown_df = unknown_df[unknown_df[\"CODICE PRODOTTO\"] != \"CODICE\"]\n",
    "\n",
    "    # Output the rows with unknown locations\n",
    "    unknown_df.to_csv(warehouse_unknown_location_path, index=False)\n",
    "    print(f\"Unknown items saved to: {warehouse_unknown_location_path}\")\n",
    "\n",
    "    # Output the small and large items\n",
    "    small_items_df.to_csv(output_csv_small_items_path, index=False)\n",
    "    large_items_df.to_csv(output_csv_large_items_path, index=False)\n",
    "    print(f\"Small items saved to: {output_csv_small_items_path}\")\n",
    "    print(f\"Large items saved to: {output_csv_large_items_path}\")\n",
    "\n",
    "    return small_items_df, large_items_df, unknown_df\n",
    "\n",
    "\n",
    "# Paths\n",
    "warehouse_location_file = \"Z:/My Drive/rcs/Web/Tulero/app/data/mgs010_Stampa Anagrafica Articoli al 21-08 con ubi e giac.xls\"\n",
    "output_csv_small_items_path = (\n",
    "    \"Z:/My Drive/rcs/Web/Tulero/app/output/output_csv_small_items_path.csv\"\n",
    ")\n",
    "output_csv_large_items_path = (\n",
    "    \"Z:/My Drive/rcs/Web/Tulero/app/output/output_csv_large_items_path.csv\"\n",
    ")\n",
    "warehouse_unknown_location_path = (\n",
    "    \"Z:/My Drive/rcs/Web/Tulero/app/output/warehouse_unknown_location_path.csv\"\n",
    ")\n",
    "cleaned_original_csv_path = \"Z:/My Drive/rcs/Web/Tulero/app/output/cleaned_original.csv\"\n",
    "\n",
    "# Execute function\n",
    "small_items_df, large_items_df, unknown_df = load_and_clean_warehouse_file(\n",
    "    warehouse_location_file,\n",
    "    output_csv_small_items_path,\n",
    "    output_csv_large_items_path,\n",
    "    warehouse_unknown_location_path,\n",
    ")\n",
    "\n",
    "# View the results\n",
    "print(\"Small Items:\")\n",
    "print(small_items_df.head())\n",
    "\n",
    "print(\"\\nLarge Items:\")\n",
    "print(large_items_df.head())\n",
    "\n",
    "print(\"\\nUnknown Items:\")\n",
    "print(unknown_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "IGNORED_BRANDS = [\n",
    "    \"AP\",\n",
    "    \"AREXONS\",\n",
    "    \"ASSO\",\n",
    "    \"ATE\",\n",
    "    \"ATECSO\",\n",
    "    \"AUTOCLIMA\",\n",
    "    \"BIRTH\",\n",
    "    \"BOSCH\",\n",
    "    \"BREMBO\",\n",
    "    \"BUGATTI\",\n",
    "    \"CASCO\",\n",
    "    \"CASTROL\",\n",
    "    \"CEI\",\n",
    "    \"CORTECO\",\n",
    "    \"COVIND\",\n",
    "    \"DAF\",\n",
    "    \"DAYCO\",\n",
    "    \"DELPHI\",\n",
    "    \"DENSO\",\n",
    "    \"DOLZ\",\n",
    "    \"ELRING\",\n",
    "    \"EMMERRE\",\n",
    "    \"ERA\",\n",
    "    \"EXIDE\",\n",
    "    \"FAG\",\n",
    "    \"FEBI\",\n",
    "    \"FERODO\",\n",
    "    \"FIAT\",\n",
    "    \"FORD\",\n",
    "    \"FRAP\",\n",
    "    \"FTE\",\n",
    "    \"GATES\",\n",
    "    \"HELLA\",\n",
    "    \"HOFFER\",\n",
    "    \"IMASAF\",\n",
    "    \"INA\",\n",
    "    \"ISUZU\",\n",
    "    \"IVECO\",\n",
    "    \"JAPANPARTS\",\n",
    "    \"JAPKO\",\n",
    "    \"KNECHT\",\n",
    "    \"KRIOS\",\n",
    "    \"LEMFORDER\",\n",
    "    \"LUK\",\n",
    "    \"MAHLE\",\n",
    "    \"MAN\",\n",
    "    \"METELLI\",\n",
    "    \"MEYLE\",\n",
    "    \"MOBIL\",\n",
    "    \"MONROE\",\n",
    "    \"MOOG\",\n",
    "    \"MULLER FILTER\",\n",
    "    \"NISSAN\",\n",
    "    \"NISSENS\",\n",
    "    \"NK\",\n",
    "    \"NRF\",\n",
    "    \"OLSA\",\n",
    "    \"OMP\",\n",
    "    \"PEUGEOT\",\n",
    "    \"PIAGGIO\",\n",
    "    \"PIERBURG\",\n",
    "    \"RAICAM\",\n",
    "    \"RENAULT\",\n",
    "    \"SACHS\",\n",
    "    \"SCANIA\",\n",
    "    \"SELENIA\",\n",
    "    \"SIDAT\",\n",
    "    \"SKF\",\n",
    "    \"TEXTAR\",\n",
    "    \"TRW\",\n",
    "    \"TUDOR\",\n",
    "    \"UFI\",\n",
    "    \"VALEO\",\n",
    "    \"VEMA\",\n",
    "    \"VITAL SUSPENSIONS\",\n",
    "    \"VOLVO\",\n",
    "    \"VOLKSWAGEN\",\n",
    "    \"ZETA-ERRE\",\n",
    "    \"ZF\",\n",
    "]\n",
    "\n",
    "\n",
    "def is_unnamed(header):\n",
    "    return str(header).startswith(\"Unnamed\")\n",
    "\n",
    "\n",
    "def validate_first_sheet(df):\n",
    "    return (\n",
    "        is_unnamed(df.columns[1])\n",
    "        and is_unnamed(df.columns[2])\n",
    "        and is_unnamed(df.columns[3])\n",
    "        and is_unnamed(df.columns[4])\n",
    "        and is_unnamed(df.columns[5])\n",
    "        and not pd.isna(df.columns[0])\n",
    "        and not pd.isna(df.columns[6])\n",
    "    )\n",
    "\n",
    "\n",
    "def validate_other_sheet(df):\n",
    "    return (\n",
    "        df.shape[1] == 6\n",
    "        and not is_unnamed(df.columns[1])\n",
    "        and not is_unnamed(df.columns[2])\n",
    "        and not is_unnamed(df.columns[3])\n",
    "        and not is_unnamed(df.columns[4])\n",
    "        and is_unnamed(df.columns[0])\n",
    "    )\n",
    "\n",
    "\n",
    "def get_oem_number(row, oem_lookup, IGNORED_BRANDS):\n",
    "    if row[\"BRAND\"] in IGNORED_BRANDS:\n",
    "        return \"\"  # Skip filling OE code for ignored brands\n",
    "    key = (row[\"CODICE PRODOTTO\"], row[\"BRAND\"][:5])\n",
    "    return \" | \".join(oem_lookup[key]) if key in oem_lookup else \"Unknown OE\"\n",
    "\n",
    "\n",
    "def find_additional_cross_codes(codice_prodotto, padded_oe, cleaned_df, IGNORED_BRANDS):\n",
    "    matches = cleaned_df[\n",
    "        (cleaned_df[\"CODICE OE\"] != \"Unknown OE\")\n",
    "        & (~cleaned_df[\"BRAND\"].isin(IGNORED_BRANDS))\n",
    "    ]\n",
    "    exact_matches = matches[\n",
    "        matches[\"padded_oe\"].str.contains(f\" {codice_prodotto} \", regex=False)\n",
    "    ]\n",
    "    if not exact_matches.empty:\n",
    "        return \" | \".join(exact_matches[\"CODICE PRODOTTO\"].unique())\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def update_brands(df_output, brands_file_path):\n",
    "    # Load the brands file\n",
    "    brands_df = pd.read_csv(brands_file_path, dtype=str)\n",
    "    brands_df[\"Brand\"] = brands_df[\"Brand\"].astype(str).str.strip()\n",
    "    brands_df[\"Match\"] = brands_df[\"Match\"].astype(str).str.strip()\n",
    "\n",
    "    # Create a lookup dictionary from the brands file\n",
    "    brand_lookup = dict(zip(brands_df[\"Brand\"], brands_df[\"Match\"]))\n",
    "\n",
    "    # Update the BRANDS in the output dataframe\n",
    "    tqdm.pandas(desc=\"Updating brands\")\n",
    "    df_output[\"BRAND\"] = df_output[\"BRAND\"].progress_apply(\n",
    "        lambda x: brand_lookup.get(x, x)\n",
    "    )\n",
    "    return df_output\n",
    "\n",
    "\n",
    "def optimized_cross_code_generation(cleaned_df, ignored_brands):\n",
    "    cross_references = {}\n",
    "    for _, row in tqdm(\n",
    "        cleaned_df.iterrows(), total=len(cleaned_df), desc=\"Generating cross codes\"\n",
    "    ):\n",
    "        codice_oe = row[\"CODICE OE\"]\n",
    "        codice_prodotto = row[\"CODICE PRODOTTO\"]\n",
    "        brand = row[\"BRAND\"]\n",
    "\n",
    "        if codice_oe != \"Unknown OE\" and brand not in ignored_brands:\n",
    "            if codice_oe not in cross_references:\n",
    "                cross_references[codice_oe] = []\n",
    "            cross_references[codice_oe].append(codice_prodotto)\n",
    "\n",
    "    cross_codes_series = pd.Series(index=cleaned_df.index, dtype=str)\n",
    "    for codice_oe, prodotti in tqdm(\n",
    "        cross_references.items(), desc=\"Updating CODICI CROSS\"\n",
    "    ):\n",
    "        cross_codes = {\n",
    "            prodotto: \" | \".join([code for code in prodotti if code != prodotto])\n",
    "            for prodotto in prodotti\n",
    "        }\n",
    "        cross_codes_series.update(pd.Series(cross_codes))\n",
    "\n",
    "    return cross_codes_series.fillna(\"\")\n",
    "\n",
    "\n",
    "def load_and_clean_warehouse_file(warehouse_location_file):\n",
    "    xls = pd.ExcelFile(warehouse_location_file)\n",
    "    relevant_sheets = []\n",
    "    first_sheet_validated = False\n",
    "\n",
    "    for sheet_name in tqdm(xls.sheet_names, desc=\"Validating sheets\"):\n",
    "        df = pd.read_excel(xls, sheet_name=sheet_name, header=0, dtype=str)\n",
    "        if not first_sheet_validated:\n",
    "            if validate_first_sheet(df):\n",
    "                relevant_sheets.append((sheet_name, df))\n",
    "                first_sheet_validated = True\n",
    "            else:\n",
    "                raise ValueError(\"First sheet is not valid\")\n",
    "        else:\n",
    "            if validate_other_sheet(df):\n",
    "                relevant_sheets.append((sheet_name, df))\n",
    "\n",
    "    if not relevant_sheets:\n",
    "        raise ValueError(\"No relevant sheets found in the Excel file\")\n",
    "\n",
    "    # Identify and drop the column that starts with 'mgs'\n",
    "    first_sheet_df = relevant_sheets[0][1]\n",
    "    mgs_column = [col for col in first_sheet_df.columns if col.startswith(\"mgs\")]\n",
    "    if mgs_column:\n",
    "        first_sheet_df = first_sheet_df.drop(columns=mgs_column)\n",
    "\n",
    "    aligned_sheets = [first_sheet_df]\n",
    "    for sheet_name, df in relevant_sheets[1:]:\n",
    "        df.columns = first_sheet_df.columns\n",
    "        aligned_sheets.append(df)\n",
    "\n",
    "    df_combined = pd.concat(aligned_sheets, ignore_index=True)\n",
    "    df_combined = df_combined[~df_combined.iloc[:, 2].isin([\"\", \".\"])]\n",
    "\n",
    "    # Drop unnecessary column\n",
    "    df_combined = df_combined.drop(columns=[\"STAMPA ANAGRAFICA ARTICOLI\"])\n",
    "\n",
    "    # Assign column names based on the number of columns\n",
    "    if len(df_combined.columns) == 5:\n",
    "        df_combined.columns = [\n",
    "            \"CODICE PRODOTTO\",\n",
    "            \"MARCA\",\n",
    "            \"DESCRIZIONE\",\n",
    "            \"UBICAZIONE\",\n",
    "            \"GIACENZA\",\n",
    "        ]\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected number of columns: {len(df_combined.columns)}\")\n",
    "\n",
    "    # Ensure specific columns are treated as strings\n",
    "    df_combined[\"CODICE PRODOTTO\"] = df_combined[\"CODICE PRODOTTO\"].astype(str)\n",
    "    df_combined[\"MARCA\"] = df_combined[\"MARCA\"].astype(str)\n",
    "    df_combined[\"DESCRIZIONE\"] = df_combined[\"DESCRIZIONE\"].astype(str)\n",
    "    df_combined[\"UBICAZIONE\"] = (\n",
    "        df_combined[\"UBICAZIONE\"]\n",
    "        .fillna(\"Location Unknown\")\n",
    "        .replace(\"\", \"Location Unknown\")\n",
    "    )\n",
    "    df_combined[\"GIACENZA\"] = pd.to_numeric(\n",
    "        df_combined[\"GIACENZA\"].str.replace(\",\", \".\"), errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    # Keep only small items (A, B, C)\n",
    "    pattern = r\"^[A-Ca-c](?:\\.|[0-9])\"\n",
    "    valid_small_items = df_combined[\"UBICAZIONE\"].str.match(pattern)\n",
    "    df_combined = df_combined[valid_small_items]\n",
    "\n",
    "    return df_combined\n",
    "\n",
    "\n",
    "def process_files(\n",
    "    articles_file_path,\n",
    "    output_file_path,\n",
    "    old_oems_folder,\n",
    "    brands_file_path,\n",
    "    warehouse_location_file,\n",
    "    final_output_with_brands_path,\n",
    "    update_progress=None,\n",
    "):\n",
    "    # Load and clean the warehouse file\n",
    "    warehouse_df = load_and_clean_warehouse_file(warehouse_location_file)\n",
    "\n",
    "    # Load and clean the articles file\n",
    "\n",
    "    xls = pd.ExcelFile(articles_file_path)\n",
    "    relevant_sheets = []\n",
    "    first_sheet_validated = False\n",
    "\n",
    "    for sheet_name in tqdm(xls.sheet_names, desc=\"Validating sheets\"):\n",
    "        df = pd.read_excel(\n",
    "            xls, sheet_name=sheet_name, header=0, dtype=str\n",
    "        )  # Ensure all columns are read as strings\n",
    "        if not first_sheet_validated:\n",
    "            if validate_first_sheet(df):\n",
    "                relevant_sheets.append((sheet_name, df))\n",
    "                first_sheet_validated = True\n",
    "            else:\n",
    "                raise ValueError(\"First sheet is not valid\")\n",
    "        else:\n",
    "            if validate_other_sheet(df):\n",
    "                relevant_sheets.append((sheet_name, df))\n",
    "\n",
    "    if not relevant_sheets:\n",
    "        raise ValueError(\"No relevant sheets found in the Excel file\")\n",
    "\n",
    "    first_sheet_df = relevant_sheets[0][1]\n",
    "    mgs_column = [col for col in first_sheet_df.columns if col.startswith(\"mgs\")]\n",
    "    if mgs_column:\n",
    "        first_sheet_df = first_sheet_df.drop(columns=mgs_column)\n",
    "\n",
    "    aligned_sheets = [first_sheet_df]\n",
    "    for sheet_name, df in relevant_sheets[1:]:\n",
    "        df.columns = first_sheet_df.columns\n",
    "        aligned_sheets.append(df)\n",
    "\n",
    "    df_combined = pd.concat(aligned_sheets, ignore_index=True)\n",
    "    df_combined = df_combined[~df_combined.iloc[:, 2].isin([\"\", \".\"])]\n",
    "    df_combined = df_combined.drop(columns=[\"STAMPA LISTINI\"])\n",
    "\n",
    "    df_combined.columns = [\n",
    "        \"CODICE PRODOTTO\",\n",
    "        \"BRAND\",\n",
    "        \"DESCRIZIONE\",\n",
    "        \"GIACENZA\",\n",
    "        \"PRZ. ULT. ACQ.\",\n",
    "    ]\n",
    "\n",
    "    for col in [\n",
    "        \"CODICE OE\",\n",
    "        \"CODICI CROSS\",\n",
    "        \"LINK IMMAGINE\",\n",
    "        \"CATEGORIA\",\n",
    "        \"SCHEDA TECNICA\",\n",
    "        \"SCHEDA DI SICUREZZA\",\n",
    "        \"CONFEZIONE\",\n",
    "        \"QUANTITÀ MINIMA\",\n",
    "        \"META.LUNGHEZZA\",\n",
    "        \"META.LARGHEZZA\",\n",
    "        \"META.PROFONDITA'\",\n",
    "        \"META. ...\",\n",
    "        \"UBICAZIONE\",\n",
    "    ]:\n",
    "        df_combined[col] = pd.NA\n",
    "\n",
    "    # Ensure specified columns are treated as strings\n",
    "    columns_to_ensure_as_strings = [\n",
    "        \"CODICE PRODOTTO\",\n",
    "        \"BRAND\",\n",
    "        \"DESCRIZIONE\",\n",
    "        \"CODICE OE\",\n",
    "        \"CODICI CROSS\",\n",
    "        \"LINK IMMAGINE\",\n",
    "        \"CATEGORIA\",\n",
    "        \"SCHEDA TECNICA\",\n",
    "        \"SCHEDA DI SICUREZZA\",\n",
    "        \"CONFEZIONE\",\n",
    "        \"META.LUNGHEZZA\",\n",
    "        \"META.LARGHEZZA\",\n",
    "        \"META.PROFONDITA'\",\n",
    "        \"META. ...\",\n",
    "        \"UBICAZIONE\",\n",
    "    ]\n",
    "    df_combined[columns_to_ensure_as_strings] = df_combined[\n",
    "        columns_to_ensure_as_strings\n",
    "    ].astype(str)\n",
    "\n",
    "    df_combined[\"GIACENZA\"] = pd.to_numeric(\n",
    "        df_combined[\"GIACENZA\"].str.replace(\",\", \".\"), errors=\"coerce\"\n",
    "    )\n",
    "    df_combined[\"PRZ. ULT. ACQ.\"] = pd.to_numeric(\n",
    "        df_combined[\"PRZ. ULT. ACQ.\"].str.replace(\",\", \".\"), errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    df_combined = df_combined[df_combined[\"GIACENZA\"] > 0]\n",
    "    df_combined = df_combined[df_combined[\"PRZ. ULT. ACQ.\"].notna()]\n",
    "\n",
    "    # Merge with warehouse data\n",
    "    df_combined = df_combined.merge(\n",
    "        warehouse_df[[\"CODICE PRODOTTO\", \"MARCA\", \"UBICAZIONE\"]],\n",
    "        left_on=[\"CODICE PRODOTTO\", \"BRAND\"],\n",
    "        right_on=[\"CODICE PRODOTTO\", \"MARCA\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    df_combined = df_combined.drop(columns=[\"MARCA\"])\n",
    "\n",
    "    # Assign shipping category based on UBICAZIONE\n",
    "    df_combined[\"SHIPPING_CAT\"] = df_combined[\"UBICAZIONE\"].apply(\n",
    "        lambda x: \"9.90\" if pd.notna(x) and re.match(r\"^[A-Ca-c]\", x) else \"UNKNOWN\"\n",
    "    )\n",
    "\n",
    "    # Drop the UBICAZIONE column since it's no longer needed\n",
    "    df_combined = df_combined.drop(columns=[\"UBICAZIONE\"])\n",
    "\n",
    "    # No need to save and reload, continue with the DataFrame in memory\n",
    "    cleaned_df = df_combined.copy()\n",
    "    cleaned_df[\"CODICE PRODOTTO\"] = cleaned_df[\"CODICE PRODOTTO\"].str.strip()\n",
    "    cleaned_df[\"CODICE OE\"] = cleaned_df[\"CODICE OE\"].str.strip()\n",
    "    cleaned_df[\"BRAND\"] = cleaned_df[\"BRAND\"].str.strip()\n",
    "\n",
    "    old_oems_files = [\n",
    "        file\n",
    "        for file in os.listdir(old_oems_folder)\n",
    "        if file.startswith(\"oemsDC\") and file.endswith(\".csv\")\n",
    "    ]\n",
    "    all_oem_mappings = pd.DataFrame()\n",
    "\n",
    "    for file_name in tqdm(old_oems_files, desc=\"Building old OEM mappings\"):\n",
    "        file_path = os.path.join(old_oems_folder, file_name)\n",
    "        oems_df = pd.read_csv(file_path, dtype=str)\n",
    "        oems_df[\"article_altc\"] = oems_df[\"article_altc\"].astype(str).str.strip()\n",
    "        oems_df[\"oem_number\"] = (\n",
    "            oems_df[\"oem_number\"].astype(str).str.strip().str.replace(\" \", \"\")\n",
    "        )\n",
    "        oems_df[\"article_alt_brands\"] = (\n",
    "            oems_df[\"article_alt_brands\"].astype(str).str.strip()\n",
    "        )\n",
    "        oems_df[\"brand_prefix\"] = oems_df[\"article_alt_brands\"].str[:5]\n",
    "        all_oem_mappings = pd.concat(\n",
    "            [all_oem_mappings, oems_df[[\"article_altc\", \"oem_number\", \"brand_prefix\"]]]\n",
    "        )\n",
    "\n",
    "    oem_lookup = (\n",
    "        all_oem_mappings.groupby([\"article_altc\", \"brand_prefix\"])[\"oem_number\"]\n",
    "        .apply(list)\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "    # Update CODICE OE using the get_oem_number function\n",
    "    tqdm.pandas(desc=\"Updating CODICE OE with old OEMs\")\n",
    "    cleaned_df[\"CODICE OE\"] = cleaned_df.progress_apply(\n",
    "        lambda row: get_oem_number(row, oem_lookup, IGNORED_BRANDS), axis=1\n",
    "    )\n",
    "\n",
    "    # Update PREZZO based on PRZ. ULT. ACQ.\n",
    "    cleaned_df[\"PRZ. ULT. ACQ.\"] = pd.to_numeric(\n",
    "        cleaned_df[\"PRZ. ULT. ACQ.\"], errors=\"coerce\"\n",
    "    )\n",
    "    cleaned_df[\"PREZZO\"] = cleaned_df[\"PRZ. ULT. ACQ.\"].apply(\n",
    "        lambda x: round(x * 1.25, 2) if pd.notnull(x) else x\n",
    "    )\n",
    "    cleaned_df[\"SHIPPING_CAT\"] = pd.to_numeric(\n",
    "        cleaned_df[\"SHIPPING_CAT\"], errors=\"coerce\"\n",
    "    )\n",
    "    cleaned_df = cleaned_df.drop(columns=[\"PRZ. ULT. ACQ.\"])\n",
    "\n",
    "    # Ensure specified columns are treated as strings before reordering\n",
    "    cleaned_df[columns_to_ensure_as_strings] = cleaned_df[\n",
    "        columns_to_ensure_as_strings\n",
    "    ].astype(str)\n",
    "\n",
    "    # Reorder columns\n",
    "    columns_order = [\n",
    "        \"CODICE PRODOTTO\",\n",
    "        \"CODICE OE\",\n",
    "        \"CODICI CROSS\",\n",
    "        \"BRAND\",\n",
    "        \"DESCRIZIONE\",\n",
    "        \"LINK IMMAGINE\",\n",
    "        \"CATEGORIA\",\n",
    "        \"GIACENZA\",\n",
    "        \"PREZZO\",\n",
    "        \"SCHEDA TECNICA\",\n",
    "        \"SCHEDA DI SICUREZZA\",\n",
    "        \"CONFEZIONE\",\n",
    "        \"QUANTITÀ MINIMA\",\n",
    "        \"SHIPPING_CAT\",  # Include SHIPPING_CAT\n",
    "        \"META.LUNGHEZZA\",\n",
    "        \"META.LARGHEZZA\",\n",
    "        \"META.PROFONDITA'\",\n",
    "        \"META. ...\",\n",
    "    ]\n",
    "    cleaned_df = cleaned_df[columns_order]\n",
    "    cleaned_df[\"CODICI CROSS\"] = \"\"\n",
    "\n",
    "    # Apply optimized cross-code generation function\n",
    "    cleaned_df[\"CODICI CROSS\"] = optimized_cross_code_generation(\n",
    "        cleaned_df, IGNORED_BRANDS\n",
    "    )\n",
    "\n",
    "    # Handle cases where CODICE OE is unknown and brand is not ignored\n",
    "    cleaned_df[\"padded_oe\"] = \" \" + cleaned_df[\"CODICE OE\"].str.strip() + \" \"\n",
    "    unknown_oe_mask = (cleaned_df[\"CODICE OE\"] == \"Unknown OE\") & (\n",
    "        ~cleaned_df[\"BRAND\"].isin(IGNORED_BRANDS)\n",
    "    )\n",
    "    cleaned_df.loc[unknown_oe_mask, \"CODICI CROSS\"] = cleaned_df.loc[\n",
    "        unknown_oe_mask, \"CODICE PRODOTTO\"\n",
    "    ].progress_apply(\n",
    "        lambda codice_prodotto: find_additional_cross_codes(\n",
    "            codice_prodotto, cleaned_df[\"padded_oe\"], cleaned_df, IGNORED_BRANDS\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Drop the 'padded_oe' column\n",
    "    cleaned_df = cleaned_df.drop(columns=[\"padded_oe\"])\n",
    "\n",
    "    # Fill the \"CONFEZIONE\" column with \"1 pz\" and the \"QUANTITÀ MINIMA\" column with \"1\"\n",
    "    cleaned_df[\"CONFEZIONE\"] = \"1 pz\"\n",
    "    cleaned_df[\"QUANTITÀ MINIMA\"] = \"1\"\n",
    "\n",
    "    # Set CATEGORIA to \"Ricambio\"\n",
    "    cleaned_df[\"CATEGORIA\"] = \"Ricambio\"\n",
    "\n",
    "    # Ensure specified columns remain empty\n",
    "    columns_to_keep_empty = [\n",
    "        \"LINK IMMAGINE\",\n",
    "        \"SCHEDA TECNICA\",\n",
    "        \"SCHEDA DI SICUREZZA\",\n",
    "        \"META.LUNGHEZZA\",\n",
    "        \"META.LARGHEZZA\",\n",
    "        \"META.PROFONDITA'\",\n",
    "        \"META. ...\",\n",
    "    ]\n",
    "    cleaned_df[columns_to_keep_empty] = \"\"\n",
    "\n",
    "    # Step 4: Update brands\n",
    "    cleaned_df = update_brands(cleaned_df, brands_file_path)\n",
    "\n",
    "    # After updating brands, set CODICE OE and CODICI CROSS to empty for ignored brands\n",
    "    ignored_brands_mask = cleaned_df[\"BRAND\"].isin(IGNORED_BRANDS)\n",
    "    cleaned_df.loc[ignored_brands_mask, [\"CODICE OE\", \"CODICI CROSS\"]] = \"\"\n",
    "\n",
    "    # Save the final output directly\n",
    "    cleaned_df.to_csv(final_output_with_brands_path, index=False)\n",
    "    print(f\"Brands updated and final file saved at {final_output_with_brands_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
