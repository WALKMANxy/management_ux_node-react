{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openpyxl  # noqa: F401\n",
    "\n",
    "def is_unnamed(header):\n",
    "    return str(header).startswith(\"Unnamed\")\n",
    "\n",
    "def validate_first_sheet(df):\n",
    "    return (\n",
    "        is_unnamed(df.columns[1])\n",
    "        and is_unnamed(df.columns[2])\n",
    "        and is_unnamed(df.columns[3])\n",
    "        and is_unnamed(df.columns[4])\n",
    "        and is_unnamed(df.columns[5])\n",
    "        and not pd.isna(df.columns[0])\n",
    "        and not pd.isna(df.columns[6])\n",
    "    )\n",
    "\n",
    "def validate_other_sheet(df):\n",
    "    return (\n",
    "        df.shape[1] == 6\n",
    "        and not is_unnamed(df.columns[1])\n",
    "        and not is_unnamed(df.columns[2])\n",
    "        and not is_unnamed(df.columns[3])\n",
    "        and not is_unnamed(df.columns[4])\n",
    "        and is_unnamed(df.columns[0])\n",
    "    )\n",
    "\n",
    "def process_files(articles_file_path, output_file_path):\n",
    "    # Load the Excel file\n",
    "    xls = pd.ExcelFile(articles_file_path)\n",
    "\n",
    "    # Validate and load relevant sheets\n",
    "    relevant_sheets = []\n",
    "    first_sheet_validated = False\n",
    "\n",
    "    for sheet_name in xls.sheet_names:\n",
    "        df = pd.read_excel(xls, sheet_name=sheet_name, header=0)\n",
    "        if not first_sheet_validated:\n",
    "            if validate_first_sheet(df):\n",
    "                relevant_sheets.append((sheet_name, df))\n",
    "                first_sheet_validated = True\n",
    "            else:\n",
    "                raise ValueError(\"First sheet is not valid\")\n",
    "        else:\n",
    "            if validate_other_sheet(df):\n",
    "                relevant_sheets.append((sheet_name, df))\n",
    "\n",
    "    if not relevant_sheets:\n",
    "        raise ValueError(\"No relevant sheets found in the Excel file\")\n",
    "\n",
    "    # Process the first sheet\n",
    "    first_sheet_df = relevant_sheets[0][1].drop(columns=[\"mgs210\"])\n",
    "\n",
    "    # Align and merge other relevant sheets\n",
    "    aligned_sheets = [first_sheet_df]\n",
    "    for sheet_name, df in relevant_sheets[1:]:\n",
    "        df.columns = first_sheet_df.columns\n",
    "        aligned_sheets.append(df)\n",
    "\n",
    "    # Concatenate all aligned sheets\n",
    "    df_combined = pd.concat(aligned_sheets, ignore_index=True)\n",
    "\n",
    "    # Drop rows where the third column (index 2) is empty or \".\"\n",
    "    df_combined = df_combined[~df_combined.iloc[:, 2].isin([\"\", \".\"])]\n",
    "    \n",
    "    # Drop the \"STAMPA LISTINI\" column\n",
    "    df_combined = df_combined.drop(columns=[\"STAMPA LISTINI\"])\n",
    "    \n",
    "    # Print the number of columns for debugging\n",
    "    print(\"Number of columns in df_combined:\", len(df_combined.columns))\n",
    "    print(\"Column names in df_combined:\", df_combined.columns.tolist())\n",
    "\n",
    "    # Rename the columns\n",
    "    df_combined.columns = [\n",
    "        \"CODICE PRODOTTO\",\n",
    "        \"BRAND\",\n",
    "        \"DESCRIZIONE\",\n",
    "        \"GIACENZA\",\n",
    "        \"PRZ. ULT. ACQ.\",\n",
    "    ]\n",
    "\n",
    "    # Add new columns\n",
    "    df_combined[\"CODICE OE\"] = pd.NA\n",
    "    df_combined[\"CODICI CROSS\"] = pd.NA\n",
    "    df_combined[\"LINK IMMAGINE\"] = pd.NA\n",
    "    df_combined[\"CATEGORIA\"] = pd.NA\n",
    "    df_combined[\"SCHEDA TECNICA\"] = pd.NA\n",
    "    df_combined[\"SCHEDA DI SICUREZZA\"] = pd.NA\n",
    "    df_combined[\"CONFEZIONE\"] = pd.NA\n",
    "    df_combined[\"QUANTITÀ MINIMA\"] = pd.NA\n",
    "    df_combined[\"META.LUNGHEZZA\"] = pd.NA\n",
    "    df_combined[\"META.LARGHEZZA\"] = pd.NA\n",
    "    df_combined[\"META.PROFONDITA'\"] = pd.NA\n",
    "    df_combined[\"META. ...\"] = pd.NA\n",
    "\n",
    "    # Reorder the columns\n",
    "    df_combined = df_combined[\n",
    "        [\n",
    "            \"CODICE PRODOTTO\",\n",
    "            \"CODICE OE\",\n",
    "            \"CODICI CROSS\",\n",
    "            \"BRAND\",\n",
    "            \"DESCRIZIONE\",\n",
    "            \"LINK IMMAGINE\",\n",
    "            \"CATEGORIA\",\n",
    "            \"PRZ. ULT. ACQ.\",\n",
    "            \"GIACENZA\",\n",
    "            \"SCHEDA TECNICA\",\n",
    "            \"SCHEDA DI SICUREZZA\",\n",
    "            \"CONFEZIONE\",\n",
    "            \"QUANTITÀ MINIMA\",\n",
    "            \"META.LUNGHEZZA\",\n",
    "            \"META.LARGHEZZA\",\n",
    "            \"META.PROFONDITA'\",\n",
    "            \"META. ...\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    # Convert 'GIACENZA' and 'PRZ. ULT. ACQ.' columns to numeric types\n",
    "    df_combined[\"GIACENZA\"] = pd.to_numeric(\n",
    "        df_combined[\"GIACENZA\"].str.replace(\",\", \".\"), errors=\"coerce\"\n",
    "    )\n",
    "    df_combined[\"PRZ. ULT. ACQ.\"] = pd.to_numeric(\n",
    "        df_combined[\"PRZ. ULT. ACQ.\"].str.replace(\",\", \".\"), errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    # Clean the data\n",
    "    df_combined = df_combined[df_combined[\"GIACENZA\"] > 0]\n",
    "    df_combined = df_combined[df_combined[\"PRZ. ULT. ACQ.\"].notna()]\n",
    "\n",
    "    # Save the cleaned data to a CSV file\n",
    "    df_combined.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Placeholders for file paths\n",
    "articles_file_path = \"Z:/My Drive/rcs/oem cross/Articles26062024.xls\"\n",
    "output_file_path = \"Z:/My Drive/rcs/oem cross/cleaned_Articles26062024.csv\"\n",
    "\n",
    "process_files(articles_file_path, output_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#MERGE OE CODES, BY APPENDING THEM#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def aggregate_oem_numbers(oems_df):\n",
    "    # Ensure 'oem_number' and 'article_alt' columns are treated as strings\n",
    "    oems_df['oem_number'] = oems_df['oem_number'].astype(str)\n",
    "    oems_df['article_alt'] = oems_df['article_alt'].astype(str)\n",
    "    \n",
    "    # Aggregate OEM numbers for each article_alt\n",
    "    oems_agg = oems_df.groupby('article_alt')['oem_number'].apply(lambda x: ' | '.join(x)).reset_index()\n",
    "    return oems_agg\n",
    "\n",
    "def append_oem_numbers(df_cleaned, oems_agg):\n",
    "    # Ensure 'CODICE PRODOTTO' is treated as a string\n",
    "    df_cleaned['CODICE PRODOTTO'] = df_cleaned['CODICE PRODOTTO'].astype(str)\n",
    "\n",
    "    # Strip leading/trailing spaces and convert to uppercase for both columns\n",
    "    df_cleaned['CODICE PRODOTTO'] = df_cleaned['CODICE PRODOTTO'].str.strip().str.upper()\n",
    "    oems_agg['article_alt'] = oems_agg['article_alt'].str.strip().str.upper()\n",
    "\n",
    "    # Merge the cleaned dataframe with the aggregated OEM numbers\n",
    "    merged_df = pd.merge(df_cleaned, oems_agg, left_on='CODICE PRODOTTO', right_on='article_alt', how='left')\n",
    "\n",
    "    # Rename the column for clarity\n",
    "    merged_df.rename(columns={'oem_number': 'CODICE OE'}, inplace=True)\n",
    "\n",
    "    # Drop the article_alt column as it's no longer needed\n",
    "    merged_df.drop(columns=['article_alt'], inplace=True)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "def process_and_merge_files(cleaned_csv_path, oems_file_path, output_file_path):\n",
    "    # Load the cleaned articles CSV file\n",
    "    df_cleaned = pd.read_csv(cleaned_csv_path)\n",
    "\n",
    "    # Load the OEM CSV file with specified delimiter and as string type\n",
    "    oems_df = pd.read_csv(oems_file_path, delimiter=';', dtype=str)\n",
    "\n",
    "    # Aggregate the OEM numbers\n",
    "    oems_agg = aggregate_oem_numbers(oems_df)\n",
    "\n",
    "    # Append OEM numbers to the cleaned dataframe\n",
    "    df_result = append_oem_numbers(df_cleaned, oems_agg)\n",
    "\n",
    "    # Save the final dataframe to a CSV file\n",
    "    df_result.to_csv(output_file_path, index=False)\n",
    "\n",
    "    # Print some sample values for debugging\n",
    "    print(\"Sample CODICE PRODOTTO values:\")\n",
    "    print(df_cleaned['CODICE PRODOTTO'].head(10))  # Print more rows for inspection\n",
    "    print(\"Sample article_alt values:\")\n",
    "    print(oems_agg['article_alt'].head(10))  # Print more rows for inspection\n",
    "    print(\"Sample merged CODICE OE values:\")\n",
    "    print(df_result[['CODICE PRODOTTO', 'CODICE OE']].head(10))  # Print more rows for inspection\n",
    "\n",
    "    # Print unique values in both columns for further inspection\n",
    "    print(\"Unique CODICE PRODOTTO values:\")\n",
    "    print(df_cleaned['CODICE PRODOTTO'].unique()[:10])  # Print more unique values\n",
    "    print(\"Unique article_alt values:\")\n",
    "    print(oems_agg['article_alt'].unique()[:10])  # Print more unique values\n",
    "\n",
    "# Placeholders for file paths\n",
    "articles_file_path = \"Z:/My Drive/rcs/oem cross/cleaned_Articles26062024.csv\"\n",
    "oems_file_path = \"Z:/My Drive/rcs/oem cross/oems.csv\"\n",
    "output_file_path = \"Z:/My Drive/rcs/oem cross/OEMERGED_cleaned_Articles26062024.csv\"\n",
    "\n",
    "# Run the process\n",
    "process_and_merge_files(articles_file_path, oems_file_path, output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#SPLIT CSV FROM DT#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def split_csv(input_csv_path, output_folder, output_csv_prefix, rows_per_file=1048575):\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Count the total number of rows in the original CSV file\n",
    "    total_rows = sum(1 for row in open(input_csv_path, 'r')) - 1  # Subtract 1 for header row\n",
    "    \n",
    "    # Read the large CSV file in chunks\n",
    "    chunk_iter = pd.read_csv(input_csv_path, chunksize=rows_per_file, delimiter=';', dtype=str)\n",
    "\n",
    "    # Initialize file counter\n",
    "    file_number = 1\n",
    "    total_split_rows = 0\n",
    "\n",
    "    for chunk in tqdm(chunk_iter, desc=\"Processing chunks\"):\n",
    "        # Drop columns at indices 5, 6, 7, and 8\n",
    "        chunk.drop(chunk.columns[[0, 4, 5, 6, 7]], axis=1, inplace=True)\n",
    "        \n",
    "        # Count the rows in the current chunk\n",
    "        chunk_rows = chunk.shape[0]\n",
    "        total_split_rows += chunk_rows\n",
    "        \n",
    "        # Generate output file name with folder path\n",
    "        output_csv_path = os.path.join(output_folder, f\"{output_csv_prefix}_{file_number}.csv\")\n",
    "        \n",
    "        # Write chunk to a new CSV file\n",
    "        chunk.to_csv(output_csv_path, index=False)\n",
    "        \n",
    "        # Increment file counter\n",
    "        file_number += 1\n",
    "        \n",
    "    # Compare the total rows\n",
    "    comparison_result = \"Match\" if total_rows == total_split_rows else \"Mismatch\"\n",
    "    \n",
    "     # Write the results to a text file\n",
    "    with open(os.path.join(output_folder, \"row_count_comparison.txt\"), \"w\") as f:\n",
    "        f.write(f\"Total rows in original CSV: {total_rows}\\n\")\n",
    "        f.write(f\"Total rows in split CSVs: {total_split_rows}\\n\")\n",
    "        f.write(f\"Comparison result: {comparison_result}\\n\")        \n",
    "\n",
    "\n",
    "# Placeholders for file paths\n",
    "input_csv_path = \"Z:/My Drive/rcs/oem cross/oems.csv\"\n",
    "output_folder = \"Z:/My Drive/rcs/oem cross/oemssplit/\"\n",
    "output_csv_prefix = \"oemsDT\"\n",
    "\n",
    "# Run the split function\n",
    "split_csv(input_csv_path, output_folder, output_csv_prefix, rows_per_file=1048575)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#SPLIT CSV FROM DC, BUT BRANDS UNIFIED WITH PRODUCT IDS#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def split_csv(\n",
    "    input_csv_path,\n",
    "    output_folder,\n",
    "    output_csv_prefix,\n",
    "    rows_per_file=1048575,\n",
    "    encoding=\"utf-8\",\n",
    "):\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Count the total number of rows in the original CSV file\n",
    "    with open(input_csv_path, \"r\", encoding=encoding) as f:\n",
    "        total_rows = sum(1 for row in f) - 1  # Subtract 1 for header row\n",
    "\n",
    "    # Read the large CSV file in chunks\n",
    "    chunk_iter = pd.read_csv(\n",
    "        input_csv_path,\n",
    "        chunksize=rows_per_file,\n",
    "        delimiter=\";\",\n",
    "        dtype=str,\n",
    "        encoding=encoding,\n",
    "    )\n",
    "\n",
    "    # Initialize file counter\n",
    "    file_number = 1\n",
    "    total_split_rows = 0\n",
    "\n",
    "    for chunk in tqdm(chunk_iter, desc=\"Processing chunks\"):\n",
    "        # Drop columns at indices 5, 6, 7, and 8\n",
    "        chunk.drop(chunk.columns[[0, 4, 5, 6, 7]], axis=1, inplace=True)\n",
    "\n",
    "        # Count the rows in the current chunk\n",
    "        chunk_rows = chunk.shape[0]\n",
    "        total_split_rows += chunk_rows\n",
    "\n",
    "        # Generate output file name with folder path\n",
    "        output_csv_path = os.path.join(\n",
    "            output_folder, f\"{output_csv_prefix}_{file_number}.csv\"\n",
    "        )\n",
    "\n",
    "        # Write chunk to a new CSV file\n",
    "        chunk.to_csv(output_csv_path, index=False)\n",
    "\n",
    "        # Increment file counter\n",
    "        file_number += 1\n",
    "\n",
    "    # Compare the total rows\n",
    "    comparison_result = \"Match\" if total_rows == total_split_rows else \"Mismatch\"\n",
    "\n",
    "    # Write the results to a text file\n",
    "    with open(os.path.join(output_folder, \"row_count_comparison.txt\"), \"w\") as f:\n",
    "        f.write(f\"Total rows in original CSV: {total_rows}\\n\")\n",
    "        f.write(f\"Total rows in split CSVs: {total_split_rows}\\n\")\n",
    "        f.write(f\"Comparison result: {comparison_result}\\n\")\n",
    "\n",
    "\n",
    "# Placeholders for file paths\n",
    "input_csv_path = \"Z:/My Drive/rcs/oem cross/oems_id.csv\"\n",
    "output_folder = \"Z:/My Drive/rcs/oem cross/oemssplit/\"\n",
    "output_csv_prefix = \"oemsDC\"\n",
    "\n",
    "# Run the split function\n",
    "split_csv(\n",
    "    input_csv_path,\n",
    "    output_folder,\n",
    "    output_csv_prefix,\n",
    "    rows_per_file=1048575,\n",
    "    encoding=\"utf-8\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#IDENTIFY BRANDS WITH MULTIPLE SPACES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def identify_brands_with_spaces(input_folder, output_folder, brands_with_spaces_file, chunksize=100000):\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Initialize list for brands with spaces\n",
    "    brands_with_spaces = []\n",
    "\n",
    "    # List all CSV files in the input folder\n",
    "    csv_files = [file for file in os.listdir(input_folder) if file.endswith(\".csv\")]\n",
    "\n",
    "    # Iterate over all files in the input folder\n",
    "    for file_name in tqdm(csv_files, desc=\"Identifying brands with spaces\"):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        \n",
    "        # Process the file in chunks\n",
    "        for chunk in pd.read_csv(file_path, dtype=str, chunksize=chunksize):\n",
    "            # Ensure all values in the first column are strings\n",
    "            chunk.iloc[:, 0] = chunk.iloc[:, 0].astype(str)\n",
    "            \n",
    "            # Strip leading/trailing spaces and identify rows with multiple spaces\n",
    "            chunk['cleaned'] = chunk.iloc[:, 0].str.strip()\n",
    "            mask = chunk['cleaned'].str.count(' ') > 1\n",
    "            brands_with_spaces.extend(chunk[mask].values)\n",
    "    \n",
    "    # Save brands with spaces to a separate CSV file\n",
    "    brands_with_spaces_df = pd.DataFrame(brands_with_spaces, columns=chunk.columns)\n",
    "    brands_with_spaces_df.to_csv(os.path.join(output_folder, brands_with_spaces_file), index=False)\n",
    "\n",
    "    print(f\"Identified and saved brands with spaces to {brands_with_spaces_file}\")\n",
    "\n",
    "# Placeholders for file paths\n",
    "input_folder = \"Z:/My Drive/rcs/oem cross/oemssplit/\"\n",
    "output_folder = \"Z:/My Drive/rcs/oem cross/oemssplit/\"\n",
    "brands_with_spaces_file = \"brandswithspaces.csv\"\n",
    "\n",
    "# Identify brands with spaces and save them to a separate file\n",
    "identify_brands_with_spaces(input_folder, output_folder, brands_with_spaces_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#IDENTIFY AND FIX BRANDS WITH SPACES#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def apply_custom_rules(brand_product):\n",
    "    if brand_product.startswith(\"SKF\"):\n",
    "        parts = brand_product.split(' ', 2)\n",
    "        return f\"{parts[0]} {parts[1]}{parts[2].replace(' ', '', 1)}\"\n",
    "    \n",
    "    if brand_product.startswith(\"DIESEL POWER\"):\n",
    "        return brand_product.replace(' ', '-', 1)\n",
    "    \n",
    "    if brand_product.startswith(\"OPEN PARTS\"):\n",
    "        return brand_product.replace(' ', '-', 1)\n",
    "    \n",
    "    if brand_product.startswith(\"DT MT \"):\n",
    "        return brand_product.replace(' ', '-', 1)\n",
    "    \n",
    "    if brand_product.startswith(\"INCAR \"):\n",
    "        parts = brand_product.split(' ', 2)\n",
    "        return f\"{parts[0]} {parts[1]}{parts[2].replace(' ', '', 1)}\"\n",
    "    \n",
    "    if brand_product.startswith(\"MERITOR\"):\n",
    "        parts = brand_product.split(' ', 3)\n",
    "        return f\"{parts[0]} {parts[1]}{parts[2].replace(' ', '', 1).replace(' ', '', 1)}\"\n",
    "    \n",
    "    if brand_product.startswith(\"OE GERMANY\"):\n",
    "        parts = brand_product.split(' ', 2)\n",
    "        return f\"{parts[0]} {parts[1]}{parts[2].replace(' ', '', 1)}\"\n",
    "    \n",
    "    if brand_product.startswith(\"RCS\"):\n",
    "        parts = brand_product.split(' ')\n",
    "        if len(parts) == 2:\n",
    "            return f\"{parts[0]} {parts[1]}\"\n",
    "        elif len(parts) == 3:\n",
    "            return f\"{parts[0]} {parts[1]}{parts[2].replace(' ', '', 1)}\"\n",
    "        elif len(parts) >= 4:\n",
    "            modified_part = parts[2].replace(' ', '', 1).replace(' ', '', 1)\n",
    "            combined_part = f\"{parts[1]}{modified_part}{parts[3]}\"\n",
    "            return f\"{parts[0]} {combined_part} {' '.join(parts[4:])}\"\n",
    "\n",
    "\n",
    "    \n",
    "    if brand_product.startswith(\"SEM LASTIK\"):\n",
    "        return brand_product.replace(' ', '-', 1)\n",
    "    \n",
    "    if brand_product.startswith(\"XXL TRUCK\"):\n",
    "        return brand_product.replace(' ', '-', 1)\n",
    "    \n",
    "    if brand_product.startswith(\"XXL  \"):\n",
    "        return brand_product.replace(' ', '', 1)\n",
    "\n",
    "    return brand_product\n",
    "\n",
    "def identify_and_apply_rules(input_folder, output_folder, brands_with_spaces_file, chunksize=100000):\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Initialize list for brands with spaces\n",
    "    brands_with_spaces = []\n",
    "\n",
    "    # List all CSV files in the input folder\n",
    "    csv_files = [file for file in os.listdir(input_folder) if file.endswith(\".csv\")]\n",
    "\n",
    "    # Iterate over all files in the input folder\n",
    "    for file_name in tqdm(csv_files, desc=\"Processing files\"):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        \n",
    "        # Process the file in chunks\n",
    "        for chunk in pd.read_csv(file_path, dtype=str, chunksize=chunksize):\n",
    "            # Ensure all values in the first column are strings\n",
    "            chunk.iloc[:, 0] = chunk.iloc[:, 0].astype(str)\n",
    "            \n",
    "            # Strip leading/trailing spaces\n",
    "            chunk.iloc[:, 0] = chunk.iloc[:, 0].str.strip()\n",
    "            \n",
    "            # Drop rows that start with \"DT CAT\" or \"DT SA-FLYER EN\"\n",
    "            chunk = chunk[~chunk.iloc[:, 0].str.startswith((\"DT CAT\", \"DT SA-FLYER EN\"))]\n",
    "\n",
    "            # Identify rows with multiple spaces\n",
    "            mask = chunk.iloc[:, 0].str.count(' ') > 1\n",
    "            \n",
    "            # Apply custom rules to the rows identified\n",
    "            for index in chunk[mask].index:\n",
    "                brand_product = chunk.at[index, chunk.columns[0]]\n",
    "                modified_brand_product = apply_custom_rules(brand_product)\n",
    "                chunk.at[index, chunk.columns[0]] = modified_brand_product\n",
    "\n",
    "            brands_with_spaces.extend(chunk[mask].values)\n",
    "    \n",
    "    # Save brands with spaces to a separate CSV file\n",
    "    brands_with_spaces_df = pd.DataFrame(brands_with_spaces, columns=chunk.columns)\n",
    "    brands_with_spaces_df.to_csv(os.path.join(output_folder, brands_with_spaces_file), index=False)\n",
    "\n",
    "    print(f\"Processed and saved brands with spaces to {brands_with_spaces_file}\")\n",
    "\n",
    "# Placeholders for file paths\n",
    "input_folder = \"Z:/My Drive/rcs/oem cross/oemssplit/\"\n",
    "output_folder = \"Z:/My Drive/rcs/oem cross/oemssplit/\"\n",
    "brands_with_spaces_file = \"brandswithspaces.csv\"\n",
    "\n",
    "# Identify brands with spaces, apply rules, and save them to a separate file\n",
    "identify_and_apply_rules(input_folder, output_folder, brands_with_spaces_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DIVIDE IDS AND BRANDS#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def apply_custom_rules(brand_product):\n",
    "    if brand_product.startswith(\"SKF\"):\n",
    "        parts = brand_product.split(' ', 2)\n",
    "        return f\"{parts[0]} {parts[1]}{parts[2].replace(' ', '', 1)}\"\n",
    "    \n",
    "    if brand_product.startswith(\"DIESEL POWER\"):\n",
    "        return brand_product.replace(' ', '-', 1)\n",
    "    \n",
    "    if brand_product.startswith(\"OPEN PARTS\"):\n",
    "        return brand_product.replace(' ', '-', 1)\n",
    "    \n",
    "    if brand_product.startswith(\"DT MT \"):\n",
    "        return brand_product.replace(' ', '-', 1)\n",
    "    \n",
    "    if brand_product.startswith(\"INCAR \"):\n",
    "        parts = brand_product.split(' ', 2)\n",
    "        return f\"{parts[0]} {parts[1]}{parts[2].replace(' ', '', 1)}\"\n",
    "    \n",
    "    if brand_product.startswith(\"MERITOR\"):\n",
    "        parts = brand_product.split(' ', 3)\n",
    "        return f\"{parts[0]} {parts[1]}{parts[2].replace(' ', '', 1).replace(' ', '', 1)}\"\n",
    "    \n",
    "    if brand_product.startswith(\"OE GERMANY\"):\n",
    "        parts = brand_product.split(' ', 2)\n",
    "        return f\"{parts[0]} {parts[1]}{parts[2].replace(' ', '', 1)}\"\n",
    "    \n",
    "    if brand_product.startswith(\"RCS\"):\n",
    "        parts = brand_product.split(' ')\n",
    "        if len(parts) == 2:\n",
    "            return f\"{parts[0]} {parts[1]}\"\n",
    "        elif len(parts) == 3:\n",
    "            return f\"{parts[0]} {parts[1]}{parts[2].replace(' ', '', 1)}\"\n",
    "        elif len(parts) >= 4:\n",
    "            modified_part = parts[2].replace(' ', '', 1).replace(' ', '', 1)\n",
    "            combined_part = f\"{parts[1]}{modified_part}{parts[3]}\"\n",
    "            return f\"{parts[0]} {combined_part} {' '.join(parts[4:])}\"\n",
    "    \n",
    "    if brand_product.startswith(\"SEM LASTIK\"):\n",
    "        return brand_product.replace(' ', '-', 1)\n",
    "    \n",
    "    if brand_product.startswith(\"XXL TRUCK\"):\n",
    "        return brand_product.replace(' ', '-', 1)\n",
    "    \n",
    "    if brand_product.startswith(\"XXL  \"):\n",
    "        return brand_product.replace(' ', '', 1)\n",
    "\n",
    "    return brand_product\n",
    "\n",
    "def process_csv_files(input_folder, output_folder, chunksize=100000):\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # List all CSV files in the input folder\n",
    "    csv_files = [file for file in os.listdir(input_folder) if file.endswith(\".csv\")]\n",
    "\n",
    "    # Iterate over all files in the input folder\n",
    "    for file_name in tqdm(csv_files, desc=\"Processing files\"):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        output_file_path = os.path.join(output_folder, file_name)\n",
    "        \n",
    "        processed_chunks = []\n",
    "\n",
    "        # Process the file in chunks\n",
    "        for chunk in pd.read_csv(file_path, dtype=str, chunksize=chunksize):\n",
    "            # Ensure all values in the first column are strings\n",
    "            chunk.iloc[:, 0] = chunk.iloc[:, 0].astype(str)\n",
    "            \n",
    "            # Strip leading/trailing spaces\n",
    "            chunk.iloc[:, 0] = chunk.iloc[:, 0].str.strip()\n",
    "            \n",
    "            # Drop rows that start with \"DT CAT\" or \"DT SA-FLYER EN\"\n",
    "            chunk = chunk[~chunk.iloc[:, 0].str.startswith((\"DT CAT\", \"DT SA-FLYER EN\"))]\n",
    "\n",
    "            # Identify rows with multiple spaces\n",
    "            mask = chunk.iloc[:, 0].str.count(' ') > 1\n",
    "            \n",
    "            # Apply custom rules to the rows identified\n",
    "            for index in chunk[mask].index:\n",
    "                brand_product = chunk.at[index, chunk.columns[0]]\n",
    "                modified_brand_product = apply_custom_rules(brand_product)\n",
    "                chunk.at[index, chunk.columns[0]] = modified_brand_product\n",
    "\n",
    "            # Separate brand and product ID into new columns\n",
    "            chunk['article_alt_brands'] = chunk.iloc[:, 0].str.extract(r'(^[^\\s]+)')\n",
    "            chunk['article_alt'] = chunk.iloc[:, 0].str.extract(r'^[^\\s]+\\s+(.+)')\n",
    "            chunk['article_altc'] = chunk['article_alt']\n",
    "            chunk = chunk.drop(columns=['article_alt'])\n",
    "            \n",
    "            # Remove any spaces from the oem_number column\n",
    "            chunk['oem_number'] = chunk['oem_number'].str.replace(' ', '')\n",
    "\n",
    "            # Reorder columns to the required order\n",
    "            chunk = chunk[['article_altc', 'article_alt_brands', 'manufacturer', 'oem_number']]\n",
    "\n",
    "            processed_chunks.append(chunk)\n",
    "\n",
    "        # Concatenate all processed chunks and save to a new CSV file\n",
    "        processed_df = pd.concat(processed_chunks)\n",
    "        processed_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(f\"Processed and saved files to {output_folder}\")\n",
    "\n",
    "# Placeholders for file paths\n",
    "input_folder = \"Z:/My Drive/rcs/oem cross/oemssplit/\"\n",
    "output_folder = \"Z:/My Drive/rcs/oem cross/oemssplit/processed/\"\n",
    "\n",
    "# Process all CSV files to separate brands and product IDs\n",
    "process_csv_files(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#FIX BRANDS NAMING#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define file paths\n",
    "oems_folder = \"Z:/My Drive/rcs/oem cross/oemssplit/processed/\"\n",
    "output_folder = \"Z:/My Drive/rcs/oem cross/oemssplit/processed/tulero/\"\n",
    "output_file = os.path.join(output_folder, \"unique_brands.csv\")\n",
    "\n",
    "# Ensure the output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Initialize a DataFrame to hold all OEM brands\n",
    "all_oem_brands = pd.DataFrame(columns=[\"article_alt_brands\"])\n",
    "\n",
    "# List all OEM files in the processed folder\n",
    "oems_files = [file for file in os.listdir(oems_folder) if file.endswith(\".csv\")]\n",
    "\n",
    "# Iterate over all OEM files and collect brands\n",
    "for file_name in tqdm(oems_files, desc=\"Collecting OEM brands\"):\n",
    "    file_path = os.path.join(oems_folder, file_name)\n",
    "    oems_df = pd.read_csv(file_path, dtype=str)\n",
    "    if \"article_alt_brands\" in oems_df.columns:\n",
    "        all_oem_brands = pd.concat([all_oem_brands, oems_df[[\"article_alt_brands\"]]])\n",
    "\n",
    "# Drop duplicates to ensure unique brands\n",
    "unique_brands = all_oem_brands.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Save the result to a CSV file\n",
    "unique_brands.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Unique brands saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "brands_to_check_file = \"Z:/My Drive/rcs/oem cross/oemssplit/processed/tulero/articlesbrands.csv\"\n",
    "existing_brands_file = \"Z:/My Drive/rcs/oem cross/oemssplit/processed/tulero/unique_brands.csv\"\n",
    "output_file = \"Z:/My Drive/rcs/oem cross/oemssplit/processed/tulero/unique_brands_result.csv\"\n",
    "\n",
    "# Load the CSV files\n",
    "brands_to_check_df = pd.read_csv(brands_to_check_file, dtype=str)\n",
    "existing_brands_df = pd.read_csv(existing_brands_file, dtype=str)\n",
    "\n",
    "# Ensure the brand columns are stripped of leading/trailing spaces\n",
    "brands_to_check_df[\"Brand\"] = brands_to_check_df[\"Brand\"].str.strip()\n",
    "existing_brands_df[\"Brands\"] = existing_brands_df[\"Brands\"].str.strip()\n",
    "\n",
    "# Create a set of existing brands for fast lookup\n",
    "existing_brands_set = set(existing_brands_df[\"Brands\"])\n",
    "\n",
    "# Check for matches and add a new column with the matched brand or \"No Match\"\n",
    "brands_to_check_df[\"Match\"] = brands_to_check_df[\"Brand\"].apply(lambda x: x if x in existing_brands_set else \"No Match\")\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "brands_to_check_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Output with matches saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DO THE THING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define file paths\n",
    "cleaned_articles_file = \"Z:/My Drive/rcs/oem cross/cleaned_Articles26062024.csv\"\n",
    "old_oems_folder = \"Z:/My Drive/rcs/oem cross/oemssplit/processed/\"\n",
    "output_folder = \"Z:/My Drive/rcs/oem cross/oemssplit/processed/tulero/\"\n",
    "log_file = os.path.join(output_folder, \"match_log.txt\")\n",
    "\n",
    "# Ensure the output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Load the cleaned articles file\n",
    "cleaned_df = pd.read_csv(cleaned_articles_file, dtype=str)\n",
    "\n",
    "# Ensure all values in the relevant columns are strings and strip any leading/trailing spaces\n",
    "cleaned_df[\"CODICE PRODOTTO\"] = cleaned_df[\"CODICE PRODOTTO\"].astype(str).str.strip()\n",
    "cleaned_df[\"CODICE OE\"] = cleaned_df[\"CODICE OE\"].astype(str).str.strip()\n",
    "cleaned_df[\"BRAND\"] = cleaned_df[\"BRAND\"].astype(str).str.strip()\n",
    "\n",
    "# List all old CSV files in the processed folder\n",
    "old_oems_files = [file for file in os.listdir(old_oems_folder) if file.startswith(\"oemsDC\") and file.endswith(\".csv\")]\n",
    "\n",
    "# Initialize a DataFrame to hold all OEM mappings\n",
    "all_oem_mappings = pd.DataFrame()\n",
    "\n",
    "# Iterate over all old oems files and build the oem_number mappings\n",
    "for file_name in tqdm(old_oems_files, desc=\"Building old OEM mappings\"):\n",
    "    file_path = os.path.join(old_oems_folder, file_name)\n",
    "    oems_df = pd.read_csv(file_path, dtype=str)\n",
    "    oems_df[\"article_altc\"] = oems_df[\"article_altc\"].astype(str).str.strip()\n",
    "    oems_df[\"oem_number\"] = oems_df[\"oem_number\"].astype(str).str.strip().str.replace(' ', '')\n",
    "    oems_df[\"article_alt_brands\"] = oems_df[\"article_alt_brands\"].astype(str).str.strip()\n",
    "    oems_df[\"brand_prefix\"] = oems_df[\"article_alt_brands\"].str[:5]\n",
    "    all_oem_mappings = pd.concat([all_oem_mappings, oems_df[[\"article_altc\", \"oem_number\", \"brand_prefix\"]]])\n",
    "\n",
    "# Create a lookup dictionary\n",
    "oem_lookup = all_oem_mappings.groupby(['article_altc', 'brand_prefix'])['oem_number'].apply(list).to_dict()\n",
    "\n",
    "# Function to get OEM number\n",
    "def get_oem_number(row):\n",
    "    key = (row[\"CODICE PRODOTTO\"], row[\"BRAND\"][:5])\n",
    "    return \" | \".join(oem_lookup[key]) if key in oem_lookup else \"Unknown OE\"\n",
    "\n",
    "# Update the CODICE OE column in the cleaned articles file\n",
    "tqdm.pandas(desc=\"Updating CODICE OE with old OEMs\")\n",
    "cleaned_df[\"CODICE OE\"] = cleaned_df.progress_apply(get_oem_number, axis=1)\n",
    "\n",
    "# Increase the values in \"PRZ. ULT. ACQ.\" by 25% and rename the column to \"PREZZO\"\n",
    "cleaned_df[\"PRZ. ULT. ACQ.\"] = pd.to_numeric(cleaned_df[\"PRZ. ULT. ACQ.\"], errors='coerce')\n",
    "cleaned_df[\"PREZZO\"] = cleaned_df[\"PRZ. ULT. ACQ.\"].apply(lambda x: round(x * 1.25, 2) if pd.notnull(x) else x)\n",
    "cleaned_df = cleaned_df.drop(columns=[\"PRZ. ULT. ACQ.\"])\n",
    "\n",
    "# Reorder the columns so that PREZZO is between GIACENZA and SCHEDA TECNICA\n",
    "columns_order = [\n",
    "    \"CODICE PRODOTTO\",\n",
    "    \"CODICE OE\",\n",
    "    \"CODICI CROSS\",\n",
    "    \"BRAND\",\n",
    "    \"DESCRIZIONE\",\n",
    "    \"LINK IMMAGINE\",\n",
    "    \"CATEGORIA\",\n",
    "    \"GIACENZA\",\n",
    "    \"PREZZO\",\n",
    "    \"SCHEDA TECNICA\",\n",
    "    \"SCHEDA DI SICUREZZA\",\n",
    "    \"CONFEZIONE\",\n",
    "    \"QUANTITÀ MINIMA\",\n",
    "    \"META.LUNGHEZZA\",\n",
    "    \"META.LARGHEZZA\",\n",
    "    \"META.PROFONDITA'\",\n",
    "    \"META. ...\"\n",
    "]\n",
    "cleaned_df = cleaned_df[columns_order]\n",
    "\n",
    "# Initialize the CODICI CROSS column\n",
    "cleaned_df[\"CODICI CROSS\"] = \"\"\n",
    "\n",
    "# Create a dictionary to hold cross references, excluding \"Unknown OE\"\n",
    "cross_references = cleaned_df[cleaned_df[\"CODICE OE\"] != \"Unknown OE\"].groupby(\"CODICE OE\")[\"CODICE PRODOTTO\"].apply(list).to_dict()\n",
    "\n",
    "# Create a Series to store cross codes\n",
    "cross_codes_series = pd.Series(index=cleaned_df.index, dtype=str)\n",
    "\n",
    "# Apply the function to update the CODICI CROSS column, ignoring \"Unknown OE\"\n",
    "for codice_oe, prodotti in tqdm(cross_references.items(), desc=\"Updating CODICI CROSS\"):\n",
    "    cross_codes = {prodotto: \" | \".join([code for code in prodotti if code != prodotto]) for prodotto in prodotti}\n",
    "    cross_codes_series.update(pd.Series(cross_codes))\n",
    "\n",
    "cleaned_df[\"CODICI CROSS\"] = cleaned_df.index.map(cross_codes_series).fillna(\"\")\n",
    "\n",
    "# Precompute padded_oe column\n",
    "cleaned_df[\"padded_oe\"] = \" \" + cleaned_df[\"CODICE OE\"].str.strip() + \" \"\n",
    "\n",
    "# Handle Unknown OE: Check if CODICE PRODOTTO is present in other CODICE OE\n",
    "def find_additional_cross_codes(codice_prodotto, padded_oe):\n",
    "    matches = cleaned_df[cleaned_df[\"CODICE OE\"] != \"Unknown OE\"]\n",
    "    exact_matches = matches[matches[\"padded_oe\"].str.contains(f\" {codice_prodotto} \", regex=False)]\n",
    "    if not exact_matches.empty:\n",
    "        return \" | \".join(exact_matches[\"CODICE PRODOTTO\"].unique())\n",
    "    return \"\"\n",
    "\n",
    "# Apply the function to update the CODICI CROSS column for Unknown OE\n",
    "unknown_oe_mask = cleaned_df[\"CODICE OE\"] == \"Unknown OE\"\n",
    "cleaned_df.loc[unknown_oe_mask, \"CODICI CROSS\"] = cleaned_df.loc[unknown_oe_mask, \"CODICE PRODOTTO\"].progress_apply(\n",
    "    lambda codice_prodotto: find_additional_cross_codes(codice_prodotto, cleaned_df[\"padded_oe\"])\n",
    ")\n",
    "\n",
    "# Save the updated cleaned articles file\n",
    "output_file_path = os.path.join(output_folder, \"tulerodataset26062024.csv\")\n",
    "cleaned_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Write the match statistics to a log file\n",
    "successful_matches = cleaned_df[\"CODICE OE\"].apply(lambda x: x != \"Unknown OE\").sum()\n",
    "unsuccessful_matches = cleaned_df[\"CODICE OE\"].apply(lambda x: x == \"Unknown OE\").sum()\n",
    "\n",
    "with open(log_file, \"w\") as log:\n",
    "    log.write(f\"Total successful matches: {successful_matches}\\n\")\n",
    "    log.write(f\"Total unsuccessful matches: {unsuccessful_matches}\\n\")\n",
    "\n",
    "print(f\"Updated CODICE OE and PREZZO in {output_file_path}\")\n",
    "print(f\"Match statistics written to {log_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#SAMPLE FILE#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define file paths\n",
    "input_file_path = \"Z:/My Drive/rcs/oem cross/oemssplit/processed/tulero/tulerodataset26062024.csv\"\n",
    "output_file_path = \"Z:/My Drive/rcs/oem cross/oemssplit/processed/tulero/sample_tulerodataset26062024.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(input_file_path, dtype=str)\n",
    "\n",
    "# Keep only the first occurrence of each unique BRAND\n",
    "df_unique_brands = df.drop_duplicates(subset=[\"BRAND\"], keep=\"first\")\n",
    "\n",
    "# Save the resulting dataframe to a new CSV file\n",
    "df_unique_brands.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Sample file with unique BRAND rows saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating sheets: 100%|██████████| 5/5 [00:00<00:00,  9.73it/s]\n",
      "Building old OEM mappings: 100%|██████████| 4/4 [00:02<00:00,  1.53it/s]\n",
      "Updating CODICE OE with old OEMs: 100%|██████████| 30376/30376 [00:00<00:00, 252486.45it/s]\n",
      "Updating CODICI CROSS: 100%|██████████| 16342/16342 [00:18<00:00, 894.97it/s]\n",
      "Updating CODICE OE with old OEMs: 100%|██████████| 13842/13842 [01:15<00:00, 182.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CODICE OE and PREZZO in Z:/My Drive/rcs/oem cross/oemssplit/processed/tulero/tulerodataset26062024.csv\n",
      "Match statistics written to Z:/My Drive/rcs/oem cross/oemssplit/processed/tulero/match_log.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating brands: 100%|██████████| 30376/30376 [00:00<00:00, 3038037.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brands updated in Z:/My Drive/rcs/oem cross/oemssplit/processed/tulero/tulerodataset_with_brands26062024.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openpyxl  # noqa: F401\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def is_unnamed(header):\n",
    "    return str(header).startswith(\"Unnamed\")\n",
    "\n",
    "\n",
    "def validate_first_sheet(df):\n",
    "    return (\n",
    "        is_unnamed(df.columns[1])\n",
    "        and is_unnamed(df.columns[2])\n",
    "        and is_unnamed(df.columns[3])\n",
    "        and is_unnamed(df.columns[4])\n",
    "        and is_unnamed(df.columns[5])\n",
    "        and not pd.isna(df.columns[0])\n",
    "        and not pd.isna(df.columns[6])\n",
    "    )\n",
    "\n",
    "\n",
    "def validate_other_sheet(df):\n",
    "    return (\n",
    "        df.shape[1] == 6\n",
    "        and not is_unnamed(df.columns[1])\n",
    "        and not is_unnamed(df.columns[2])\n",
    "        and not is_unnamed(df.columns[3])\n",
    "        and not is_unnamed(df.columns[4])\n",
    "        and is_unnamed(df.columns[0])\n",
    "    )\n",
    "\n",
    "\n",
    "def process_files(articles_file_path, output_file_path):\n",
    "    xls = pd.ExcelFile(articles_file_path)\n",
    "    relevant_sheets = []\n",
    "    first_sheet_validated = False\n",
    "\n",
    "    for sheet_name in tqdm(xls.sheet_names, desc=\"Validating sheets\"):\n",
    "        df = pd.read_excel(xls, sheet_name=sheet_name, header=0)\n",
    "        if not first_sheet_validated:\n",
    "            if validate_first_sheet(df):\n",
    "                relevant_sheets.append((sheet_name, df))\n",
    "                first_sheet_validated = True\n",
    "            else:\n",
    "                raise ValueError(\"First sheet is not valid\")\n",
    "        else:\n",
    "            if validate_other_sheet(df):\n",
    "                relevant_sheets.append((sheet_name, df))\n",
    "\n",
    "    if not relevant_sheets:\n",
    "        raise ValueError(\"No relevant sheets found in the Excel file\")\n",
    "\n",
    "    first_sheet_df = relevant_sheets[0][1].drop(columns=[\"mgs210\"])\n",
    "    aligned_sheets = [first_sheet_df]\n",
    "    for sheet_name, df in relevant_sheets[1:]:\n",
    "        df.columns = first_sheet_df.columns\n",
    "        aligned_sheets.append(df)\n",
    "\n",
    "    df_combined = pd.concat(aligned_sheets, ignore_index=True)\n",
    "    df_combined = df_combined[~df_combined.iloc[:, 2].isin([\"\", \".\"])]\n",
    "    df_combined = df_combined.drop(columns=[\"STAMPA LISTINI\"])\n",
    "\n",
    "    df_combined.columns = [\n",
    "        \"CODICE PRODOTTO\",\n",
    "        \"BRAND\",\n",
    "        \"DESCRIZIONE\",\n",
    "        \"GIACENZA\",\n",
    "        \"PRZ. ULT. ACQ.\",\n",
    "    ]\n",
    "\n",
    "    for col in [\"CODICE OE\", \"CODICI CROSS\", \"LINK IMMAGINE\", \"CATEGORIA\", \"SCHEDA TECNICA\", \"SCHEDA DI SICUREZZA\", \"CONFEZIONE\", \"QUANTITÀ MINIMA\", \"META.LUNGHEZZA\", \"META.LARGHEZZA\", \"META.PROFONDITA'\", \"META. ...\"]:\n",
    "        df_combined[col] = pd.NA\n",
    "\n",
    "    df_combined = df_combined[\n",
    "        [\n",
    "            \"CODICE PRODOTTO\",\n",
    "            \"CODICE OE\",\n",
    "            \"CODICI CROSS\",\n",
    "            \"BRAND\",\n",
    "            \"DESCRIZIONE\",\n",
    "            \"LINK IMMAGINE\",\n",
    "            \"CATEGORIA\",\n",
    "            \"PRZ. ULT. ACQ.\",\n",
    "            \"GIACENZA\",\n",
    "            \"SCHEDA TECNICA\",\n",
    "            \"SCHEDA DI SICUREZZA\",\n",
    "            \"CONFEZIONE\",\n",
    "            \"QUANTITÀ MINIMA\",\n",
    "            \"META.LUNGHEZZA\",\n",
    "            \"META.LARGHEZZA\",\n",
    "            \"META.PROFONDITA'\",\n",
    "            \"META. ...\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    df_combined[\"GIACENZA\"] = pd.to_numeric(\n",
    "        df_combined[\"GIACENZA\"].str.replace(\",\", \".\"), errors=\"coerce\"\n",
    "    )\n",
    "    df_combined[\"PRZ. ULT. ACQ.\"] = pd.to_numeric(\n",
    "        df_combined[\"PRZ. ULT. ACQ.\"].str.replace(\",\", \".\"), errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    df_combined = df_combined[df_combined[\"GIACENZA\"] > 0]\n",
    "    df_combined = df_combined[df_combined[\"PRZ. ULT. ACQ.\"].notna()]\n",
    "    df_combined.to_csv(output_file_path, index=False)\n",
    "\n",
    "\n",
    "def aggregate_oem_numbers(oems_df):\n",
    "    oems_df['oem_number'] = oems_df['oem_number'].astype(str)\n",
    "    oems_df['article_alt'] = oems_df['article_alt'].astype(str)\n",
    "    oems_agg = oems_df.groupby('article_alt')['oem_number'].apply(lambda x: ' | '.join(x)).reset_index()\n",
    "    return oems_agg\n",
    "\n",
    "\n",
    "def append_oem_numbers(df_cleaned, oems_agg):\n",
    "    df_cleaned['CODICE PRODOTTO'] = df_cleaned['CODICE PRODOTTO'].astype(str).str.strip().str.upper()\n",
    "    oems_agg['article_alt'] = oems_agg['article_alt'].str.strip().str.upper()\n",
    "    merged_df = pd.merge(df_cleaned, oems_agg, left_on='CODICE PRODOTTO', right_on='article_alt', how='left')\n",
    "    merged_df.rename(columns={'oem_number': 'CODICE OE'}, inplace=True)\n",
    "    merged_df.drop(columns=['article_alt'], inplace=True)\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def process_and_merge_files(cleaned_csv_path, oems_file_path, output_file_path):\n",
    "    df_cleaned = pd.read_csv(cleaned_csv_path)\n",
    "    oems_df = pd.read_csv(oems_file_path, delimiter=';', dtype=str)\n",
    "    oems_agg = aggregate_oem_numbers(oems_df)\n",
    "    df_result = append_oem_numbers(df_cleaned, oems_agg)\n",
    "    df_result.to_csv(output_file_path, index=False)\n",
    "    \n",
    "def update_brands(output_file_path, brands_file_path, final_output_file_path):\n",
    "    # Load the output file\n",
    "    df_output = pd.read_csv(output_file_path, dtype=str)\n",
    "    \n",
    "    # Load the brands file\n",
    "    brands_df = pd.read_csv(brands_file_path, dtype=str)\n",
    "    \n",
    "    # Ensure all values in the relevant columns are strings and strip any leading/trailing spaces\n",
    "    df_output[\"BRAND\"] = df_output[\"BRAND\"].astype(str).str.strip()\n",
    "    brands_df[\"Brand\"] = brands_df[\"Brand\"].astype(str).str.strip()\n",
    "    brands_df[\"Match\"] = brands_df[\"Match\"].astype(str).str.strip()\n",
    "    \n",
    "    # Create a lookup dictionary from the brands file\n",
    "    brand_lookup = dict(zip(brands_df[\"Brand\"], brands_df[\"Match\"]))\n",
    "    \n",
    "    # Update the BRANDS in the output dataframe\n",
    "    tqdm.pandas(desc=\"Updating brands\")\n",
    "    df_output[\"BRAND\"] = df_output[\"BRAND\"].progress_apply(lambda x: brand_lookup.get(x, x))\n",
    "    \n",
    "    # Save the updated dataframe to a new CSV file\n",
    "    df_output.to_csv(final_output_file_path, index=False)\n",
    "    \n",
    "\n",
    "\n",
    "def main():\n",
    "    # Define file paths\n",
    "    articles_file_path = \"Z:/My Drive/rcs/oem cross/Articles26062024.xls\"\n",
    "    cleaned_csv_path = \"Z:/My Drive/rcs/oem cross/cleaned_Articles26062024.csv\"\n",
    "    old_oems_folder = \"Z:/My Drive/rcs/oem cross/oemssplit/processed/\"\n",
    "    output_folder = \"Z:/My Drive/rcs/oem cross/oemssplit/processed/tulero/\"\n",
    "    final_output_file_path = os.path.join(output_folder, \"tulerodataset26062024.csv\")\n",
    "    final_output_with_brands_path = os.path.join(output_folder, \"tulerodataset_with_brands26062024.csv\")\n",
    "    log_file = os.path.join(output_folder, \"match_log.txt\")\n",
    "    brands_file_path = \"Z:/My Drive/rcs/oem cross/BRANDS.csv\"\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Step 1: Clean dataset\n",
    "    process_files(articles_file_path, cleaned_csv_path)\n",
    "\n",
    "    # Step 2: Do the thing\n",
    "    cleaned_df = pd.read_csv(cleaned_csv_path, dtype=str)\n",
    "    cleaned_df[\"CODICE PRODOTTO\"] = cleaned_df[\"CODICE PRODOTTO\"].astype(str).str.strip()\n",
    "    cleaned_df[\"CODICE OE\"] = cleaned_df[\"CODICE OE\"].astype(str).str.strip()\n",
    "    cleaned_df[\"BRAND\"] = cleaned_df[\"BRAND\"].astype(str).str.strip()\n",
    "\n",
    "    old_oems_files = [file for file in os.listdir(old_oems_folder) if file.startswith(\"oemsDC\") and file.endswith(\".csv\")]\n",
    "    all_oem_mappings = pd.DataFrame()\n",
    "\n",
    "    for file_name in tqdm(old_oems_files, desc=\"Building old OEM mappings\"):\n",
    "        file_path = os.path.join(old_oems_folder, file_name)\n",
    "        oems_df = pd.read_csv(file_path, dtype=str)\n",
    "        oems_df[\"article_altc\"] = oems_df[\"article_altc\"].astype(str).str.strip()\n",
    "        oems_df[\"oem_number\"] = oems_df[\"oem_number\"].astype(str).str.strip().str.replace(' ', '')\n",
    "        oems_df[\"article_alt_brands\"] = oems_df[\"article_alt_brands\"].astype(str).str.strip()\n",
    "        oems_df[\"brand_prefix\"] = oems_df[\"article_alt_brands\"].str[:5]\n",
    "        all_oem_mappings = pd.concat([all_oem_mappings, oems_df[[\"article_altc\", \"oem_number\", \"brand_prefix\"]]])\n",
    "\n",
    "    oem_lookup = all_oem_mappings.groupby(['article_altc', 'brand_prefix'])['oem_number'].apply(list).to_dict()\n",
    "\n",
    "    def get_oem_number(row):\n",
    "        key = (row[\"CODICE PRODOTTO\"], row[\"BRAND\"][:5])\n",
    "        return \" | \".join(oem_lookup[key]) if key in oem_lookup else \"Unknown OE\"\n",
    "\n",
    "    tqdm.pandas(desc=\"Updating CODICE OE with old OEMs\")\n",
    "    cleaned_df[\"CODICE OE\"] = cleaned_df.progress_apply(get_oem_number, axis=1)\n",
    "\n",
    "    cleaned_df[\"PRZ. ULT. ACQ.\"] = pd.to_numeric(cleaned_df[\"PRZ. ULT. ACQ.\"], errors='coerce')\n",
    "    cleaned_df[\"PREZZO\"] = cleaned_df[\"PRZ. ULT. ACQ.\"].apply(lambda x: round(x * 1.25, 2) if pd.notnull(x) else x)\n",
    "    cleaned_df = cleaned_df.drop(columns=[\"PRZ. ULT. ACQ.\"])\n",
    "\n",
    "    columns_order = [\n",
    "        \"CODICE PRODOTTO\",\n",
    "        \"CODICE OE\",\n",
    "        \"CODICI CROSS\",\n",
    "        \"BRAND\",\n",
    "        \"DESCRIZIONE\",\n",
    "        \"LINK IMMAGINE\",\n",
    "        \"CATEGORIA\",\n",
    "        \"GIACENZA\",\n",
    "        \"PREZZO\",\n",
    "        \"SCHEDA TECNICA\",\n",
    "        \"SCHEDA DI SICUREZZA\",\n",
    "        \"CONFEZIONE\",\n",
    "        \"QUANTITÀ MINIMA\",\n",
    "        \"META.LUNGHEZZA\",\n",
    "        \"META.LARGHEZZA\",\n",
    "        \"META.PROFONDITA'\",\n",
    "        \"META. ...\"\n",
    "    ]\n",
    "    cleaned_df = cleaned_df[columns_order]\n",
    "    cleaned_df[\"CODICI CROSS\"] = \"\"\n",
    "\n",
    "    cross_references = cleaned_df[cleaned_df[\"CODICE OE\"] != \"Unknown OE\"].groupby(\"CODICE OE\")[\"CODICE PRODOTTO\"].apply(list).to_dict()\n",
    "    cross_codes_series = pd.Series(index=cleaned_df.index, dtype=str)\n",
    "\n",
    "    for codice_oe, prodotti in tqdm(cross_references.items(), desc=\"Updating CODICI CROSS\"):\n",
    "        cross_codes = {prodotto: \" | \".join([code for code in prodotti if code != prodotto]) for prodotto in prodotti}\n",
    "        cross_codes_series.update(pd.Series(cross_codes))\n",
    "\n",
    "    cleaned_df[\"CODICI CROSS\"] = cleaned_df.index.map(cross_codes_series).fillna(\"\")\n",
    "    cleaned_df[\"padded_oe\"] = \" \" + cleaned_df[\"CODICE OE\"].str.strip() + \" \"\n",
    "\n",
    "    def find_additional_cross_codes(codice_prodotto, padded_oe):\n",
    "        matches = cleaned_df[cleaned_df[\"CODICE OE\"] != \"Unknown OE\"]\n",
    "        exact_matches = matches[matches[\"padded_oe\"].str.contains(f\" {codice_prodotto} \", regex=False)]\n",
    "        if not exact_matches.empty:\n",
    "            return \" | \".join(exact_matches[\"CODICE PRODOTTO\"].unique())\n",
    "        return \"\"\n",
    "\n",
    "    unknown_oe_mask = cleaned_df[\"CODICE OE\"] == \"Unknown OE\"\n",
    "    cleaned_df.loc[unknown_oe_mask, \"CODICI CROSS\"] = cleaned_df.loc[unknown_oe_mask, \"CODICE PRODOTTO\"].progress_apply(\n",
    "        lambda codice_prodotto: find_additional_cross_codes(codice_prodotto, cleaned_df[\"padded_oe\"])\n",
    "    )\n",
    "\n",
    "    cleaned_df.to_csv(final_output_file_path, index=False)\n",
    "\n",
    "    successful_matches = cleaned_df[\"CODICE OE\"].apply(lambda x: x != \"Unknown OE\").sum()\n",
    "    unsuccessful_matches = cleaned_df[\"CODICE OE\"].apply(lambda x: x == \"Unknown OE\").sum()\n",
    "\n",
    "    with open(log_file, \"w\") as log:\n",
    "        log.write(f\"Total successful matches: {successful_matches}\\n\")\n",
    "        log.write(f\"Total unsuccessful matches: {unsuccessful_matches}\\n\")\n",
    "\n",
    "    print(f\"Updated CODICE OE and PREZZO in {final_output_file_path}\")\n",
    "    print(f\"Match statistics written to {log_file}\")\n",
    "\n",
    "    # Step 4: Update brands\n",
    "    update_brands(final_output_file_path, brands_file_path, final_output_with_brands_path)\n",
    "    print(f\"Brands updated in {final_output_with_brands_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
