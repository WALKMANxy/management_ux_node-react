{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:80: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:80: SyntaxWarning: invalid escape sequence '\\ '\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_19320\\1455470529.py:80: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  (\"Diagrammi Dettagliati\", \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'D:/coding/git-projects/rcs_management_stats/python.pdf'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fpdf import FPDF\n",
    "\n",
    "# Create instance of FPDF class\n",
    "pdf = FPDF()\n",
    "\n",
    "# Add a page\n",
    "pdf.add_page()\n",
    "\n",
    "# Set font\n",
    "pdf.set_font(\"Arial\", size=12)\n",
    "\n",
    "# Add a cell\n",
    "pdf.cell(200, 10, txt=\"Schema del Progetto di Software di Gestione\", ln=True, align=\"C\")\n",
    "\n",
    "# Add introductory text\n",
    "intro_text = \"\"\"\n",
    "Questo documento descrive i requisiti tecnici e il progetto di base per un software di gestione per una società B2B che si occupa di parti auto. Il software utilizzerà React, TypeScript e Bootstrap per il frontend, Node.js/Express.js per il backend, MongoDB per il database e Python per il motore statistico.\n",
    "\"\"\"\n",
    "pdf.multi_cell(0, 10, intro_text)\n",
    "\n",
    "# Add section headers and content\n",
    "sections = [\n",
    "    (\n",
    "        \"Ruoli degli Utenti e Permessi\",\n",
    "        \"\"\"\n",
    "- **Amministratore:**\n",
    "  - Accesso completo a tutte le funzionalità.\n",
    "  - Gestione di utenti, prodotti e promozioni.\n",
    "  - Visualizzazione e analisi di tutte le statistiche e gli avvisi.\n",
    "  - Visualizzazione della progressione e delle promozioni attive per tutti i clienti.\n",
    "\n",
    "- **Agente di Vendita:**\n",
    "  - Gestione delle promozioni.\n",
    "  - Visualizzazione delle statistiche dei clienti e degli avvisi per i clienti associati.\n",
    "  - Visualizzazione delle promozioni relative ai clienti associati.\n",
    "\n",
    "- **Cliente:**\n",
    "  - Visualizzazione della propria cronologia degli acquisti e delle statistiche di spesa.\n",
    "  - Visualizzazione delle promozioni disponibili.\n",
    "  - Visualizzazione della progressione verso le promozioni.\n",
    "    \"\"\",\n",
    "    ),\n",
    "    (\n",
    "        \"Funzionalità Principali\",\n",
    "        \"\"\"\n",
    "- **Gestione Clienti:**\n",
    "  - Tracciare le abitudini di spesa e la cronologia degli acquisti.\n",
    "  - Generare avvisi per clienti inattivi (visualizzabili solo da amministratori e agenti).\n",
    "  - Assegnare i clienti agli agenti (un cliente per agente, più clienti per agente).\n",
    "\n",
    "- **Gestione Promozioni:**\n",
    "  - Creare, inviare e gestire promozioni via SMS/Email.\n",
    "  - Tracciare la progressione verso le promozioni in base alla spesa.\n",
    "\n",
    "- **Gestione Ordini:**\n",
    "  - Importare ordini da file CSV.\n",
    "  - Futuro: Integrare con l'API di e-commerce per ottenere gli ordini direttamente.\n",
    "\n",
    "- **Statistiche e Avvisi:**\n",
    "  - Generare e visualizzare statistiche sul comportamento dei clienti e sulle vendite.\n",
    "  - Inviare avvisi per cambiamenti significativi nel comportamento dei clienti.\n",
    "    \"\"\",\n",
    "    ),\n",
    "    (\n",
    "        \"Architettura di Alto Livello\",\n",
    "        \"\"\"\n",
    "1. **Frontend:**\n",
    "   - **Framework:** React.js\n",
    "   - **Linguaggio:** TypeScript\n",
    "   - **Stile:** Bootstrap\n",
    "\n",
    "2. **Backend:**\n",
    "   - **Framework:** Node.js/Express.js\n",
    "   - **Database:** MongoDB\n",
    "   - **Autenticazione:** JWT, OAuth2\n",
    "   - **Parsing CSV:** Utilizzare una libreria come `csv-parser` per gestire i file CSV.\n",
    "   - **Integrazione API:** Predisposizione per l'integrazione con un'API di e-commerce.\n",
    "\n",
    "3. **Motore Statistico:**\n",
    "   - **Linguaggio:** Python\n",
    "   - **Librerie:** Pandas, NumPy, Scikit-learn\n",
    "   - **Comunicazione:** RabbitMQ per la coda dei messaggi\n",
    "\n",
    "4. **Deployment:**\n",
    "   - **Provider Cloud:** AWS, Azure, o Google Cloud\n",
    "   - **CI/CD:** Jenkins, GitHub Actions, o GitLab CI\n",
    "    \"\"\",\n",
    "    ),\n",
    "    (\n",
    "        \"Diagrammi Dettagliati\",\n",
    "        \"\"\"\n",
    "##### Diagramma dei Casi d'Uso\n",
    "\n",
    "                   +---------------------+\n",
    "                   |     Sistema di      |\n",
    "                   |    Gestione         |\n",
    "                   +---------------------+\n",
    "                    /      |       /      //\n",
    "                   /       |        /      //\n",
    "                  /        |         /      //\n",
    "                 /         |          /      //\n",
    "      +---------+          |           /      +---------+\n",
    "      | Amministratore |          |           /     | Cliente |\n",
    "      +---------+          |            /    +---------+\n",
    "           |               |             /       |\n",
    "  +--------v--------+ +----v-------v----+   +-----v-----+\n",
    "  | Gestione Utenti | | Gestione Promozioni | | Visualizza Statistiche |\n",
    "  +-----------------+ +-----------------+   +-----------+\n",
    "                           |\n",
    "                     +-----v-----+\n",
    "                     | Promozioni |\n",
    "                     +-----------+\n",
    "                           |\n",
    "                     +-----v-----+\n",
    "                     |  Ordini    |\n",
    "                     +-----------+\n",
    "    \"\"\",\n",
    "    ),\n",
    "    (\n",
    "        \"Component Diagram\",\n",
    "        \"\"\"\n",
    "##### Diagramma dei Componenti\n",
    "\n",
    " +-------------------+     +-------------------+\n",
    " |     Frontend      |     |      Backend      |\n",
    " | (React.js)        |<--> | (Node.js/Express) |\n",
    " +-------------------+     +-------------------+\n",
    "            |                      |\n",
    "            |                      |\n",
    "            v                      v\n",
    "    +--------------+        +-------------+\n",
    "    |   MongoDB    |        |   Python    |\n",
    "    +--------------+        +-------------+\n",
    "            |                      |\n",
    "            |                      |\n",
    "            v                      v\n",
    "        +---------+         +-------------+\n",
    "        | RabbitMQ|<------->| Python Stats|\n",
    "        +---------+         +-------------+\n",
    "            |\n",
    "            |\n",
    "        +---------+\n",
    "        | CSV/API |\n",
    "        +---------+\n",
    "    \"\"\",\n",
    "    ),\n",
    "    (\n",
    "        \"Entity-Relationship Diagram (ERD)\",\n",
    "        \"\"\"\n",
    "##### Diagramma Entità-Relazioni (ERD)\n",
    "\n",
    " +-------------+     +-------------+     +-------------+\n",
    " |  Cliente    |     |   Prodotto   |     | Promozione  |\n",
    " +-------------+     +-------------+     +-------------+\n",
    "        |                   |                   |\n",
    "        |                   |                   |\n",
    "        +-------------------+-------------------+\n",
    "                            |\n",
    "                            |\n",
    "                       +----------+\n",
    "                       |  Ordine  |\n",
    "                       +----------+\n",
    "                            |\n",
    "                       +----------+\n",
    "                       | Statistiche|\n",
    "                       +----------+\n",
    "                            |\n",
    "                       +----------+\n",
    "                       |   Agente  |\n",
    "                       +----------+\n",
    "    \"\"\",\n",
    "    ),\n",
    "    (\n",
    "        \"Panoramica Funzionale\",\n",
    "        \"\"\"\n",
    "#### Componenti del Frontend\n",
    "\n",
    "1. **Dashboard Amministratore:**\n",
    "   - Gestione Utenti: Aggiungere, modificare, eliminare utenti.\n",
    "   - Gestione Promozioni: Creare e inviare promozioni.\n",
    "   - Visualizzare Statistiche Clienti: Analisi e report dettagliati.\n",
    "   - Importazione Ordini: Caricare file CSV per importare ordini.\n",
    "   - Visualizzare e Gestire Avvisi.\n",
    "   - Visualizzare la progressione e le promozioni attive per tutti i clienti.\n",
    "\n",
    "2. **Dashboard Agente di Vendita:**\n",
    "   - Gestione Promozioni: Creare e inviare promozioni.\n",
    "   - Visualizzare Statistiche Clienti: Informazioni sul comportamento dei clienti associati.\n",
    "   - Visualizzare Avvisi: Notifiche per cambiamenti significativi nel comportamento dei clienti associati.\n",
    "   - Visualizzare Promozioni: Relativi ai clienti associati.\n",
    "\n",
    "3. **Dashboard Cliente:**\n",
    "   - Visualizzare Statistiche Personali: Accesso alla cronologia degli acquisti personali e alle statistiche di spesa.\n",
    "   - Visualizzare Promozioni: Accesso alle promozioni disponibili.\n",
    "   - Visualizzare Progressione: Progressione verso le promozioni.\n",
    "\n",
    "#### Servizi del Backend\n",
    "\n",
    "1. **Gestione Utenti:**\n",
    "   - Endpoint API per operazioni CRUD sugli utenti.\n",
    "   - Autenticazione e autorizzazione tramite JWT.\n",
    "   - Assegnare i clienti agli agenti.\n",
    "\n",
    "2. **Gestione Promozioni:**\n",
    "   - Endpoint API per creare e inviare promozioni via SMS/Email.\n",
    "   - Tracciare la progressione dei clienti verso le promozioni.\n",
    "\n",
    "3. **Gestione Ordini:**\n",
    "   - Endpoint API per caricare e analizzare file CSV.\n",
    "   - Servizio futuro per l'integrazione con l'API di e-commerce.\n",
    "\n",
    "4. **Gestione Statistiche e Avvisi:**\n",
    "   - Endpoint API per generare e recuperare statistiche.\n",
    "   - Integrazione con Python per analisi avanzate.\n",
    "    \"\"\",\n",
    "    ),\n",
    "    (\n",
    "        \"Prossimi Passi\",\n",
    "        \"\"\"\n",
    "1. **Definire gli Endpoint API:**\n",
    "   - Elencare tutti gli endpoint necessari per la gestione di utenti, promozioni, ordini e statistiche.\n",
    "   - Includere endpoint per il caricamento di file CSV e l'integrazione futura con l'API.\n",
    "\n",
    "2. **Configurare l'Ambiente Iniziale:**\n",
    "   - Configurare MongoDB, Node.js/Express, e le strutture del progetto React.\n",
    "   - Configurare RabbitMQ e l'ambiente Python.\n",
    "\n",
    "3. **Sviluppare le Funzionalità Core:**\n",
    "   - Implementare le funzionalità principali come l'autenticazione degli utenti, la gestione delle promozioni e il tracciamento base degli ordini.\n",
    "   - Assegnare i clienti agli agenti e gestire le associazioni.\n",
    "\n",
    "4. **Integrare la Gestione dei CSV:**\n",
    "   - Implementare la funzionalità di caricamento e analisi dei file CSV.\n",
    "\n",
    "5. **Pianificare l'Integrazione Futura con l'API:**\n",
    "   - Progettare il sistema in modo da poter passare facilmente dai caricamenti CSV alle chiamate API.\n",
    "   - Implementare un servizio di placeholder per l'integrazione con l'API.\n",
    "\n",
    "6. **Implementare il Frontend:**\n",
    "   - Creare componenti React per le dashboard di amministratori, agenti di vendita e clienti.\n",
    "\n",
    "7. **Test e Deployment:**\n",
    "   - Eseguire test approfonditi di tutte le funzionalità.\n",
    "   - Configurare una pipeline CI/CD per il deployment.\n",
    "    \"\"\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "for title, content in sections:\n",
    "    pdf.set_font(\"Arial\", size=12, style=\"B\")\n",
    "    pdf.cell(0, 10, title, ln=True)\n",
    "    pdf.set_font(\"Arial\", size=12)\n",
    "    pdf.multi_cell(0, 10, content)\n",
    "\n",
    "# Save the PDF\n",
    "pdf_output = \"D:/coding/git-projects/rcs_management_stats/project_basis.pdf\"\n",
    "pdf.output(pdf_output)\n",
    "\n",
    "pdf_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fpdf import FPDF\n",
    "\n",
    "\n",
    "class PDF(FPDF):\n",
    "    def header(self):\n",
    "        self.set_font(\"Arial\", \"B\", 12)\n",
    "        self.cell(0, 10, \"Piano di Implementazione\", 0, 1, \"C\")\n",
    "        self.ln(10)\n",
    "\n",
    "    def chapter_title(self, title):\n",
    "        self.set_font(\"Arial\", \"B\", 12)\n",
    "        self.cell(0, 10, title, 0, 1, \"L\")\n",
    "        self.ln(4)\n",
    "\n",
    "    def chapter_body(self, body):\n",
    "        self.set_font(\"Arial\", \"\", 12)\n",
    "        self.multi_cell(0, 10, body)\n",
    "        self.ln()\n",
    "\n",
    "    def add_chapter(self, title, body):\n",
    "        self.add_page()\n",
    "        self.chapter_title(title)\n",
    "        self.chapter_body(body)\n",
    "\n",
    "\n",
    "pdf = PDF()\n",
    "\n",
    "# Title\n",
    "pdf.set_title(\"Piano di Implementazione\")\n",
    "\n",
    "# Content\n",
    "content = \"\"\"\n",
    "1. Panoramica delle Pagine e Componenti\n",
    "Costruiremo le seguenti pagine per agenti, amministratori e clienti:\n",
    "\n",
    "Pagina Clienti\n",
    "Pagina Visite\n",
    "Pagina Statistiche (In Costruzione)\n",
    "Pagina Promozioni\n",
    "Pagina Avvisi\n",
    "Pagina Ordini\n",
    "Pagina Articoli\n",
    "Pagina Crea Avviso\n",
    "Pagina Rapporto Visita\n",
    "Successivamente, creeremo i dashboard per amministratori e clienti:\n",
    "\n",
    "Dashboard Amministratore\n",
    "Dashboard Cliente\n",
    "\n",
    "2. Struttura delle Pagine e Componenti\n",
    "\n",
    "Pagina Clienti\n",
    "Componenti:\n",
    "Elenco Clienti: Visualizza un elenco di clienti.\n",
    "Ricerca e Filtri Clienti: Funzionalità di ricerca per filtrare i clienti per vari criteri (data, agente, cliente).\n",
    "Dettagli Cliente: Mostra informazioni dettagliate su un cliente selezionato.\n",
    "Storico Visite: Visualizza le note delle visite relative al cliente selezionato.\n",
    "Funzionalità:\n",
    "Vista Agente: Vede solo i clienti associati all'agente.\n",
    "Vista Amministratore: Vede tutti i clienti, con informazioni sugli agenti associati.\n",
    "Vista Cliente: Vede informazioni dettagliate su se stesso e lo storico delle visite.\n",
    "\n",
    "Pagina Visite\n",
    "Componenti:\n",
    "Elenco Visite: Visualizza un elenco paginato di visite (25 per pagina).\n",
    "Dettagli Visita: Mostra informazioni dettagliate su una visita selezionata.\n",
    "Modulo Rapporto Visita (Pagina Separata): Un modulo per agenti e clienti per riportare i dettagli sulla visita.\n",
    "Funzionalità:\n",
    "Vista Agente: Vede solo le visite relative all'agente. Le visite senza rapporti sono evidenziate.\n",
    "Vista Amministratore: Vede tutte le visite. Monitora i rapporti di visita in ritardo.\n",
    "Vista Cliente: Vede le visite relative al cliente. Può inviare un rapporto di visita.\n",
    "Avvisi automatizzati per i rapporti di visita in ritardo (24 ore dopo la visita).\n",
    "Visite modificabili: Gli agenti possono spostare o riprogrammare le visite. Questo genera un avviso per gli amministratori.\n",
    "Modulo Rapporto Visita: Diviso in una parte pubblica (visibile ai clienti) e una parte privata (visibile solo agli agenti e amministratori).\n",
    "\n",
    "Pagina Statistiche\n",
    "Componenti:\n",
    "In Costruzione: Segnaposto che indica che la pagina è in costruzione.\n",
    "\n",
    "Pagina Promozioni\n",
    "Componenti:\n",
    "Elenco Promozioni: Visualizza un elenco di promozioni attuali e passate.\n",
    "Dettagli Promozione: Mostra informazioni dettagliate su una promozione selezionata.\n",
    "Modulo Crea Promozione: Un modulo per creare nuove promozioni.\n",
    "Funzionalità:\n",
    "Vista Agente: Vede le promozioni relative ai clienti dell'agente.\n",
    "Vista Amministratore: Vede tutte le promozioni.\n",
    "Vista Cliente: Vede le promozioni relative a se stesso, se ha raggiunto gli obiettivi.\n",
    "\n",
    "Pagina Avvisi\n",
    "Componenti:\n",
    "Elenco Avvisi: Visualizza un elenco di avvisi (automatici e manuali).\n",
    "Dettagli Avviso: Mostra informazioni dettagliate su un avviso selezionato.\n",
    "Funzionalità:\n",
    "Vista Agente: Vede gli avvisi relativi all'agente.\n",
    "Vista Amministratore: Vede tutti gli avvisi. Crea e gestisce avvisi.\n",
    "Vista Cliente: Vede gli avvisi relativi a se stesso.\n",
    "\n",
    "Pagina Ordini\n",
    "Componenti:\n",
    "Elenco Ordini: Visualizza un elenco di ordini dei clienti.\n",
    "Dettagli Ordine: Mostra informazioni dettagliate su un ordine selezionato.\n",
    "Funzionalità:\n",
    "Vista Agente: Vede solo gli ordini dei clienti associati all'agente.\n",
    "Vista Amministratore: Vede tutti gli ordini dei clienti.\n",
    "Vista Cliente: Vede solo i propri ordini.\n",
    "Informazioni sugli articoli collegati agli ordini, comprese le promozioni utilizzate e lo stato dei debiti.\n",
    "\n",
    "Pagina Articoli\n",
    "Componenti:\n",
    "Elenco Articoli: Visualizza un elenco di articoli.\n",
    "Ricerca e Filtri Articoli: Funzionalità di ricerca e filtro per ordinare gli articoli (migliori vendite, ecc.).\n",
    "Dettagli Articolo: Mostra informazioni dettagliate su un articolo selezionato.\n",
    "Funzionalità:\n",
    "Vista Agente: Vede solo gli articoli relativi agli ordini dei clienti associati all'agente.\n",
    "Vista Amministratore: Vede tutti gli articoli.\n",
    "Vista Cliente: Vede solo gli articoli relativi ai propri ordini, con un link all'ordine relativo.\n",
    "Informazioni sugli articoli: numero OEM compatibile, ID del produttore, nome, descrizione, promozioni o coupon associati.\n",
    "Collegamento automatico degli articoli con lo stesso numero OEM.\n",
    "\n",
    "Pagina Crea Avviso\n",
    "Componenti:\n",
    "Modulo Crea Avviso: Un modulo per creare nuovi avvisi.\n",
    "Funzionalità:\n",
    "Vista Amministratore: Crea nuovi avvisi.\n",
    "\n",
    "Pagina Rapporto Visita\n",
    "Componenti:\n",
    "Modulo Rapporto Visita: Un modulo per riportare i dettagli su una visita.\n",
    "Funzionalità:\n",
    "Vista Agente: Invia rapporti di visita.\n",
    "Vista Cliente: Invia feedback su una visita.\n",
    "Parte Pubblica: Visibile ai clienti.\n",
    "Parte Privata: Visibile solo agli agenti e amministratori.\n",
    "\n",
    "3. Dashboard\n",
    "\n",
    "Dashboard Amministratore\n",
    "Componenti:\n",
    "Panoramica: Riepilogo delle metriche e delle statistiche chiave.\n",
    "Gestione Utenti: Gestisce agenti e clienti.\n",
    "Avvisi di Sistema: Visualizza avvisi e notifiche di sistema.\n",
    "Funzionalità:\n",
    "Visualizza statistiche generali del sistema.\n",
    "Gestisce gli utenti (agenti e clienti).\n",
    "Monitora avvisi e notifiche di sistema.\n",
    "Interagisce con tutte le funzionalità degli agenti per la supervisione amministrativa.\n",
    "\n",
    "Dashboard Cliente\n",
    "Componenti:\n",
    "Panoramica: Riepilogo dell'attività e delle statistiche del cliente.\n",
    "Storico Acquisti: Visualizza uno storico degli ordini del cliente.\n",
    "Promozioni Attive: Visualizza le promozioni attuali disponibili per il cliente.\n",
    "Abitudini di Spesa: Rappresentazione visiva delle abitudini di spesa del cliente.\n",
    "Funzionalità:\n",
    "Visualizza attività e statistiche personali.\n",
    "Accede allo storico degli acquisti.\n",
    "Visualizza promozioni attive.\n",
    "Monitora abitudini di spesa.\n",
    "Interazione con Promozioni e Avvisi per dati personalizzati.\n",
    "\n",
    "4. Tipi di Agenti\n",
    "- Agenti di Vendita: Gestiscono le vendite e il rapporto con i clienti.\n",
    "- Agenti di Vendita Tecnici: Forniscono supporto tecnico durante le vendite.\n",
    "- Agenti di Supporto Tecnico: Offrono supporto tecnico post-vendita.\n",
    "Nota: Tutti i tipi di agenti saranno trattati allo stesso modo all'interno dell'app. L'app verificherà il tipo di agente al momento del login e fornirà l'accesso alle funzioni richieste in base al loro ruolo.\n",
    "\n",
    "5. Piano di Implementazione\n",
    "\n",
    "Impostare le Rotte e i Componenti delle Pagine\n",
    "Definire le rotte per ogni pagina nei dashboard dell'agente, dell'amministratore e del cliente.\n",
    "Creare la struttura di base per ogni pagina.\n",
    "\n",
    "Implementare le Funzionalità per Ogni Pagina\n",
    "Pagina Clienti: Implementare ricerca clienti, filtri, dettagli e storico visite in base al tipo di utente loggato.\n",
    "Pagina Visite: Implementare elenco visite, dettagli e funzionalità di reporting. Automatizzare avvisi per rapporti in ritardo.\n",
    "Pagina Promozioni: Implementare elenco promozioni, dettagli e modulo di creazione con tracciamento dell'efficacia.\n",
    "Pagina Avvisi: Sviluppare elenco avvisi, dettagli e modulo di creazione con tracciamento della risoluzione.\n",
    "Pagina Ordini: Implementare elenco ordini, dettagli e informazioni sugli articoli.\n",
    "Pagina Articoli: Implementare elenco articoli, ricerca, filtri, dettagli e collegamenti automatici.\n",
    "Pagina Crea Avviso: Implementare il modulo per la creazione di nuovi avvisi da parte degli amministratori.\n",
    "Pagina Rapporto Visita: Implementare il modulo per l'invio di rapporti di visita da parte di agenti e clienti.\n",
    "\n",
    "Sviluppare i Dashboard\n",
    "Impostare le rotte e la struttura di base per i dashboard di amministratori e clienti.\n",
    "Implementare funzionalità specifiche e componenti per ogni dashboard.\n",
    "\n",
    "Integrare e Testare\n",
    "Assicurarsi che tutte le pagine e i componenti siano integrati e funzionali.\n",
    "Condurre test approfonditi per identificare e risolvere problemi.\n",
    "\n",
    "Esempio Dettagliato: Implementazione della Pagina Clienti\n",
    "Componenti:\n",
    "ClientsList.tsx: Visualizza un elenco di clienti.\n",
    "ClientSearchFilters.tsx: Filtra i clienti per data, agente, cliente.\n",
    "ClientDetails.tsx: Mostra informazioni dettagliate su un cliente selezionato.\n",
    "VisitsHistory.tsx: Visualizza le note delle visite relative al cliente selezionato.\n",
    "\n",
    "Interazione:\n",
    "Filtrare per cliente o data aggiorna il componente VisitsHistory per mostrare le note di visita pertinenti.\n",
    "\n",
    "Sommario del Piano Generale\n",
    "Pagina Clienti: Creare e integrare componenti con filtri e interazione cliente-visita.\n",
    "Pagina Visite: Implementare elenco visite, dettagli e funzionalità di reporting. Automatizzare avvisi per rapporti in ritardo.\n",
    "Pagina Promozioni: Implementare elenco promozioni, dettagli e modulo di creazione con tracciamento dell'efficacia.\n",
    "Pagina Avvisi: Sviluppare elenco avvisi, dettagli e modulo di creazione con tracciamento della risoluzione.\n",
    "Pagina Ordini: Implementare elenco ordini, dettagli e informazioni sugli articoli.\n",
    "Pagina Articoli: Implementare elenco articoli, ricerca, filtri, dettagli e collegamenti automatici.\n",
    "Dashboard Amministratore e Cliente: Personalizzare i dashboard con funzionalità e dati rilevanti.\n",
    "Pagina Crea Avviso: Implementare il modulo per la creazione di nuovi avvisi da parte degli amministratori.\n",
    "Pagina Rapporto Visita: Implementare il modulo per l'invio di rapporti di visita da parte di agenti e clienti.\n",
    "\n",
    "Seguendo questo piano strutturato, ci assicuriamo che tutte le funzionalità siano ben pensate e integrate senza problemi. Una volta che avremo un piano chiaro e comprensione, possiamo iniziare a codificare ogni componente passo dopo passo. Se hai domande specifiche o necessiti di ulteriori dettagli su qualsiasi parte, non esitare a chiedere!\n",
    "\"\"\"\n",
    "\n",
    "pdf.add_chapter(\"Piano di Implementazione\", content)\n",
    "\n",
    "# Save the PDF to a file\n",
    "pdf.output(\"Piano_di_Implementazione.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Data Lista', 'Tipo Documento Attuale', 'Data Documento Attuale',\n",
      "       'Numero Documento Attuale', 'Lettera Documento Attuale',\n",
      "       'Tipo Documento Precedente', 'Data Documento Precedente',\n",
      "       'Numero Documento Precedente', 'Lettera Documento Precedente',\n",
      "       'Codice Cliente', 'Ragione Sociale Cliente', 'Codice Agente',\n",
      "       'Identificativo Articolo', 'Codice Articolo', 'Marca Articolo',\n",
      "       'Descrizione Articolo', 'Quantita Articolo', 'Prezzo Articolo',\n",
      "       'Valore', 'Mese', 'Anno', 'Costo', 'Gruppo Merceologico', 'Famiglia',\n",
      "       'Categoria Sconto Vendita', 'Reparto', 'Numero Lista'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_18188\\3502614722.py:20: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sorted_df = grouped.apply(sort_within_group).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file created successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from numpyencoder import NumpyEncoder\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"X:/artor/Downloads/mov_ven_01_06_2024.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Inspect the columns\n",
    "print(df.columns)\n",
    "\n",
    "# Group by 'Numero Documento Attuale' and 'Codice Cliente'\n",
    "grouped = df.groupby([\"Numero Documento Attuale\", \"Codice Cliente\"])\n",
    "\n",
    "\n",
    "# Function to sort within groups\n",
    "def sort_within_group(group):\n",
    "    return group.sort_values(by=[\"Numero Lista\"])\n",
    "\n",
    "\n",
    "# Apply sorting within each group\n",
    "sorted_df = grouped.apply(sort_within_group).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Convert the sorted DataFrame to a list of dictionaries\n",
    "def convert_to_json(df):\n",
    "    orders = []\n",
    "\n",
    "    for (doc_num, client_code), group in df.groupby(\n",
    "        [\"Numero Documento Attuale\", \"Codice Cliente\"]\n",
    "    ):\n",
    "        movements = group.to_dict(orient=\"records\")\n",
    "        order = {\n",
    "            \"documentNumber\": doc_num,\n",
    "            \"clientCode\": client_code,\n",
    "            \"movements\": movements,\n",
    "        }\n",
    "        orders.append(order)\n",
    "\n",
    "    return orders\n",
    "\n",
    "\n",
    "# Convert to JSON-like structure\n",
    "orders_json = convert_to_json(sorted_df)\n",
    "\n",
    "# Save the orders JSON to a file\n",
    "output_path = \"X:/artor/Downloads/json.json\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(orders_json, f, indent=4, cls=NumpyEncoder)\n",
    "\n",
    "print(\"JSON file created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Data Lista', 'Tipo Documento Attuale', 'Data Documento Attuale',\n",
      "       'Numero Documento Attuale', 'Lettera Documento Attuale',\n",
      "       'Tipo Documento Precedente', 'Data Documento Precedente',\n",
      "       'Numero Documento Precedente', 'Lettera Documento Precedente',\n",
      "       'Codice Cliente', 'Ragione Sociale Cliente', 'Codice Agente',\n",
      "       'Identificativo Articolo', 'Codice Articolo', 'Marca Articolo',\n",
      "       'Descrizione Articolo', 'Quantita Articolo', 'Prezzo Articolo',\n",
      "       'Valore', 'Mese', 'Anno', 'Costo', 'Gruppo Merceologico', 'Famiglia',\n",
      "       'Categoria Sconto Vendita', 'Reparto', 'Numero Lista'],\n",
      "      dtype='object')\n",
      "JSON file created successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from numpyencoder import NumpyEncoder\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"X:/artor/Downloads/mov_ven_01_06_2024.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Inspect the columns\n",
    "print(df.columns)\n",
    "\n",
    "\n",
    "# Function to sort within groups\n",
    "def sort_within_group(group):\n",
    "    return group.sort_values(by=[\"Numero Lista\"])\n",
    "\n",
    "\n",
    "# Apply sorting by 'Ragione Sociale Cliente' and then group by 'Data Documento Attuale'\n",
    "df_sorted = df.sort_values(by=[\"Ragione Sociale Cliente\", \"Data Documento Attuale\"])\n",
    "\n",
    "# Group by 'Ragione Sociale Cliente' and 'Data Documento Attuale'\n",
    "grouped = df_sorted.groupby([\"Ragione Sociale Cliente\", \"Data Documento Attuale\"])\n",
    "\n",
    "\n",
    "# Convert the sorted DataFrame to a list of dictionaries\n",
    "def convert_to_json(df):\n",
    "    orders = []\n",
    "\n",
    "    for (client_name, doc_date), group in df.groupby(\n",
    "        [\"Ragione Sociale Cliente\", \"Data Documento Attuale\"]\n",
    "    ):\n",
    "        group_sorted = group.sort_values(by=[\"Numero Lista\"])\n",
    "        movements = group_sorted.to_dict(orient=\"records\")\n",
    "        order = {\n",
    "            \"clientName\": client_name,\n",
    "            \"documentDate\": doc_date,\n",
    "            \"movements\": movements,\n",
    "        }\n",
    "        orders.append(order)\n",
    "\n",
    "    return orders\n",
    "\n",
    "\n",
    "# Convert to JSON-like structure\n",
    "orders_json = convert_to_json(df_sorted)\n",
    "\n",
    "# Save the orders JSON to a file\n",
    "output_path = \"X:/artor/Downloads/json.json\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(orders_json, f, indent=4, cls=NumpyEncoder)\n",
    "\n",
    "print(\"JSON file created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_18188\\1289265450.py:14: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sorted_df = grouped.apply(lambda x: x.sort_values(by='Data Documento Precedente')).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cleaned and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"X:/artor/Downloads/mov_ven_01_06_2024.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Sort by 'Numero Lista'\n",
    "df_sorted = df.sort_values(by=[\"Numero Lista\"])\n",
    "\n",
    "# Group by 'Ragione Sociale Cliente'\n",
    "grouped = df_sorted.groupby([\"Ragione Sociale Cliente\"])\n",
    "\n",
    "# Sort each group by 'Data Documento Attuale'\n",
    "sorted_df = grouped.apply(\n",
    "    lambda x: x.sort_values(by=\"Data Documento Precedente\")\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Drop columns that are not useful for our purposes\n",
    "columns_to_keep = [\n",
    "    \"Ragione Sociale Cliente\",\n",
    "    \"Codice Cliente\",\n",
    "    \"Codice Agente\",\n",
    "    \"Identificativo Articolo\",\n",
    "    \"Codice Articolo\",\n",
    "    \"Marca Articolo\",\n",
    "    \"Descrizione Articolo\",\n",
    "    \"Valore\",\n",
    "    \"Mese\",\n",
    "    \"Anno\",\n",
    "    \"Costo\",\n",
    "    \"Numero Lista\",\n",
    "    \"Codice Cliente\",\n",
    "    \"Prezzo Articolo\",\n",
    "    \"Valore\",\n",
    "    \"Data Documento Precedente\",\n",
    "    \"Categoria Sconto Vendita\",\n",
    "]\n",
    "\n",
    "# Save the cleaned DataFrame to a new Excel file\n",
    "output_path = \"X:/artor/Downloads/cleaned_mov_ven_01_06_2024.xlsx\"\n",
    "cleaned_df.to_excel(output_path, index=False)\n",
    "\n",
    "print(\"Dataset cleaned and saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to convert the dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_18188\\246304612.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sorted_df = grouped.apply(lambda x: x.sort_values(by='Data Documento Precedente')).reset_index(drop=True)\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_18188\\246304612.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '<DatetimeArray>\n",
      "[]\n",
      "Length: 0, dtype: datetime64[ns]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  sorted_df.loc[missing_dates_mask, 'Data Documento Precedente'] = pd.to_datetime(\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_18188\\246304612.py:22: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  sorted_df['Codice Agente'] = sorted_df.groupby('Ragione Sociale Cliente')['Codice Agente'].fillna(method='ffill')\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_18188\\246304612.py:22: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  sorted_df['Codice Agente'] = sorted_df.groupby('Ragione Sociale Cliente')['Codice Agente'].fillna(method='ffill')\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_18188\\246304612.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleaned_df['Valore'] = cleaned_df['Valore'].str.replace(',', '.').astype(float)\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_18188\\246304612.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleaned_df['Costo'] = cleaned_df['Costo'].str.replace(',', '.').astype(float)\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_18188\\246304612.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleaned_df['Prezzo Articolo'] = cleaned_df['Prezzo Articolo'].str.replace(',', '.').astype(float)\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_18188\\246304612.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleaned_df['Data Documento Precedente'] = pd.to_datetime(cleaned_df['Data Documento Precedente'], format='%Y%m%d', errors='coerce')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'X:/artor/Downloads/mov_01-01-2024_to_12-06-2024_final.xlsx'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the original dataset\n",
    "file_path = \"X:/artor/Downloads/mov_ven_01_06_2024_original.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Sort by 'Numero Lista'\n",
    "df_sorted = df.sort_values(by=[\"Numero Lista\"])\n",
    "\n",
    "# Group by 'Ragione Sociale Cliente' and sort each group by 'Data Documento Precedente'\n",
    "grouped = df_sorted.groupby([\"Ragione Sociale Cliente\"])\n",
    "sorted_df = grouped.apply(\n",
    "    lambda x: x.sort_values(by=\"Data Documento Precedente\")\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Fill missing 'Data Documento Precedente' using 'Mese' and 'Anno'\n",
    "missing_dates_mask = sorted_df[\"Data Documento Precedente\"].isna()\n",
    "sorted_df.loc[missing_dates_mask, \"Data Documento Precedente\"] = pd.to_datetime(\n",
    "    sorted_df.loc[missing_dates_mask, \"Anno\"].astype(str)\n",
    "    + \"-\"\n",
    "    + sorted_df.loc[missing_dates_mask, \"Mese\"].astype(str)\n",
    "    + \"-01\"\n",
    ")\n",
    "\n",
    "# Forward fill the 'Codice Agente' based on the most recent known value for each 'Ragione Sociale Cliente'\n",
    "sorted_df[\"Codice Agente\"] = sorted_df.groupby(\"Ragione Sociale Cliente\")[\n",
    "    \"Codice Agente\"\n",
    "].fillna(method=\"ffill\")\n",
    "\n",
    "# Drop unnecessary columns, keep 'Categoria Sconto Vendita'\n",
    "columns_to_keep = [\n",
    "    \"Ragione Sociale Cliente\",\n",
    "    \"Codice Cliente\",\n",
    "    \"Codice Agente\",\n",
    "    \"Identificativo Articolo\",\n",
    "    \"Codice Articolo\",\n",
    "    \"Marca Articolo\",\n",
    "    \"Descrizione Articolo\",\n",
    "    \"Valore\",\n",
    "    \"Mese\",\n",
    "    \"Anno\",\n",
    "    \"Costo\",\n",
    "    \"Numero Lista\",\n",
    "    \"Prezzo Articolo\",\n",
    "    \"Data Documento Precedente\",\n",
    "    \"Categoria Sconto Vendita\",\n",
    "]\n",
    "cleaned_df = sorted_df[columns_to_keep]\n",
    "\n",
    "# Convert numeric columns to appropriate data types\n",
    "cleaned_df[\"Valore\"] = cleaned_df[\"Valore\"].str.replace(\",\", \".\").astype(float)\n",
    "cleaned_df[\"Costo\"] = cleaned_df[\"Costo\"].str.replace(\",\", \".\").astype(float)\n",
    "cleaned_df[\"Prezzo Articolo\"] = (\n",
    "    cleaned_df[\"Prezzo Articolo\"].str.replace(\",\", \".\").astype(float)\n",
    ")\n",
    "\n",
    "# Convert 'Data Documento Precedente' to datetime format\n",
    "cleaned_df[\"Data Documento Precedente\"] = pd.to_datetime(\n",
    "    cleaned_df[\"Data Documento Precedente\"], format=\"%Y%m%d\", errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# Save the cleaned dataset to a new Excel file\n",
    "cleaned_file_path = \"X:/artor/Downloads/mov_01-01-2024_to_12-06-2024_final.xlsx\"\n",
    "cleaned_df.to_excel(cleaned_file_path, index=False)\n",
    "\n",
    "cleaned_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_23496\\537073587.py:13: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sorted_df = grouped.apply(lambda x: x.sort_values(by='Data Documento Precedente')).reset_index(drop=True)\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_23496\\537073587.py:27: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  sorted_df['Codice Agente'] = sorted_df.groupby('Ragione Sociale Cliente')['Codice Agente'].fillna(method='ffill')\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_23496\\537073587.py:27: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  sorted_df['Codice Agente'] = sorted_df.groupby('Ragione Sociale Cliente')['Codice Agente'].fillna(method='ffill')\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_23496\\537073587.py:30: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  sorted_df['Codice Agente'] = sorted_df.groupby('Ragione Sociale Cliente')['Codice Agente'].fillna(method='bfill')\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_23496\\537073587.py:30: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  sorted_df['Codice Agente'] = sorted_df.groupby('Ragione Sociale Cliente')['Codice Agente'].fillna(method='bfill')\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_23496\\537073587.py:33: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  sorted_df['Codice Agente'].fillna(99, inplace=True)\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_23496\\537073587.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleaned_df['Valore'] = cleaned_df['Valore'].str.replace(',', '.').astype(float)\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_23496\\537073587.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleaned_df['Costo'] = cleaned_df['Costo'].str.replace(',', '.').astype(float)\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_23496\\537073587.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleaned_df['Prezzo Articolo'] = cleaned_df['Prezzo Articolo'].str.replace(',', '.').astype(float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing 'Data Documento Precedente' after filling: 0\n",
      "Missing 'Codice Agente' after filling: 0\n",
      "Cleaning process completed and saved to X:/artor/Downloads/mov_01-01-2024_to_12-06-2024_finalv3.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the original dataset\n",
    "file_path = \"X:/artor/Downloads/mov_ven_01_06_2024_original.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Sort by 'Numero Lista'\n",
    "df_sorted = df.sort_values(by=[\"Numero Lista\"])\n",
    "\n",
    "# Group by 'Ragione Sociale Cliente' and sort each group by 'Data Documento Precedente'\n",
    "df_sorted[\"Data Documento Precedente\"] = pd.to_datetime(\n",
    "    df_sorted[\"Data Documento Precedente\"], format=\"%Y%m%d\", errors=\"coerce\"\n",
    ")\n",
    "grouped = df_sorted.groupby([\"Ragione Sociale Cliente\"])\n",
    "sorted_df = grouped.apply(\n",
    "    lambda x: x.sort_values(by=\"Data Documento Precedente\")\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Fill missing 'Data Documento Precedente' using 'Mese' and 'Anno'\n",
    "missing_dates_mask = sorted_df[\"Data Documento Precedente\"].isna()\n",
    "sorted_df.loc[missing_dates_mask, \"Data Documento Precedente\"] = pd.to_datetime(\n",
    "    sorted_df.loc[missing_dates_mask, \"Anno\"].astype(str)\n",
    "    + \"-\"\n",
    "    + sorted_df.loc[missing_dates_mask, \"Mese\"].astype(str)\n",
    "    + \"-01\"\n",
    ")\n",
    "\n",
    "# Verify that no 'Data Documento Precedente' values are missing\n",
    "missing_dates_after = sorted_df[\"Data Documento Precedente\"].isna().sum()\n",
    "print(f\"Missing 'Data Documento Precedente' after filling: {missing_dates_after}\")\n",
    "\n",
    "# Forward fill the 'Codice Agente' based on the most recent known value for each 'Ragione Sociale Cliente'\n",
    "sorted_df[\"Codice Agente\"] = sorted_df.groupby(\"Ragione Sociale Cliente\")[\n",
    "    \"Codice Agente\"\n",
    "].fillna(method=\"ffill\")\n",
    "\n",
    "# Backward fill in case there are still missing values at the beginning of the groups\n",
    "sorted_df[\"Codice Agente\"] = sorted_df.groupby(\"Ragione Sociale Cliente\")[\n",
    "    \"Codice Agente\"\n",
    "].fillna(method=\"bfill\")\n",
    "\n",
    "# Assign temporary Codice Agente for any remaining missing values\n",
    "sorted_df[\"Codice Agente\"].fillna(99, inplace=True)\n",
    "\n",
    "# Verify that no 'Codice Agente' values are missing\n",
    "missing_agente_after = sorted_df[\"Codice Agente\"].isna().sum()\n",
    "print(f\"Missing 'Codice Agente' after filling: {missing_agente_after}\")\n",
    "\n",
    "# Drop unnecessary columns, keep 'Categoria Sconto Vendita'\n",
    "columns_to_keep = [\n",
    "    \"Data Documento Precedente\",\n",
    "    \"Mese\",\n",
    "    \"Anno\",\n",
    "    \"Ragione Sociale Cliente\",\n",
    "    \"Codice Cliente\",\n",
    "    \"Codice Agente\",\n",
    "    \"Identificativo Articolo\",\n",
    "    \"Codice Articolo\",\n",
    "    \"Marca Articolo\",\n",
    "    \"Descrizione Articolo\",\n",
    "    \"Valore\",\n",
    "    \"Costo\",\n",
    "    \"Numero Lista\",\n",
    "    \"Prezzo Articolo\",\n",
    "    \"Categoria Sconto Vendita\",\n",
    "]\n",
    "cleaned_df = sorted_df[columns_to_keep]\n",
    "\n",
    "# Convert numeric columns to appropriate data types\n",
    "cleaned_df[\"Valore\"] = cleaned_df[\"Valore\"].str.replace(\",\", \".\").astype(float)\n",
    "cleaned_df[\"Costo\"] = cleaned_df[\"Costo\"].str.replace(\",\", \".\").astype(float)\n",
    "cleaned_df[\"Prezzo Articolo\"] = (\n",
    "    cleaned_df[\"Prezzo Articolo\"].str.replace(\",\", \".\").astype(float)\n",
    ")\n",
    "\n",
    "# Save the cleaned dataset to a new Excel file\n",
    "cleaned_file_path = \"X:/artor/Downloads/mov_01-01-2024_to_12-06-2024_finalv3.xlsx\"\n",
    "cleaned_df.to_excel(cleaned_file_path, index=False)\n",
    "\n",
    "print(\"Cleaning process completed and saved to\", cleaned_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Shape Comparison': False,\n",
       " 'Column Names Comparison': False,\n",
       " 'First Few Rows Comparison': False}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the cleaned dataset provided by the user\n",
    "cleaned_user_file_path = \"X:/artor/Downloads/movimenti01_2024_06_2024_cleaned.xlsx\"\n",
    "cleaned_user_df = pd.read_excel(cleaned_user_file_path)\n",
    "\n",
    "# Load the cleaned dataset from our process\n",
    "cleaned_our_file_path = \"X:/artor/Downloads/mov_01-01-2024_to_12-06-2024_final.xlsx\"\n",
    "cleaned_our_df = pd.read_excel(cleaned_our_file_path)\n",
    "\n",
    "# Compare the two datasets\n",
    "comparison_results = {\n",
    "    \"Shape Comparison\": cleaned_user_df.shape == cleaned_our_df.shape,\n",
    "    \"Column Names Comparison\": cleaned_user_df.columns.tolist()\n",
    "    == cleaned_our_df.columns.tolist(),\n",
    "    \"First Few Rows Comparison\": cleaned_user_df.head().equals(cleaned_our_df.head()),\n",
    "}\n",
    "\n",
    "comparison_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\p'\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_9468\\2893529883.py:3: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  input_file_path = 'frontend\\public\\datasetsfrom01JANto12JUN.json'  # Path to your dataset JSON file\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_9468\\2893529883.py:4: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  output_file_path = 'frontend\\public\\datasetsfrom01JANto12JUN.min.json'  # Path for the minified JSON file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minified JSON file created successfully!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "input_file_path = \"frontend\\public\\data\\datasetsfrom01JANto12JUN.json\"  # Path to your dataset JSON file\n",
    "output_file_path = \"frontend\\public\\data\\datasetsfrom01JANto12JUN.min.json\"  # Path for the minified JSON file\n",
    "\n",
    "with open(input_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "minified_json = json.dumps(data, separators=(\",\", \":\"))\n",
    "\n",
    "with open(output_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(minified_json)\n",
    "\n",
    "print(\"Minified JSON file created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minified JSON file created successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\p'\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_3292\\2193760862.py:3: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  input_file_path = 'frontend\\public\\clientdetailsdataset02072024.json'  # Path to your dataset JSON file\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_3292\\2193760862.py:4: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  output_file_path = 'frontend\\public\\clientdetailsdataset02072024.min.json'  # Path for the minified JSON file\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "input_file_path = \"frontend\\public\\data\\clientdetailsdataset02072024.json\"  # Path to your dataset JSON file\n",
    "output_file_path = \"frontend\\public\\data\\clientdetailsdataset02072024.min.json\"  # Path for the minified JSON file\n",
    "\n",
    "with open(input_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "minified_json = json.dumps(data, separators=(\",\", \":\"))\n",
    "\n",
    "with open(output_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(minified_json)\n",
    "\n",
    "print(\"Minified JSON file created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_23496\\1128323243.py:13: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sorted_df = grouped.apply(lambda x: x.sort_values(by='Data Documento Precedente')).reset_index(drop=True)\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_23496\\1128323243.py:27: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  sorted_df['Codice Agente'] = sorted_df.groupby('Ragione Sociale Cliente')['Codice Agente'].fillna(method='ffill')\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_23496\\1128323243.py:27: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  sorted_df['Codice Agente'] = sorted_df.groupby('Ragione Sociale Cliente')['Codice Agente'].fillna(method='ffill')\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_23496\\1128323243.py:30: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  sorted_df['Codice Agente'] = sorted_df.groupby('Ragione Sociale Cliente')['Codice Agente'].fillna(method='bfill')\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_23496\\1128323243.py:30: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  sorted_df['Codice Agente'] = sorted_df.groupby('Ragione Sociale Cliente')['Codice Agente'].fillna(method='bfill')\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_23496\\1128323243.py:33: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  sorted_df['Codice Agente'].fillna(99, inplace=True)\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_23496\\1128323243.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleaned_df['Valore'] = cleaned_df['Valore'].str.replace(',', '.').astype(float)\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_23496\\1128323243.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleaned_df['Costo'] = cleaned_df['Costo'].str.replace(',', '.').astype(float)\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_23496\\1128323243.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleaned_df['Prezzo Articolo'] = cleaned_df['Prezzo Articolo'].str.replace(',', '.').astype(float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing 'Data Documento Precedente' after filling: 0\n",
      "Missing 'Codice Agente' after filling: 0\n",
      "Cleaning process completed and saved to X:/artor/Downloads/mov_01-01-2024_to_12-06-2024_finalv3.xlsx\n",
      "JSON file saved to X:/artor/Downloads/mov_01-01-2024_to_12-06-2024_final.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the original dataset\n",
    "file_path = \"X:/artor/Downloads/rcs/mov_ven_01_06_2024.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Sort by 'Numero Lista'\n",
    "df_sorted = df.sort_values(by=[\"Numero Lista\"])\n",
    "\n",
    "# Group by 'Ragione Sociale Cliente' and sort each group by 'Data Documento Precedente'\n",
    "df_sorted[\"Data Documento Precedente\"] = pd.to_datetime(\n",
    "    df_sorted[\"Data Documento Precedente\"], format=\"%Y%m%d\", errors=\"coerce\"\n",
    ")\n",
    "grouped = df_sorted.groupby([\"Ragione Sociale Cliente\"])\n",
    "sorted_df = grouped.apply(\n",
    "    lambda x: x.sort_values(by=\"Data Documento Precedente\")\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Fill missing 'Data Documento Precedente' using 'Mese' and 'Anno'\n",
    "missing_dates_mask = sorted_df[\"Data Documento Precedente\"].isna()\n",
    "sorted_df.loc[missing_dates_mask, \"Data Documento Precedente\"] = pd.to_datetime(\n",
    "    sorted_df.loc[missing_dates_mask, \"Anno\"].astype(str)\n",
    "    + \"-\"\n",
    "    + sorted_df.loc[missing_dates_mask, \"Mese\"].astype(str)\n",
    "    + \"-01\"\n",
    ")\n",
    "\n",
    "# Verify that no 'Data Documento Precedente' values are missing\n",
    "missing_dates_after = sorted_df[\"Data Documento Precedente\"].isna().sum()\n",
    "print(f\"Missing 'Data Documento Precedente' after filling: {missing_dates_after}\")\n",
    "\n",
    "# Forward fill the 'Codice Agente' based on the most recent known value for each 'Ragione Sociale Cliente'\n",
    "sorted_df[\"Codice Agente\"] = sorted_df.groupby(\"Ragione Sociale Cliente\")[\n",
    "    \"Codice Agente\"\n",
    "].fillna(method=\"ffill\")\n",
    "\n",
    "# Backward fill in case there are still missing values at the beginning of the groups\n",
    "sorted_df[\"Codice Agente\"] = sorted_df.groupby(\"Ragione Sociale Cliente\")[\n",
    "    \"Codice Agente\"\n",
    "].fillna(method=\"bfill\")\n",
    "\n",
    "# Assign temporary Codice Agente for any remaining missing values\n",
    "sorted_df[\"Codice Agente\"].fillna(99, inplace=True)\n",
    "\n",
    "# Verify that no 'Codice Agente' values are missing\n",
    "missing_agente_after = sorted_df[\"Codice Agente\"].isna().sum()\n",
    "print(f\"Missing 'Codice Agente' after filling: {missing_agente_after}\")\n",
    "\n",
    "# Drop unnecessary columns, keep 'Categoria Sconto Vendita'\n",
    "columns_to_keep = [\n",
    "    \"Data Documento Precedente\",\n",
    "    \"Mese\",\n",
    "    \"Anno\",\n",
    "    \"Ragione Sociale Cliente\",\n",
    "    \"Codice Cliente\",\n",
    "    \"Codice Agente\",\n",
    "    \"Identificativo Articolo\",\n",
    "    \"Codice Articolo\",\n",
    "    \"Marca Articolo\",\n",
    "    \"Descrizione Articolo\",\n",
    "    \"Valore\",\n",
    "    \"Costo\",\n",
    "    \"Numero Lista\",\n",
    "    \"Prezzo Articolo\",\n",
    "    \"Categoria Sconto Vendita\",\n",
    "]\n",
    "cleaned_df = sorted_df[columns_to_keep]\n",
    "\n",
    "# Convert numeric columns to appropriate data types\n",
    "cleaned_df[\"Valore\"] = cleaned_df[\"Valore\"].str.replace(\",\", \".\").astype(float)\n",
    "cleaned_df[\"Costo\"] = cleaned_df[\"Costo\"].str.replace(\",\", \".\").astype(float)\n",
    "cleaned_df[\"Prezzo Articolo\"] = (\n",
    "    cleaned_df[\"Prezzo Articolo\"].str.replace(\",\", \".\").astype(float)\n",
    ")\n",
    "\n",
    "# Save the cleaned dataset to a new Excel file\n",
    "cleaned_file_path = \"X:/artor/Downloads/mov_01-01-2024_to_12-06-2024_finalv3.xlsx\"\n",
    "cleaned_df.to_excel(cleaned_file_path, index=False)\n",
    "\n",
    "# Convert to JSON format\n",
    "json_file_path = \"X:/artor/Downloads/mov_01-01-2024_to_12-06-2024_final.json\"\n",
    "cleaned_df.to_json(json_file_path, orient=\"records\", date_format=\"iso\")\n",
    "\n",
    "print(\"Cleaning process completed and saved to\", cleaned_file_path)\n",
    "print(\"JSON file saved to\", json_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_11384\\1692439486.py:13: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sorted_df = grouped.apply(lambda x: x.sort_values(by='Data Documento Precedente', ascending=False)).reset_index(drop=True)\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_11384\\1692439486.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  group['Codice Agente'].fillna(latest_agente, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing 'Data Documento Precedente' after filling: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_11384\\1692439486.py:33: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sorted_df = sorted_df.groupby('Ragione Sociale Cliente').apply(fill_agente_reverse).reset_index(drop=True)\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_11384\\1692439486.py:36: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  sorted_df['Codice Agente'].fillna(100, inplace=True)\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_11384\\1692439486.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleaned_df['Valore'] = cleaned_df['Valore'].str.replace(',', '.').astype(float)\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_11384\\1692439486.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleaned_df['Costo'] = cleaned_df['Costo'].str.replace(',', '.').astype(float)\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_11384\\1692439486.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleaned_df['Prezzo Articolo'] = cleaned_df['Prezzo Articolo'].str.replace(',', '.').astype(float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing 'Codice Agente' after filling: 0\n",
      "Cleaning process completed and saved to Z:/My Drive/rcs/mov_ven_01_06_2024_final.xlsx\n",
      "JSON file saved to Z:/My Drive/rcs/mov_ven_01_06_2024_final.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the original dataset\n",
    "file_path = \"Z:/My Drive/rcs/mov_ven_01_06_2024.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Sort by 'Numero Lista'\n",
    "df_sorted = df.sort_values(by=[\"Numero Lista\"])\n",
    "\n",
    "# Group by 'Ragione Sociale Cliente' and sort each group by 'Data Documento Precedente'\n",
    "df_sorted[\"Data Documento Precedente\"] = pd.to_datetime(\n",
    "    df_sorted[\"Data Documento Precedente\"], format=\"%Y%m%d\", errors=\"coerce\"\n",
    ")\n",
    "grouped = df_sorted.groupby([\"Ragione Sociale Cliente\"])\n",
    "sorted_df = grouped.apply(\n",
    "    lambda x: x.sort_values(by=\"Data Documento Precedente\", ascending=False)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Fill missing 'Data Documento Precedente' using 'Mese' and 'Anno'\n",
    "missing_dates_mask = sorted_df[\"Data Documento Precedente\"].isna()\n",
    "sorted_df.loc[missing_dates_mask, \"Data Documento Precedente\"] = pd.to_datetime(\n",
    "    sorted_df.loc[missing_dates_mask, \"Anno\"].astype(str)\n",
    "    + \"-\"\n",
    "    + sorted_df.loc[missing_dates_mask, \"Mese\"].astype(str)\n",
    "    + \"-01\"\n",
    ")\n",
    "\n",
    "# Verify that no 'Data Documento Precedente' values are missing\n",
    "missing_dates_after = sorted_df[\"Data Documento Precedente\"].isna().sum()\n",
    "print(f\"Missing 'Data Documento Precedente' after filling: {missing_dates_after}\")\n",
    "\n",
    "\n",
    "# Apply the Codice Agente in reverse order based on the latest entry\n",
    "def fill_agente_reverse(group):\n",
    "    group = group.sort_values(by=\"Data Documento Precedente\", ascending=False)\n",
    "    latest_agente = group[\"Codice Agente\"].iloc[0]\n",
    "    group[\"Codice Agente\"].fillna(latest_agente, inplace=True)\n",
    "    return group\n",
    "\n",
    "\n",
    "sorted_df = (\n",
    "    sorted_df.groupby(\"Ragione Sociale Cliente\")\n",
    "    .apply(fill_agente_reverse)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Assign temporary Codice Agente for any remaining missing values\n",
    "sorted_df[\"Codice Agente\"].fillna(100, inplace=True)\n",
    "\n",
    "# Verify that no 'Codice Agente' values are missing\n",
    "missing_agente_after = sorted_df[\"Codice Agente\"].isna().sum()\n",
    "print(f\"Missing 'Codice Agente' after filling: {missing_agente_after}\")\n",
    "\n",
    "# Drop entries where 'Codice Articolo', 'Marca Articolo', or 'Categoria Sconto Vendita' are \"NC\"\n",
    "filtered_df = sorted_df[\n",
    "    (sorted_df[\"Codice Articolo\"] != \"NC\")\n",
    "    & (sorted_df[\"Marca Articolo\"] != \"NC\")\n",
    "    & (sorted_df[\"Categoria Sconto Vendita\"] != \"NC\")\n",
    "]\n",
    "\n",
    "# Drop unnecessary columns, keep 'Categoria Sconto Vendita'\n",
    "columns_to_keep = [\n",
    "    \"Data Documento Precedente\",\n",
    "    \"Mese\",\n",
    "    \"Anno\",\n",
    "    \"Ragione Sociale Cliente\",\n",
    "    \"Codice Cliente\",\n",
    "    \"Codice Agente\",\n",
    "    \"Identificativo Articolo\",\n",
    "    \"Codice Articolo\",\n",
    "    \"Marca Articolo\",\n",
    "    \"Descrizione Articolo\",\n",
    "    \"Valore\",\n",
    "    \"Costo\",\n",
    "    \"Numero Lista\",\n",
    "    \"Prezzo Articolo\",\n",
    "    \"Categoria Sconto Vendita\",\n",
    "]\n",
    "cleaned_df = filtered_df[columns_to_keep]\n",
    "\n",
    "# Convert numeric columns to appropriate data types\n",
    "cleaned_df[\"Valore\"] = cleaned_df[\"Valore\"].str.replace(\",\", \".\").astype(float)\n",
    "cleaned_df[\"Costo\"] = cleaned_df[\"Costo\"].str.replace(\",\", \".\").astype(float)\n",
    "cleaned_df[\"Prezzo Articolo\"] = (\n",
    "    cleaned_df[\"Prezzo Articolo\"].str.replace(\",\", \".\").astype(float)\n",
    ")\n",
    "\n",
    "# Save the cleaned dataset to a new Excel file\n",
    "cleaned_file_path = \"Z:/My Drive/rcs/mov_ven_01_06_2024_final.xlsx\"\n",
    "cleaned_df.to_excel(cleaned_file_path, index=False)\n",
    "\n",
    "# Convert to JSON format\n",
    "json_file_path = \"Z:/My Drive/rcs/mov_ven_01_06_2024_final.json\"\n",
    "cleaned_df.to_json(json_file_path, orient=\"records\", date_format=\"iso\")\n",
    "\n",
    "print(\"Cleaning process completed and saved to\", cleaned_file_path)\n",
    "print(\"JSON file saved to\", json_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLS file X:/artor/Downloads/rcs/Estrapolo clienti con dettagli per Damiano.xls successfully processed. Cleaned file saved to X:/artor/Downloads/rcs/Estrapolo clienti con dettagli per Damiano_cleaned.csv.\n"
     ]
    }
   ],
   "source": [
    "import xlrd\n",
    "import csv\n",
    "\n",
    "xls_file = \"X:/artor/Downloads/rcs/Estrapolo clienti con dettagli per Damiano.xls\"\n",
    "csv_output_file = (\n",
    "    \"X:/artor/Downloads/rcs/Estrapolo clienti con dettagli per Damiano_cleaned.csv\"\n",
    ")\n",
    "\n",
    "\n",
    "# Function to clean and process each row from the XLS file\n",
    "def process_xls(input_file, output_file):\n",
    "    wb = xlrd.open_workbook(input_file)\n",
    "    sheet = wb.sheet_by_index(0)\n",
    "\n",
    "    with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as csvout:\n",
    "        csvwriter = csv.writer(csvout)\n",
    "\n",
    "        # Write header row\n",
    "        csvwriter.writerow(sheet.row_values(0))\n",
    "\n",
    "        # Process each row starting from the second row (index 1)\n",
    "        for row_idx in range(1, sheet.nrows):\n",
    "            row = sheet.row_values(row_idx)\n",
    "\n",
    "            # Convert row values to strings\n",
    "            row = [str(val) for val in row]\n",
    "\n",
    "            # Remove leading zeros from CODICE (assuming it's the first column, adjust if necessary)\n",
    "            codice_cliente = row[0].lstrip(\"0\")\n",
    "            row[0] = codice_cliente\n",
    "\n",
    "            # Assign placeholder AG code 100 if missing (assuming AG is the last column, adjust if necessary)\n",
    "            ag_code = row[-1]\n",
    "            if not ag_code.strip():\n",
    "                row[-1] = \"100\"\n",
    "\n",
    "            # Write cleaned row to CSV\n",
    "            csvwriter.writerow(row)\n",
    "\n",
    "    print(\n",
    "        f\"XLS file {input_file} successfully processed. Cleaned file saved to {output_file}.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Process the XLS file\n",
    "process_xls(xls_file, csv_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLS file Z:/My Drive/rcs/Estrapolo clienti con dettagli per Damiano.xls successfully processed. Cleaned data saved to D:/coding/rcs_management_project-1/frontend/public/data/clientdetailsdataset02072024.min.json.\n"
     ]
    }
   ],
   "source": [
    "import xlrd\n",
    "import json\n",
    "\n",
    "# Define the input and output file paths\n",
    "xls_file = \"Z:/My Drive/rcs/Estrapolo clienti con dettagli per Damiano.xls\"\n",
    "json_output_file = \"D:/coding/rcs_management_project-1/frontend/public/data/clientdetailsdataset02072024.min.json\"\n",
    "\n",
    "# List of foreign \"RAGIONE SOCIALE\"\n",
    "foreign_ragione_sociale = [\n",
    "    \"KOSSER JSC\",\n",
    "    \"PRESTOLITE ELECTIRC LIMITED\",\n",
    "    \"SHF TRADE TD LTD\",\n",
    "    \"NEAT AUTOS LTD\",\n",
    "    \"SUPERIORCARUSO SERVICE\",\n",
    "    \"MOTOR & TRANSMISSIONSTEKNIK\",\n",
    "    \"RAIN GROUP TRADING FZE\",\n",
    "    \"LUCAS DIESEL Inyeccion Diesel\",\n",
    "    \"SURIPE_x001a_S - IMPORTA_x001a_O E\",\n",
    "    \"AOI SOLUTIONS GMBH\",\n",
    "    \"MARIO SPITERI PARTS & SERVICE\",\n",
    "    \"HOLSTEIN-HANDEL\",\n",
    "    \"RAY CINI TRANS\",\n",
    "    \"GAMMA TECHNICAL CORPORATION\",\n",
    "    \"GOBITRADE LTD.\",\n",
    "    \"CARPARTS IMPORT EXPORT KFT\",\n",
    "    \"REMANTE GROUP S.R.O.\",\n",
    "    \"PURIC D.O.O.\",\n",
    "    \"FERDINAND BILSTEIN UK LTD\",\n",
    "    \"ATLANTIC SPARE PARTS\",\n",
    "    \"GEZAIRI CO. Attn: Mr. Halo\",\n",
    "    \"RECANPRI, S.L.\",\n",
    "    \"SPH - SPARE PARTS HOUSE LDA\",\n",
    "    \"DELPHI DIESEL AFTERMARKET\",\n",
    "    \"PRESTOLITE ELECTRIC LIMITED\",\n",
    "    \"AVESA S.L.\",\n",
    "    \"TECNODIESEL MURCIA S.L.\",\n",
    "    \"MERLIN DIESEL SYSTEMS LTD\",\n",
    "    \"MERLIN DIESEL SYSTEM LTD\",\n",
    "    \"AMAZON EU S.A.R.L.\",\n",
    "    \"JOHN'S DIESEL SERVICE'S SQAQ\",\n",
    "    \"S.C DIESEL SOF S.R.L. STR\",\n",
    "    \"ARROWTRONIC LTD-EMS\",\n",
    "    \"HORIZON PROCUREMENTS\",\n",
    "    \"SARL PROIETTI FARNESE\",\n",
    "    \"TYRONE DIESEL SYSTEMS\",\n",
    "    \"DIESELWELT BRENNER GMBH\",\n",
    "    \"TURBOTECNIC AUTOMOCIO S.L.\",\n",
    "    \"KUNSHAN POWER ASIA IMPORT &\",\n",
    "    \"ORION PART OTOM.IC VE DIS\",\n",
    "    \"GUANGZHOU BAISITE AUTO PARTS\",\n",
    "    \"IPT OTOMOTIV DIS TICARET\",\n",
    "    \"KONING TECHNISCH BEDRIJF B.V.\",\n",
    "    \"PORTLAOISE DIESEL INJECTION\",\n",
    "    \"CORCORANS ENGINEERING\",\n",
    "    \"ZHENGZHOU LISERON OIL PUMP &\",\n",
    "    \"LKV TEKNIK OTOMOTIV SANAYI IC\",\n",
    "    \"GUANGDONG TOPWAY TECH CO LTD\",\n",
    "    \"UNITED FUEL INJECTION\",\n",
    "    \"LA CASA DEL DIESEL S.L.\",\n",
    "    \"OTT-PUTIAN DIESEL SPARE PARTS\",\n",
    "    \"AUTORAK SP. Z.O.O.\",\n",
    "    \"IB TRADE\",\n",
    "    \"FLOTA SUARDIAZ S.L.U.\",\n",
    "    \"2T + M KFT\",\n",
    "    \"KENDIESEL\",\n",
    "    \"CASTY-ROMER S.L.\",\n",
    "    \"DIESELSERVICE STOKKING B.V.\",\n",
    "    \"SUPER ENGINEERING WORKS\",\n",
    "    \"DIJITAL PARCA PLATFORMU SATIS\",\n",
    "    \"GOODAPP\",\n",
    "    \"INJECCIO DIESEL JORDA, S.L\",\n",
    "    \"GUNEYBAGLILAR OTO YEDEK PAR.\",\n",
    "    \"D. HAUPT GMBH\",\n",
    "    \"ASIST OTO YEDEK PARCA SANAYI\",\n",
    "    \"UHURU MAX LIMITED\",\n",
    "    \"SYDNEY DIESEL CENTRE\",\n",
    "    \"TALLERES RUFRE S.L.\",\n",
    "    \"AB IMPORTS\",\n",
    "    \"FACEWORKS LIMITED\",\n",
    "    \"William Psaila\",\n",
    "    \"EURO PARTS TRUCKS & BUSES\",\n",
    "    \"PORTLAOISE DIESEL LTD\",\n",
    "    \"SUARDIAZ MANAGEMENT SERVICES\",\n",
    "    \"EAST HOI TRADING COMPANY FLAT\",\n",
    "    \"Karadaglar group otomotiv\",\n",
    "    \"KARCOM LTD\",\n",
    "    \"ERB ITALY SRL\",\n",
    "    \"A.F. BAEDER GES.M.B.H.\",\n",
    "    \"DIESEL INJECTOR REPAIR Ltd\",\n",
    "    \"FAHRENHEIT LOGISTICS LIMITED\",\n",
    "    \"S T EARTHMOVING SERVICES\",\n",
    "    \"GOLDFARB  and ASSOCIATES\",\n",
    "    \"PANDIESEL AEBE\",\n",
    "    \"ADF DIESEL\",\n",
    "    \"AutoDieselPart LLC\",\n",
    "    \"Goldfarb & Associates, Inc\",\n",
    "]\n",
    "\n",
    "\n",
    "# Function to clean and process each row from the XLS file and convert to JSON\n",
    "def process_xls_to_json(input_file, output_file):\n",
    "    wb = xlrd.open_workbook(input_file)\n",
    "    sheet = wb.sheet_by_index(0)\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # Process each row starting from the second row (index 1)\n",
    "    for row_idx in range(1, sheet.nrows):\n",
    "        row = sheet.row_values(row_idx)\n",
    "\n",
    "        # Convert row values to strings\n",
    "        row = [str(val) for val in row]\n",
    "\n",
    "        # Remove leading zeros from CODICE (assuming it's the first column, adjust if necessary)\n",
    "        codice_cliente = row[0].lstrip(\"0\")\n",
    "        row[0] = codice_cliente\n",
    "\n",
    "        # Assign placeholder AG code 100 if missing or if AG code is 0 (assuming AG is the last column, adjust if necessary)\n",
    "        ag_code = row[-1]\n",
    "        if not ag_code.strip() or ag_code == \"0\":\n",
    "            row[-1] = \"100\"\n",
    "\n",
    "        # Check for foreign \"RAGIONE SOCIALE\" and assign AG 50\n",
    "        if row[1] in foreign_ragione_sociale:\n",
    "            row[-1] = \"50\"\n",
    "\n",
    "        # Check for \"RAGIONE SOCIALE\" exactly matching \"CLIENTE DA WEB\" and assign AG 60\n",
    "        if row[1].strip().upper() == \"CLIENTE DA WEB\":\n",
    "            row[-1] = \"60\"\n",
    "\n",
    "        # Append cleaned row to data list as a dictionary\n",
    "        data.append(dict(zip(sheet.row_values(0), row)))\n",
    "\n",
    "    # Write data to JSON file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as jsonout:\n",
    "        json.dump(data, jsonout, separators=(\",\", \":\"), ensure_ascii=False)\n",
    "\n",
    "    print(\n",
    "        f\"XLS file {input_file} successfully processed. Cleaned data saved to {output_file}.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Process the XLS file\n",
    "process_xls_to_json(xls_file, json_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent data with nested clients has been written to Z:/My Drive/rcs/business analyst/agentDetailsYear2024-StatsSoFar-cleaned.min.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the paths to the input and output files\n",
    "client_data_path = \"Z:/My Drive/rcs/business analyst/clientdetailsdataset02072024.min.json\"  # Path to the existing client data JSON file\n",
    "agent_data_path = \"Z:/My Drive/rcs/business analyst/agentDetailsYear2024-StatsSoFar-cleaned.min.json\"  # Path to the new agent data JSON file\n",
    "\n",
    "# Load client data\n",
    "with open(client_data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    clients = json.load(f)\n",
    "\n",
    "# Define a default dictionary to store agents and their clients\n",
    "agents_dict = defaultdict(\n",
    "    lambda: {\"id\": \"\", \"name\": \"\", \"email\": \"\", \"phone\": \"\", \"clients\": []}\n",
    ")\n",
    "\n",
    "# Define agent details (normally you would load this from a file or database)\n",
    "agent_details = [\n",
    "    {\"id\": \"10\", \"name\": \"Fazio Adriano Salvatore\"},\n",
    "    {\"id\": \"11\", \"name\": \"Salvatore Spinella\"},\n",
    "    {\"id\": \"12\", \"name\": \"Riccardo Carpentiere\"},\n",
    "    {\"id\": \"13\", \"name\": \"Vito D'Antonio\"},\n",
    "    {\"id\": \"14\", \"name\": \"G.C.\"},\n",
    "    {\"id\": \"15\", \"name\": \"Luddeni Renato\"},\n",
    "    {\"id\": \"16\", \"name\": \"Luca Scaffo\"},\n",
    "    {\"id\": \"50\", \"name\": \"Marco Coppola\"},\n",
    "    {\"id\": \"60\", \"name\": \"Web\"},\n",
    "    {\"id\": \"90\", \"name\": \"Direzionale\"},\n",
    "    {\"id\": \"91\", \"name\": \"Direzionale Diesel\"},\n",
    "    {\"id\": \"92\", \"name\": \"Direzionale D\"},\n",
    "    {\"id\": \"95\", \"name\": \"Cliente Agente\"},\n",
    "    {\"id\": \"99\", \"name\": \"Seguito da Avvocato\"},\n",
    "    {\"id\": \"100\", \"name\": \"Non Assegnato\"},\n",
    "]\n",
    "\n",
    "# Initialize the agents dictionary with agent details\n",
    "for agent in agent_details:\n",
    "    agents_dict[agent[\"id\"]][\"id\"] = agent[\"id\"]\n",
    "    agents_dict[agent[\"id\"]][\"name\"] = agent[\"name\"]\n",
    "\n",
    "# Assign clients to their respective agents\n",
    "for client in clients:\n",
    "    agent_id = client[\"AG\"]\n",
    "    agents_dict[agent_id][\"clients\"].append(client)\n",
    "\n",
    "# Convert the default dictionary to a list\n",
    "agents_list = list(agents_dict.values())\n",
    "\n",
    "# Write the output to a new JSON file\n",
    "with open(agent_data_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(agents_list, f, separators=(\",\", \":\"), ensure_ascii=False)\n",
    "\n",
    "print(\"Agent data with nested clients has been written to\", agent_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing 'AGENTE' after filling: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_5392\\828893886.py:116: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['DATA_MOV'] = pd.to_datetime(filtered_df['DATA_MOV'], format='%d/%m/%Y', errors='coerce')\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_5392\\828893886.py:118: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sorted_df = grouped.apply(lambda x: x.sort_values(by='DATA_MOV', ascending=True)).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing 'AGENTE' after filling: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_5392\\828893886.py:116: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['DATA_MOV'] = pd.to_datetime(filtered_df['DATA_MOV'], format='%d/%m/%Y', errors='coerce')\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_5392\\828893886.py:118: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sorted_df = grouped.apply(lambda x: x.sort_values(by='DATA_MOV', ascending=True)).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing 'AGENTE' after filling: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_5392\\828893886.py:116: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['DATA_MOV'] = pd.to_datetime(filtered_df['DATA_MOV'], format='%d/%m/%Y', errors='coerce')\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_5392\\828893886.py:118: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sorted_df = grouped.apply(lambda x: x.sort_values(by='DATA_MOV', ascending=True)).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing 'AGENTE' after filling: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_5392\\828893886.py:116: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['DATA_MOV'] = pd.to_datetime(filtered_df['DATA_MOV'], format='%d/%m/%Y', errors='coerce')\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_5392\\828893886.py:118: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sorted_df = grouped.apply(lambda x: x.sort_values(by='DATA_MOV', ascending=True)).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing 'AGENTE' after filling: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_5392\\828893886.py:116: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['DATA_MOV'] = pd.to_datetime(filtered_df['DATA_MOV'], format='%d/%m/%Y', errors='coerce')\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_5392\\828893886.py:118: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sorted_df = grouped.apply(lambda x: x.sort_values(by='DATA_MOV', ascending=True)).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing 'AGENTE' after filling: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_5392\\828893886.py:116: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['DATA_MOV'] = pd.to_datetime(filtered_df['DATA_MOV'], format='%d/%m/%Y', errors='coerce')\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_5392\\828893886.py:118: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sorted_df = grouped.apply(lambda x: x.sort_values(by='DATA_MOV', ascending=True)).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing 'AGENTE' after filling: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_5392\\828893886.py:116: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['DATA_MOV'] = pd.to_datetime(filtered_df['DATA_MOV'], format='%d/%m/%Y', errors='coerce')\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_5392\\828893886.py:118: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sorted_df = grouped.apply(lambda x: x.sort_values(by='DATA_MOV', ascending=True)).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning process completed and saved to Z:/My Drive/rcs/business analyst/Year2024-StatsSoFar-cleaned.xlsx\n",
      "Dropped data saved to Z:/My Drive/rcs/business analyst/Year2024-StatsSoFar-dropped.csv\n",
      "JSON file saved to Z:/My Drive/rcs/business analyst/Year2024-StatsSoFar-cleaned.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Define the path pattern for the monthly CSV files\n",
    "file_pattern = 'Z:/My Drive/rcs/business analyst/Statistica_del_Venduto_e_Acquistato_2024_*.csv'\n",
    "file_paths = glob.glob(file_pattern)\n",
    "\n",
    "# Initialize lists to store dataframes\n",
    "df_list = []\n",
    "dropped_df_list = []\n",
    "\n",
    "# Define the foreign \"RAGIONE SOCIALE\" list\n",
    "foreign_ragione_sociale = [\n",
    "    \"KOSSER JSC\", \"PRESTOLITE ELECTIRC LIMITED\", \"SHF TRADE TD LTD\", \"NEAT AUTOS LTD\", \"SUPERIORCARUSO SERVICE\",\n",
    "    \"MOTOR & TRANSMISSIONSTEKNIK\", \"RAIN GROUP TRADING FZE\", \"LUCAS DIESEL Inyeccion Diesel\", \n",
    "    \"SURIPE_x001a_S - IMPORTA_x001a_O E\", \"AOI SOLUTIONS GMBH\", \"MARIO SPITERI PARTS & SERVICE\", \"HOLSTEIN-HANDEL\",\n",
    "    \"RAY CINI TRANS\", \"GAMMA TECHNICAL CORPORATION\", \"GOBITRADE LTD.\", \"CARPARTS IMPORT EXPORT KFT\", \n",
    "    \"REMANTE GROUP S.R.O.\", \"PURIC D.O.O.\", \"FERDINAND BILSTEIN UK LTD\", \"ATLANTIC SPARE PARTS\", \"GEZAIRI CO. Attn: Mr. Halo\",\n",
    "    \"RECANPRI, S.L.\", \"SPH - SPARE PARTS HOUSE LDA\", \"DELPHI DIESEL AFTERMARKET\", \"PRESTOLITE ELECTRIC LIMITED\", \n",
    "    \"AVESA S.L.\", \"TECNODIESEL MURCIA S.L.\", \"MERLIN DIESEL SYSTEMS LTD\", \"MERLIN DIESEL SYSTEM LTD\", \"AMAZON EU S.A.R.L.\",\n",
    "    \"JOHN'S DIESEL SERVICE'S SQAQ\", \"S.C DIESEL SOF S.R.L. STR\", \"ARROWTRONIC LTD-EMS\", \"HORIZON PROCUREMENTS\",\n",
    "    \"SARL PROIETTI FARNESE\", \"TYRONE DIESEL SYSTEMS\", \"DIESELWELT BRENNER GMBH\", \"TURBOTECNIC AUTOMOCIO S.L.\",\n",
    "    \"KUNSHAN POWER ASIA IMPORT &\", \"ORION PART OTOM.IC VE DIS\", \"GUANGZHOU BAISITE AUTO PARTS\", \"IPT OTOMOTIV DIS TICARET\",\n",
    "    \"KONING TECHNISCH BEDRIJF B.V.\", \"PORTLAOISE DIESEL INJECTION\", \"CORCORANS ENGINEERING\", \n",
    "    \"ZHENGZHOU LISERON OIL PUMP &\", \"LKV TEKNIK OTOMOTIV SANAYI IC\", \"GUANGDONG TOPWAY TECH CO LTD\", \"UNITED FUEL INJECTION\",\n",
    "    \"LA CASA DEL DIESEL S.L.\", \"OTT-PUTIAN DIESEL SPARE PARTS\", \"AUTORAK SP. Z.O.O.\", \"IB TRADE\", \"FLOTA SUARDIAZ S.L.U.\",\n",
    "    \"2T + M KFT\", \"KENDIESEL\", \"CASTY-ROMER S.L.\", \"DIESELSERVICE STOKKING B.V.\", \"SUPER ENGINEERING WORKS\",\n",
    "    \"DIJITAL PARCA PLATFORMU SATIS\", \"GOODAPP\", \"INJECCIO DIESEL JORDA, S.L\", \"GUNEYBAGLILAR OTO YEDEK PAR.\",\n",
    "    \"D. HAUPT GMBH\", \"ASIST OTO YEDEK PARCA SANAYI\", \"UHURU MAX LIMITED\", \"SYDNEY DIESEL CENTRE\", \"TALLERES RUFRE S.L.\",\n",
    "    \"AB IMPORTS\", \"FACEWORKS LIMITED\", \"William Psaila\", \"EURO PARTS TRUCKS & BUSES\", \"PORTLAOISE DIESEL LTD\",\n",
    "    \"SUARDIAZ MANAGEMENT SERVICES\", \"EAST HOI TRADING COMPANY FLAT\", \"Karadaglar group otomotiv\", \"KARCOM LTD\", \"ERB ITALY SRL\",\n",
    "    \"A.F. BAEDER GES.M.B.H.\", \"DIESEL INJECTOR REPAIR Ltd\", \"FAHRENHEIT LOGISTICS LIMITED\", \"S T EARTHMOVING SERVICES\",\n",
    "    \"GOLDFARB and ASSOCIATES\", \"PANDIESEL AEBE\", \"ADF DIESEL\", \"AutoDieselPart LLC\", \"Goldfarb & Associates, Inc\"\n",
    "]\n",
    "\n",
    "# Loop through each file and process it\n",
    "for file_path in file_paths:\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "    \n",
    "    # If you're working with the 'LISTA' column before renaming\n",
    "    df['LISTA'] = df['LISTA'].apply(lambda x: int(x.split('/')[1]) if '/' in x else int(x))\n",
    "        \n",
    "    # Fix warranty, trasporto, imballo, and eco friendly issues\n",
    "    df['CODICE_ART'] = df['CODICE_ART'].str.upper()\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('warranty', case=False, na=False) & \n",
    "           df['CODICE_ART'].isin(['', '.', 'NC', 'nc']), 'CODICE_ART'] = 'WARRANTY'\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('trasporto', case=False, na=False) & \n",
    "           df['CODICE_ART'].isin(['', '.', 'NC', 'nc', 'SPESE']), 'CODICE_ART'] = 'TRASPORTO'\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('imballo', case=False, na=False) & \n",
    "           df['CODICE_ART'].isin(['', '.', 'NC', 'nc']), 'CODICE_ART'] = 'IMBALLO'\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('ECO FRIENDLY|ECO-FRIENDLY|INIZIATIVA ECO-FRIENDLY', case=False, na=False), ['CODICE_ART', 'MARCA_ART']] = ['SPESE', 'Aziendale']\n",
    "\n",
    "    # Correct the IMBALLO logic\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('IMBALLO', case=False, na=False), 'MARCA_ART'] = 'Aziendale'\n",
    "    \n",
    "    # Fix specific values in DESCRIZIONE_ART\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('PREMIO FATTURATO', case=False, na=False), ['CODICE_ART', 'MARCA_ART']] = ['SPESE', 'Aziendale']\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('INSOLUTO|INSOLUTI', case=False, na=False), ['CODICE_ART', 'MARCA_ART']] = ['INSOLUTO', 'Aziendale']\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('spese', case=False, na=False), ['CODICE_ART', 'DESCRIZIONE_ART']] = ['SPESE', 'IMBALLO']\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('accredito per err.applic.penale', case=False, na=False), 'CODICE_ART'] = 'SPESE'\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('errato addeb. spese di incasso', case=False, na=False), 'CODICE_ART'] = 'SPESE'\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('MANODOPERA STACCO/RIATT CON PRESSA', case=False, na=False), ['CODICE_ART', 'MARCA_ART']] = ['EXTRA', 'RCS']\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('SBLOCCANTE WD40|WD-40 SBLOCCANTE|WD-40 SBLOCCANTE ', case=False, na=False), ['CODICE_ART', 'MARCA_ART']] = ['490040', 'WD-40']\n",
    "    \n",
    "    # Additional condition for SPESE URGENZA\n",
    "    df.loc[df['CODICE_ART'] == 'SPESE URGENZA', ['CODICE_ART', 'DESCRIZIONE_ART']] = ['TRASPORTO', 'TRASPORTO URGENTE']\n",
    "\n",
    "    # Fix specific values in CODICE_ART\n",
    "    df.loc[df['CODICE_ART'].isin(['TBF1214035', 'TBF1414035']) & df['MARCA_ART'].isin(['NC', '.', 'nc']), 'MARCA_ART'] = 'RCS'\n",
    "    df.loc[(df['CODICE_ART'] == '26870') & df['MARCA_ART'].isin(['NC', '.', 'nc']), 'MARCA_ART'] = 'FEBI'\n",
    "    df.loc[(df['CODICE_ART'] == '3182654192') & df['MARCA_ART'].isin(['NC', '.', 'nc']), 'MARCA_ART'] = 'SACHS'\n",
    "    \n",
    "    # Fix IMPOSTA OLIO in DESCRIZIONE_ART\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('IMPOSTA OLIO', case=False, na=False), ['CODICE_ART', 'MARCA_ART']] = ['OILVAT', 'VAT']\n",
    "    \n",
    "    # Additional conditions\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('BONUS15|BONUS_15|BONUS_25|BONUS-15', case=False, na=False), ['CODICE_ART', 'MARCA_ART']] = ['EXTRA', 'Aziendale']\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('ASSICURAZIONE 6% CASCO|ASSICURAZIONE 4033195 6%|ASSICURAZIONE', case=False, na=False), ['CODICE_ART', 'MARCA_ART']] = ['ASSICURAZIONE', 'Aziendale']\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('periodo q4/2023|PERIODO Q3/2023', case=False, na=False), ['CODICE_ART', 'MARCA_ART']] = ['BOSCHCORP', 'Aziendale']\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('Kit tubaz/flessib Mercedes', case=False, na=False), ['CODICE_ART', 'MARCA_ART']] = ['Unknown', 'MERCEDES']\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('PACKAGING', case=False, na=False), ['CODICE_ART', 'DESCRIZIONE_ART', 'MARCA_ART']] = ['SPESE', 'IMBALLO', 'Aziendale']\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('Urgenza Bosch', case=False, na=False), ['CODICE_ART', 'DESCRIZIONE_ART', 'MARCA_ART']] = ['SPESE', 'IMBALLO', 'Aziendale']\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('PROVA AL BANCO INIETT DENSO C.R.|PROVA AL BANCO INIETT DENSO', case=False, na=False), ['CODICE_ART', 'MARCA_ART']] = ['EXTRA', 'Aziendale']\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('INIETT 0445110351', case=False, na=False), ['CODICE_ART', 'MARCA_ART']] = ['0445110351', 'BOSCH']\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('INIETT 0445110351 REVIS.', case=False, na=False), ['CODICE_ART', 'MARCA_ART']] = ['0986435204', 'BOSCH']\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('premio', case=False, na=False), ['CODICE_ART', 'MARCA_ART']] = ['SPESE', 'Aziendale']\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('EUROTECH 95006227', case=False, na=False), ['CODICE_ART', 'MARCA_ART']] = ['95006227', 'EUROTEC']\n",
    "    \n",
    "    # Set ULTIMO_ACQ to 0 for specific DESCRIZIONE_ART values\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('WD40|WD-40|WARRANTY|IMPOSTA OLIO|IMBALLO|TRASPORTO|PACKAGING|Urgenza Bosch', case=False, na=False), 'ULTIMO_ACQ'] = 0\n",
    "    \n",
    "    # Assign AGENTE value of 60 for 'CLIENTE DA WEB'\n",
    "    df.loc[df['RAGIONE_SOCIALE'] == 'CLIENTE DA WEB', 'AGENTE'] = 60\n",
    "    \n",
    "    # Assign placeholder Codice Agente for any remaining missing or zero values\n",
    "    df['AGENTE'] = df['AGENTE'].apply(lambda x: '100' if pd.isna(x) or x == 0 else x)\n",
    "    \n",
    "    # Assign AG 50 for foreign \"RAGIONE SOCIALE\"\n",
    "    df['AGENTE'] = df.apply(lambda row: '50' if row['RAGIONE_SOCIALE'] in foreign_ragione_sociale else row['AGENTE'], axis=1)\n",
    "\n",
    "    # Verify that no 'AGENTE' values are missing\n",
    "    missing_agente_after = df['AGENTE'].isna().sum()\n",
    "    print(f\"Missing 'AGENTE' after filling: {missing_agente_after}\")\n",
    "\n",
    "    # Assign 'MARCA_ART' to 'Aziendale' for specific 'CODICE_ART' values\n",
    "    df.loc[df['CODICE_ART'].str.contains('EXTRA|ASSICURAZIONE|PENALE|ABBONAM-TRASP-0001|OMAGGIO|SPESE|TRASPORTO|WARRANTY', case=False, na=False), 'MARCA_ART'] = 'Aziendale'\n",
    "    \n",
    "    # Separate the rows to be dropped\n",
    "    dropped_df = df[df['MARCA_ART'].isin(['NC', '.', '', 'nc'])]\n",
    "    dropped_df_list.append(dropped_df)\n",
    "    \n",
    "    # Drop entries where 'MARCA_ART' is 'NC' or '.' or empty\n",
    "    filtered_df = df[~df['MARCA_ART'].isin(['NC', '.', '', 'nc'])]\n",
    "    \n",
    "    # Group by 'Ragione Sociale Cliente' and sort each group by 'Data Documento Precedente'\n",
    "    filtered_df['DATA_MOV'] = pd.to_datetime(filtered_df['DATA_MOV'], format='%d/%m/%Y', errors='coerce')\n",
    "    grouped = filtered_df.groupby(['RAGIONE_SOCIALE'])\n",
    "    sorted_df = grouped.apply(lambda x: x.sort_values(by='DATA_MOV', ascending=True)).reset_index(drop=True)\n",
    "    \n",
    "    # Keep specific columns and rename them\n",
    "    columns_to_keep = {\n",
    "        'DATA_MOV': 'Data Documento Precedente', \n",
    "        'LISTA': 'Numero Lista', \n",
    "        'MESE': 'Mese', \n",
    "        'ANNO': 'Anno', \n",
    "        'RAGIONE_SOCIALE': 'Ragione Sociale Cliente', \n",
    "        'CLIENTE': 'Codice Cliente', \n",
    "        'AGENTE': 'Codice Agente', \n",
    "        'CODICE_ART': 'Codice Articolo', \n",
    "        'MARCA_ART': 'Marca Articolo', \n",
    "        'DESCRIZIONE_ART': 'Descrizione Articolo', \n",
    "        'QUANTITA': 'Quantita', \n",
    "        'VAL_MERCE': 'Valore', \n",
    "        'ULTIMO_ACQ': 'Costo',\n",
    "        'PREZZO': 'Prezzo Articolo',\n",
    "    }\n",
    "    cleaned_df = filtered_df[columns_to_keep.keys()].rename(columns=columns_to_keep)\n",
    "    \n",
    "    # Update 'Costo' based on 'Prezzo Articolo'\n",
    "    cleaned_df['Costo'] = cleaned_df.apply(\n",
    "        lambda row: row['Prezzo Articolo'] * 0.9 if pd.isna(row['Costo']) or row['Costo'] == 0 and row['Prezzo Articolo'] > 0 \n",
    "        else 0 if pd.isna(row['Costo']) or row['Costo'] == 0 and row['Prezzo Articolo'] == 0 \n",
    "        else abs(row['Prezzo Articolo']) if pd.isna(row['Costo']) or row['Costo'] == 0 and row['Prezzo Articolo'] < 0 \n",
    "        else row['Costo'],\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "\n",
    "    \n",
    "    # Append the cleaned dataframe to the list\n",
    "    df_list.append(cleaned_df)\n",
    "\n",
    "# Concatenate all the dataframes\n",
    "final_df = pd.concat(df_list, ignore_index=True)\n",
    "dropped_final_df = pd.concat(dropped_df_list, ignore_index=True)\n",
    "\n",
    "# Save the cleaned dataset to a new Excel file\n",
    "cleaned_file_path = 'Z:/My Drive/rcs/business analyst/Year2024-StatsSoFar-cleaned.xlsx'\n",
    "final_df.to_excel(cleaned_file_path, index=False)\n",
    "\n",
    "# Save the dropped dataset to a new CSV file\n",
    "dropped_file_path = 'Z:/My Drive/rcs/business analyst/Year2024-StatsSoFar-dropped.csv'\n",
    "dropped_final_df.to_csv(dropped_file_path, index=False)\n",
    "\n",
    "# Convert to JSON format\n",
    "json_file_path = 'Z:/My Drive/rcs/business analyst/Year2024-StatsSoFar-cleaned.json'\n",
    "final_df.to_json(json_file_path, orient='records', date_format='iso')\n",
    "\n",
    "print(\"Cleaning process completed and saved to\", cleaned_file_path)\n",
    "print(\"Dropped data saved to\", dropped_file_path)\n",
    "print(\"JSON file saved to\", json_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing 'AGENTE' after filling: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_27408\\2068243009.py:141: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['DATA_MOV'] = pd.to_datetime(filtered_df['DATA_MOV'], format='%d/%m/%Y', errors='coerce')\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_27408\\2068243009.py:143: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sorted_df = grouped.apply(lambda x: x.sort_values(by='DATA_MOV', ascending=True)).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing 'AGENTE' after filling: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_27408\\2068243009.py:141: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['DATA_MOV'] = pd.to_datetime(filtered_df['DATA_MOV'], format='%d/%m/%Y', errors='coerce')\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_27408\\2068243009.py:143: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sorted_df = grouped.apply(lambda x: x.sort_values(by='DATA_MOV', ascending=True)).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing 'AGENTE' after filling: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_27408\\2068243009.py:141: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['DATA_MOV'] = pd.to_datetime(filtered_df['DATA_MOV'], format='%d/%m/%Y', errors='coerce')\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_27408\\2068243009.py:143: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sorted_df = grouped.apply(lambda x: x.sort_values(by='DATA_MOV', ascending=True)).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing 'AGENTE' after filling: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_27408\\2068243009.py:141: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['DATA_MOV'] = pd.to_datetime(filtered_df['DATA_MOV'], format='%d/%m/%Y', errors='coerce')\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_27408\\2068243009.py:143: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sorted_df = grouped.apply(lambda x: x.sort_values(by='DATA_MOV', ascending=True)).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing 'AGENTE' after filling: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_27408\\2068243009.py:141: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['DATA_MOV'] = pd.to_datetime(filtered_df['DATA_MOV'], format='%d/%m/%Y', errors='coerce')\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_27408\\2068243009.py:143: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sorted_df = grouped.apply(lambda x: x.sort_values(by='DATA_MOV', ascending=True)).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing 'AGENTE' after filling: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_27408\\2068243009.py:141: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['DATA_MOV'] = pd.to_datetime(filtered_df['DATA_MOV'], format='%d/%m/%Y', errors='coerce')\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_27408\\2068243009.py:143: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sorted_df = grouped.apply(lambda x: x.sort_values(by='DATA_MOV', ascending=True)).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing 'AGENTE' after filling: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_27408\\2068243009.py:141: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['DATA_MOV'] = pd.to_datetime(filtered_df['DATA_MOV'], format='%d/%m/%Y', errors='coerce')\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_27408\\2068243009.py:143: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sorted_df = grouped.apply(lambda x: x.sort_values(by='DATA_MOV', ascending=True)).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning process completed and saved to Z:/My Drive/rcs/business analyst/Year2024-StatsSoFar-cleaned.xlsx\n",
      "Dropped data saved to Z:/My Drive/rcs/business analyst/Year2024-StatsSoFar-dropped.csv\n",
      "JSON file saved to Z:/My Drive/rcs/business analyst/Year2024-StatsSoFar-cleaned.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Define the path pattern for the monthly CSV files\n",
    "file_pattern = 'Z:/My Drive/rcs/business analyst/Statistica_del_Venduto_e_Acquistato_2024_*.csv'\n",
    "file_paths = glob.glob(file_pattern)\n",
    "\n",
    "# Initialize lists to store dataframes\n",
    "df_list = []\n",
    "dropped_df_list = []\n",
    "\n",
    "# Define the foreign \"RAGIONE SOCIALE\" list\n",
    "foreign_ragione_sociale = [\n",
    "    \"KOSSER JSC\", \"PRESTOLITE ELECTIRC LIMITED\", \"SHF TRADE TD LTD\", \"NEAT AUTOS LTD\", \"SUPERIORCARUSO SERVICE\",\n",
    "    \"MOTOR & TRANSMISSIONSTEKNIK\", \"RAIN GROUP TRADING FZE\", \"LUCAS DIESEL Inyeccion Diesel\", \n",
    "    \"SURIPE_x001a_S - IMPORTA_x001a_O E\", \"AOI SOLUTIONS GMBH\", \"MARIO SPITERI PARTS & SERVICE\", \"HOLSTEIN-HANDEL\",\n",
    "    \"RAY CINI TRANS\", \"GAMMA TECHNICAL CORPORATION\", \"GOBITRADE LTD.\", \"CARPARTS IMPORT EXPORT KFT\", \n",
    "    \"REMANTE GROUP S.R.O.\", \"PURIC D.O.O.\", \"FERDINAND BILSTEIN UK LTD\", \"ATLANTIC SPARE PARTS\", \"GEZAIRI CO. Attn: Mr. Halo\",\n",
    "    \"RECANPRI, S.L.\", \"SPH - SPARE PARTS HOUSE LDA\", \"DELPHI DIESEL AFTERMARKET\", \"PRESTOLITE ELECTRIC LIMITED\", \n",
    "    \"AVESA S.L.\", \"TECNODIESEL MURCIA S.L.\", \"MERLIN DIESEL SYSTEMS LTD\", \"MERLIN DIESEL SYSTEM LTD\", \"AMAZON EU S.A.R.L.\",\n",
    "    \"JOHN'S DIESEL SERVICE'S SQAQ\", \"S.C DIESEL SOF S.R.L. STR\", \"ARROWTRONIC LTD-EMS\", \"HORIZON PROCUREMENTS\",\n",
    "    \"SARL PROIETTI FARNESE\", \"TYRONE DIESEL SYSTEMS\", \"DIESELWELT BRENNER GMBH\", \"TURBOTECNIC AUTOMOCIO S.L.\",\n",
    "    \"KUNSHAN POWER ASIA IMPORT &\", \"ORION PART OTOM.IC VE DIS\", \"GUANGZHOU BAISITE AUTO PARTS\", \"IPT OTOMOTIV DIS TICARET\",\n",
    "    \"KONING TECHNISCH BEDRIJF B.V.\", \"PORTLAOISE DIESEL INJECTION\", \"CORCORANS ENGINEERING\", \n",
    "    \"ZHENGZHOU LISERON OIL PUMP &\", \"LKV TEKNIK OTOMOTIV SANAYI IC\", \"GUANGDONG TOPWAY TECH CO LTD\", \"UNITED FUEL INJECTION\",\n",
    "    \"LA CASA DEL DIESEL S.L.\", \"OTT-PUTIAN DIESEL SPARE PARTS\", \"AUTORAK SP. Z.O.O.\", \"IB TRADE\", \"FLOTA SUARDIAZ S.L.U.\",\n",
    "    \"2T + M KFT\", \"KENDIESEL\", \"CASTY-ROMER S.L.\", \"DIESELSERVICE STOKKING B.V.\", \"SUPER ENGINEERING WORKS\",\n",
    "    \"DIJITAL PARCA PLATFORMU SATIS\", \"GOODAPP\", \"INJECCIO DIESEL JORDA, S.L\", \"GUNEYBAGLILAR OTO YEDEK PAR.\",\n",
    "    \"D. HAUPT GMBH\", \"ASIST OTO YEDEK PARCA SANAYI\", \"UHURU MAX LIMITED\", \"SYDNEY DIESEL CENTRE\", \"TALLERES RUFRE S.L.\",\n",
    "    \"AB IMPORTS\", \"FACEWORKS LIMITED\", \"William Psaila\", \"EURO PARTS TRUCKS & BUSES\", \"PORTLAOISE DIESEL LTD\",\n",
    "    \"SUARDIAZ MANAGEMENT SERVICES\", \"EAST HOI TRADING COMPANY FLAT\", \"Karadaglar group otomotiv\", \"KARCOM LTD\", \"ERB ITALY SRL\",\n",
    "    \"A.F. BAEDER GES.M.B.H.\", \"DIESEL INJECTOR REPAIR Ltd\", \"FAHRENHEIT LOGISTICS LIMITED\", \"S T EARTHMOVING SERVICES\",\n",
    "    \"GOLDFARB and ASSOCIATES\", \"PANDIESEL AEBE\", \"ADF DIESEL\", \"AutoDieselPart LLC\", \"Goldfarb & Associates, Inc\"\n",
    "]\n",
    "\n",
    "# Loop through each file and process it\n",
    "for file_path in file_paths:\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "    \n",
    "    # If you're working with the 'LISTA' column before renaming\n",
    "    df['LISTA'] = df['LISTA'].apply(lambda x: int(x.split('/')[1]) if '/' in x else int(x))\n",
    "        \n",
    "    # Fix warranty, trasporto, imballo, and eco friendly issues\n",
    "    df['CODICE_ART'] = df['CODICE_ART'].str.upper()\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('warranty', case=False, na=False) & \n",
    "           df['CODICE_ART'].isin(['', '.', 'NC', 'nc']), 'CODICE_ART'] = 'WARR01'\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('trasporto', case=False, na=False) & \n",
    "           df['CODICE_ART'].isin(['', '.', 'NC', 'nc', 'SPESE']), 'CODICE_ART'] = 'EXPSHIPREGULAR'\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('imballo', case=False, na=False) & \n",
    "           df['CODICE_ART'].isin(['', '.', 'NC', 'nc']), 'CODICE_ART'] = 'EXPPACK01'\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('ECO FRIENDLY|ECO-FRIENDLY|INIZIATIVA ECO-FRIENDLY', case=False, na=False), \n",
    "           ['CODICE_ART', 'MARCA_ART', 'DESCRIZIONE_ART']] = ['EXPECO01', 'Aziendale', 'ECO FRIENDLY']\n",
    "\n",
    "    # Correct the IMBALLO logic\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('IMBALLO', case=False, na=False), 'MARCA_ART'] = 'Aziendale'\n",
    "    \n",
    "    # Fix specific values in DESCRIZIONE_ART\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('PREMIO FATTURATO', case=False, na=False), \n",
    "           ['CODICE_ART', 'MARCA_ART']] = ['ACCREWCONTRACT01', 'Aziendale']\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('INSOLUTO|INSOLUTI', case=False, na=False), \n",
    "           ['CODICE_ART', 'MARCA_ART']] = ['INSOLUTO', 'Aziendale']\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('spese', case=False, na=False), \n",
    "           ['CODICE_ART', 'DESCRIZIONE_ART']] = ['SPESE', 'IMBALLO']\n",
    "    df.loc[df['DESCRIZIONE_ART'] == 'accredito per err.applic.penale', 'CODICE_ART'] = 'ACCERR20240417'\n",
    "    df.loc[df['DESCRIZIONE_ART'] == 'errato addeb. spese di incasso', 'CODICE_ART'] = 'EXPSHIPERR20240614'\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('MANODOPERA STACCO/RIATT CON PRESSA', case=False, na=False), \n",
    "           ['CODICE_ART', 'MARCA_ART']] = ['ACCMANODOP01', 'RCS']\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('SBLOCCANTE WD40|WD-40 SBLOCCANTE|WD-40 SBLOCCANTE ', case=False, na=False), \n",
    "           ['CODICE_ART', 'MARCA_ART']] = ['490040', 'WD-40']\n",
    "    \n",
    "    # Additional condition for SPESE URGENZA\n",
    "    df.loc[df['CODICE_ART'] == 'SPESE URGENZA', ['CODICE_ART', 'DESCRIZIONE_ART']] = ['EXPSHIPURGENT', 'TRASPORTO URGENTE']\n",
    "\n",
    "    # Fix specific values in CODICE_ART\n",
    "    df.loc[df['CODICE_ART'].isin(['TBF1214035', 'TBF1414035']) & df['MARCA_ART'].isin(['NC', '.', 'nc']), 'MARCA_ART'] = 'RCS'\n",
    "    df.loc[(df['CODICE_ART'] == '26870') & df['MARCA_ART'].isin(['NC', '.', 'nc']), 'MARCA_ART'] = 'FEBI'\n",
    "    df.loc[(df['CODICE_ART'] == '3182654192') & df['MARCA_ART'].isin(['NC', '.', 'nc']), 'MARCA_ART'] = 'SACHS'\n",
    "    \n",
    "    # Fix IMPOSTA OLIO in DESCRIZIONE_ART\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('IMPOSTA OLIO', case=False, na=False), ['CODICE_ART', 'MARCA_ART']] = ['OILVAT', 'VAT']\n",
    "    \n",
    "    # Additional conditions\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('BONUS15|BONUS_15|BONUS_25|BONUS-15', case=False, na=False), \n",
    "           ['CODICE_ART', 'MARCA_ART']] = ['EXTRA', 'Aziendale']\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('ASSICURAZIONE 6% CASCO|ASSICURAZIONE 4033195 6%|ASSICURAZIONE', case=False, na=False), \n",
    "           ['CODICE_ART', 'MARCA_ART']] = ['ASSICURAZIONE', 'Aziendale']\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('periodo q4/2023|PERIODO Q3/2023', case=False, na=False), \n",
    "           ['CODICE_ART', 'MARCA_ART']] = ['BOSCHCORP', 'Aziendale']\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('Kit tubaz/flessib Mercedes', case=False, na=False), \n",
    "           ['CODICE_ART', 'MARCA_ART']] = ['Unknown', 'MERCEDES']\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('PACKAGING', case=False, na=False), \n",
    "           ['CODICE_ART', 'DESCRIZIONE_ART', 'MARCA_ART']] = ['EXPPACK01', 'IMBALLO', 'Aziendale']\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('Urgenza Bosch', case=False, na=False), \n",
    "           ['CODICE_ART', 'DESCRIZIONE_ART', 'MARCA_ART']] = ['EXPPACK01', 'IMBALLO', 'Aziendale']\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('PROVA AL BANCO INIETT DENSO C.R.|PROVA AL BANCO INIETT DENSO', case=False, na=False), \n",
    "           ['CODICE_ART', 'MARCA_ART']] = ['EXTRA', 'Aziendale']\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('INIETT 0445110351', case=False, na=False), \n",
    "           ['CODICE_ART', 'MARCA_ART']] = ['0445110351', 'BOSCH']\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('INIETT 0445110351 REVIS.', case=False, na=False), \n",
    "           ['CODICE_ART', 'MARCA_ART']] = ['0986435204', 'BOSCH']\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('premio', case=False, na=False), \n",
    "           ['CODICE_ART', 'MARCA_ART']] = ['ACCREWCONTRACT01', 'Aziendale']\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('EUROTECH 95006227', case=False, na=False), \n",
    "           ['CODICE_ART', 'MARCA_ART']] = ['95006227', 'EUROTEC']\n",
    "\n",
    "    # Fix specific values in DESCRIZIONE_ART for unique conditions\n",
    "    df.loc[df['DESCRIZIONE_ART'] == 'ASSEGNO IMPAGATO N 0214310167', 'CODICE_ART'] = 'ACC0214310167'\n",
    "    df.loc[df['DESCRIZIONE_ART'] == 'ASSEGNO IMPAG N 100593023 DEL 07/06', 'CODICE_ART'] = 'ACC100593023'\n",
    "    df.loc[df['DESCRIZIONE_ART'] == 'accredito per err.applic.penale', 'CODICE_ART'] = 'ACCERR20240417'\n",
    "    df.loc[df['DESCRIZIONE_ART'] == 'INCASSO, VARIE', 'CODICE_ART'] = 'ACC20240318'\n",
    "    df.loc[df['DESCRIZIONE_ART'] == 'INDICARE MOTIVAZIONE', 'CODICE_ART'] = 'ACCERRU'\n",
    "    \n",
    "    # Set ULTIMO_ACQ to 0 for specific DESCRIZIONE_ART values\n",
    "    df.loc[df['DESCRIZIONE_ART'].str.contains('WD40|WD-40|WARRANTY|IMPOSTA OLIO|IMBALLO|TRASPORTO|PACKAGING|Urgenza Bosch', case=False, na=False), 'ULTIMO_ACQ'] = 0\n",
    "    \n",
    "    # Assign AGENTE value of 60 for 'CLIENTE DA WEB'\n",
    "    df.loc[df['RAGIONE_SOCIALE'] == 'CLIENTE DA WEB', 'AGENTE'] = 60\n",
    "    \n",
    "    # Assign placeholder Codice Agente for any remaining missing or zero values\n",
    "    df['AGENTE'] = df['AGENTE'].apply(lambda x: '100' if pd.isna(x) or x == 0 else x)\n",
    "    \n",
    "    # Assign AG 50 for foreign \"RAGIONE SOCIALE\"\n",
    "    df['AGENTE'] = df.apply(lambda row: '50' if row['RAGIONE_SOCIALE'] in foreign_ragione_sociale else row['AGENTE'], axis=1)\n",
    "\n",
    "    # Verify that no 'AGENTE' values are missing\n",
    "    missing_agente_after = df['AGENTE'].isna().sum()\n",
    "    print(f\"Missing 'AGENTE' after filling: {missing_agente_after}\")\n",
    "\n",
    "    # Assign 'MARCA_ART' to 'Aziendale' for specific 'CODICE_ART' values\n",
    "    df.loc[df['CODICE_ART'].str.contains('EXTRA|ASSICURAZIONE|PENALE|ABBONAM-TRASP-0001|OMAGGIO|SPESE|TRASPORTO|WARRANTY', case=False, na=False), 'MARCA_ART'] = 'Aziendale'\n",
    "    \n",
    "    # Separate the rows to be dropped\n",
    "    dropped_df = df[df['MARCA_ART'].isin(['NC', '.', '', 'nc'])]\n",
    "    dropped_df_list.append(dropped_df)\n",
    "    \n",
    "    # Drop entries where 'MARCA_ART' is 'NC' or '.' or empty\n",
    "    filtered_df = df[~df['MARCA_ART'].isin(['NC', '.', '', 'nc'])]\n",
    "    \n",
    "    # Group by 'Ragione Sociale Cliente' and sort each group by 'Data Documento Precedente'\n",
    "    filtered_df['DATA_MOV'] = pd.to_datetime(filtered_df['DATA_MOV'], format='%d/%m/%Y', errors='coerce')\n",
    "    grouped = filtered_df.groupby(['RAGIONE_SOCIALE'])\n",
    "    sorted_df = grouped.apply(lambda x: x.sort_values(by='DATA_MOV', ascending=True)).reset_index(drop=True)\n",
    "    \n",
    "    # Keep specific columns and rename them\n",
    "    columns_to_keep = {\n",
    "        'DATA_MOV': 'Data Documento Precedente', \n",
    "        'LISTA': 'Numero Lista', \n",
    "        'MESE': 'Mese', \n",
    "        'ANNO': 'Anno', \n",
    "        'RAGIONE_SOCIALE': 'Ragione Sociale Cliente', \n",
    "        'CLIENTE': 'Codice Cliente', \n",
    "        'AGENTE': 'Codice Agente', \n",
    "        'CODICE_ART': 'Codice Articolo', \n",
    "        'MARCA_ART': 'Marca Articolo', \n",
    "        'DESCRIZIONE_ART': 'Descrizione Articolo', \n",
    "        'QUANTITA': 'Quantita', \n",
    "        'VAL_MERCE': 'Valore', \n",
    "        'ULTIMO_ACQ': 'Costo',\n",
    "        'PREZZO': 'Prezzo Articolo',\n",
    "    }\n",
    "    cleaned_df = filtered_df[columns_to_keep.keys()].rename(columns=columns_to_keep)\n",
    "    \n",
    "    # Update 'Costo' based on 'Prezzo Articolo'\n",
    "    cleaned_df['Costo'] = cleaned_df.apply(\n",
    "        lambda row: row['Prezzo Articolo'] * 0.9 if pd.isna(row['Costo']) or row['Costo'] == 0 and row['Prezzo Articolo'] > 0 \n",
    "        else 0 if pd.isna(row['Costo']) or row['Costo'] == 0 and row['Prezzo Articolo'] == 0 \n",
    "        else abs(row['Prezzo Articolo']) if pd.isna(row['Costo']) or row['Costo'] == 0 and row['Prezzo Articolo'] < 0 \n",
    "        else row['Costo'],\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "\n",
    "    \n",
    "    # Append the cleaned dataframe to the list\n",
    "    df_list.append(cleaned_df)\n",
    "\n",
    "# Concatenate all the dataframes\n",
    "final_df = pd.concat(df_list, ignore_index=True)\n",
    "dropped_final_df = pd.concat(dropped_df_list, ignore_index=True)\n",
    "\n",
    "# Save the cleaned dataset to a new Excel file\n",
    "cleaned_file_path = 'Z:/My Drive/rcs/business analyst/Year2024-StatsSoFar-cleaned.xlsx'\n",
    "final_df.to_excel(cleaned_file_path, index=False)\n",
    "\n",
    "# Save the dropped dataset to a new CSV file\n",
    "dropped_file_path = 'Z:/My Drive/rcs/business analyst/Year2024-StatsSoFar-dropped.csv'\n",
    "dropped_final_df.to_csv(dropped_file_path, index=False)\n",
    "\n",
    "# Convert to JSON format\n",
    "json_file_path = 'Z:/My Drive/rcs/business analyst/Year2024-StatsSoFar-cleaned.json'\n",
    "final_df.to_json(json_file_path, orient='records', date_format='iso')\n",
    "\n",
    "print(\"Cleaning process completed and saved to\", cleaned_file_path)\n",
    "print(\"Dropped data saved to\", dropped_file_path)\n",
    "print(\"JSON file saved to\", json_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before applying 'imballo' condition:\n",
      "                   DESCRIZIONE_ART   CODICE_ART  MARCA_ART\n",
      "2444                       IMBALLO        EXTRA          .\n",
      "2648  spese di trasporto e imballo  SHIPREGULAR  Aziendale\n",
      "2650     spese trasporto e imballo  SHIPREGULAR  Aziendale\n",
      "2948                       IMBALLO      IMBALLO          .\n",
      "4690                       IMBALLO        EXTRA          .\n",
      "4729                       IMBALLO      IMBALLO          .\n",
      "6354                       IMBALLO      IMBALLO          .\n",
      "8332                       IMBALLO      IMBALLO          .\n",
      "After applying 'imballo' condition:\n",
      "                   DESCRIZIONE_ART   CODICE_ART  MARCA_ART\n",
      "2444                       IMBALLO    PACKING01  Aziendale\n",
      "2648  spese di trasporto e imballo  SHIPREGULAR  Aziendale\n",
      "2650     spese trasporto e imballo  SHIPREGULAR  Aziendale\n",
      "2948                       IMBALLO    PACKING01  Aziendale\n",
      "4690                       IMBALLO    PACKING01  Aziendale\n",
      "4729                       IMBALLO    PACKING01  Aziendale\n",
      "6354                       IMBALLO    PACKING01  Aziendale\n",
      "8332                       IMBALLO    PACKING01  Aziendale\n",
      "Missing 'AGENTE' after filling: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_27408\\225378347.py:591: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df[\"DATA_MOV\"] = pd.to_datetime(\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_27408\\225378347.py:595: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sorted_df = grouped.apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before applying 'imballo' condition:\n",
      "      DESCRIZIONE_ART CODICE_ART MARCA_ART\n",
      "43            Imballo         NC        NC\n",
      "527           IMBALLO         NC        NC\n",
      "534           Imballo         NC        NC\n",
      "539           Imballo         NC        NC\n",
      "543           Imballo         NC        NC\n",
      "552           Imballo         NC        NC\n",
      "556           Imballo         NC        NC\n",
      "561           Imballo         NC        NC\n",
      "563           IMBALLO         NC        NC\n",
      "655           Imballo         NC        NC\n",
      "1040          Imballo         NC        NC\n",
      "1042          Imballo         NC        NC\n",
      "1045          Imballo         NC        NC\n",
      "1050          Imballo         NC        NC\n",
      "1066          Imballo         NC        NC\n",
      "1075          Imballo         NC        NC\n",
      "1080          Imballo         NC        NC\n",
      "1874          IMBALLO         NC        NC\n",
      "2444          Imballo         NC        NC\n",
      "2449          Imballo         NC        NC\n",
      "2455          Imballo         NC        NC\n",
      "2463          Imballo         NC        NC\n",
      "2469          Imballo         NC        NC\n",
      "2489          Imballo         NC        NC\n",
      "2780          Imballo         NC        NC\n",
      "2791          Imballo         NC        NC\n",
      "3634          Imballo         NC        NC\n",
      "4290          Imballo         NC        NC\n",
      "4428          Imballo         NC        NC\n",
      "4678          Imballo         NC        NC\n",
      "4908          Imballo         NC        NC\n",
      "5040          Imballo         NC        NC\n",
      "5076          Imballo         NC        NC\n",
      "6092          Imballo         NC        NC\n",
      "6415          Imballo         NC        NC\n",
      "7092          Imballo         NC        NC\n",
      "7263          Imballo         NC        NC\n",
      "7404          Imballo         NC        NC\n",
      "7426          Imballo         NC        NC\n",
      "12735         IMBALLO         NC        NC\n",
      "After applying 'imballo' condition:\n",
      "      DESCRIZIONE_ART CODICE_ART  MARCA_ART\n",
      "43            Imballo  PACKING01  Aziendale\n",
      "527           IMBALLO  PACKING01  Aziendale\n",
      "534           Imballo  PACKING01  Aziendale\n",
      "539           Imballo  PACKING01  Aziendale\n",
      "543           Imballo  PACKING01  Aziendale\n",
      "552           Imballo  PACKING01  Aziendale\n",
      "556           Imballo  PACKING01  Aziendale\n",
      "561           Imballo  PACKING01  Aziendale\n",
      "563           IMBALLO  PACKING01  Aziendale\n",
      "655           Imballo  PACKING01  Aziendale\n",
      "1040          Imballo  PACKING01  Aziendale\n",
      "1042          Imballo  PACKING01  Aziendale\n",
      "1045          Imballo  PACKING01  Aziendale\n",
      "1050          Imballo  PACKING01  Aziendale\n",
      "1066          Imballo  PACKING01  Aziendale\n",
      "1075          Imballo  PACKING01  Aziendale\n",
      "1080          Imballo  PACKING01  Aziendale\n",
      "1874          IMBALLO  PACKING01  Aziendale\n",
      "2444          Imballo  PACKING01  Aziendale\n",
      "2449          Imballo  PACKING01  Aziendale\n",
      "2455          Imballo  PACKING01  Aziendale\n",
      "2463          Imballo  PACKING01  Aziendale\n",
      "2469          Imballo  PACKING01  Aziendale\n",
      "2489          Imballo  PACKING01  Aziendale\n",
      "2780          Imballo  PACKING01  Aziendale\n",
      "2791          Imballo  PACKING01  Aziendale\n",
      "3634          Imballo  PACKING01  Aziendale\n",
      "4290          Imballo  PACKING01  Aziendale\n",
      "4428          Imballo  PACKING01  Aziendale\n",
      "4678          Imballo  PACKING01  Aziendale\n",
      "4908          Imballo  PACKING01  Aziendale\n",
      "5040          Imballo  PACKING01  Aziendale\n",
      "5076          Imballo  PACKING01  Aziendale\n",
      "6092          Imballo  PACKING01  Aziendale\n",
      "6415          Imballo  PACKING01  Aziendale\n",
      "7092          Imballo  PACKING01  Aziendale\n",
      "7263          Imballo  PACKING01  Aziendale\n",
      "7404          Imballo  PACKING01  Aziendale\n",
      "7426          Imballo  PACKING01  Aziendale\n",
      "12735         IMBALLO  PACKING01  Aziendale\n",
      "Missing 'AGENTE' after filling: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_27408\\225378347.py:591: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df[\"DATA_MOV\"] = pd.to_datetime(\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_27408\\225378347.py:595: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sorted_df = grouped.apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before applying 'imballo' condition:\n",
      "           DESCRIZIONE_ART CODICE_ART MARCA_ART\n",
      "1084  STORNO SPESE IMBALLO         NC        NC\n",
      "1555               IMBALLO         NC        NC\n",
      "1954               IMBALLO         NC        NC\n",
      "3233               IMBALLO         NC        NC\n",
      "3241              IMBALLO          NC        NC\n",
      "4249               IMBALLO         NC        NC\n",
      "4316               IMBALLO         NC        NC\n",
      "4778               IMBALLO         NC        NC\n",
      "5525               IMBALLO         NC        NC\n",
      "6980               IMBALLO         NC        NC\n",
      "9066              IMBALLO          NC        NC\n",
      "After applying 'imballo' condition:\n",
      "           DESCRIZIONE_ART CODICE_ART  MARCA_ART\n",
      "1084  STORNO SPESE IMBALLO  PACKING01  Aziendale\n",
      "1555               IMBALLO  PACKING01  Aziendale\n",
      "1954               IMBALLO  PACKING01  Aziendale\n",
      "3233               IMBALLO  PACKING01  Aziendale\n",
      "3241              IMBALLO   PACKING01  Aziendale\n",
      "4249               IMBALLO  PACKING01  Aziendale\n",
      "4316               IMBALLO  PACKING01  Aziendale\n",
      "4778               IMBALLO  PACKING01  Aziendale\n",
      "5525               IMBALLO  PACKING01  Aziendale\n",
      "6980               IMBALLO  PACKING01  Aziendale\n",
      "9066              IMBALLO   PACKING01  Aziendale\n",
      "Missing 'AGENTE' after filling: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_27408\\225378347.py:591: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df[\"DATA_MOV\"] = pd.to_datetime(\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_27408\\225378347.py:595: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sorted_df = grouped.apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before applying 'imballo' condition:\n",
      "     DESCRIZIONE_ART CODICE_ART MARCA_ART\n",
      "470          IMBALLO          .         .\n",
      "501          IMBALLO          .         .\n",
      "1039         IMBALLO          .         .\n",
      "3360         IMBALLO          .         .\n",
      "6027        IMBALLO           .         .\n",
      "8826         IMBALLO      SPESE         .\n",
      "8872         IMBALLO      SPESE         .\n",
      "After applying 'imballo' condition:\n",
      "     DESCRIZIONE_ART CODICE_ART  MARCA_ART\n",
      "470          IMBALLO  PACKING01  Aziendale\n",
      "501          IMBALLO  PACKING01  Aziendale\n",
      "1039         IMBALLO  PACKING01  Aziendale\n",
      "3360         IMBALLO  PACKING01  Aziendale\n",
      "6027        IMBALLO   PACKING01  Aziendale\n",
      "8826         IMBALLO      SPESE          .\n",
      "8872         IMBALLO      SPESE          .\n",
      "Missing 'AGENTE' after filling: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_27408\\225378347.py:591: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df[\"DATA_MOV\"] = pd.to_datetime(\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_27408\\225378347.py:595: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sorted_df = grouped.apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before applying 'imballo' condition:\n",
      "     DESCRIZIONE_ART CODICE_ART MARCA_ART\n",
      "440          IMBALLO          .         .\n",
      "5338         IMBALLO      SPESE         .\n",
      "5352         IMBALLO      SPESE         .\n",
      "5855         IMBALLO         NC        NC\n",
      "5881         IMBALLO         NC        NC\n",
      "6388         IMBALLO          .         .\n",
      "9238         IMBALLO    IMBALLO         .\n",
      "After applying 'imballo' condition:\n",
      "     DESCRIZIONE_ART CODICE_ART  MARCA_ART\n",
      "440          IMBALLO  PACKING01  Aziendale\n",
      "5338         IMBALLO      SPESE          .\n",
      "5352         IMBALLO      SPESE          .\n",
      "5855         IMBALLO  PACKING01  Aziendale\n",
      "5881         IMBALLO  PACKING01  Aziendale\n",
      "6388         IMBALLO  PACKING01  Aziendale\n",
      "9238         IMBALLO  PACKING01  Aziendale\n",
      "Missing 'AGENTE' after filling: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_27408\\225378347.py:591: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df[\"DATA_MOV\"] = pd.to_datetime(\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_27408\\225378347.py:595: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sorted_df = grouped.apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before applying 'imballo' condition:\n",
      "      DESCRIZIONE_ART CODICE_ART MARCA_ART\n",
      "3892          IMBALLO    IMBALLO         .\n",
      "3898          IMBALLO    IMBALLO         .\n",
      "4984          IMBALLO    IMBALLO         .\n",
      "5956          IMBALLO    IMBALLO         .\n",
      "6927          IMBALLO    IMBALLO         .\n",
      "7088          IMBALLO    IMBALLO         .\n",
      "7464          IMBALLO    IMBALLO         .\n",
      "9881          IMBALLO          .         .\n",
      "10379         IMBALLO    IMBALLO         .\n",
      "10842         IMBALLO    IMBALLO         .\n",
      "After applying 'imballo' condition:\n",
      "      DESCRIZIONE_ART CODICE_ART  MARCA_ART\n",
      "3892          IMBALLO  PACKING01  Aziendale\n",
      "3898          IMBALLO  PACKING01  Aziendale\n",
      "4984          IMBALLO  PACKING01  Aziendale\n",
      "5956          IMBALLO  PACKING01  Aziendale\n",
      "6927          IMBALLO  PACKING01  Aziendale\n",
      "7088          IMBALLO  PACKING01  Aziendale\n",
      "7464          IMBALLO  PACKING01  Aziendale\n",
      "9881          IMBALLO  PACKING01  Aziendale\n",
      "10379         IMBALLO  PACKING01  Aziendale\n",
      "10842         IMBALLO  PACKING01  Aziendale\n",
      "Missing 'AGENTE' after filling: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_27408\\225378347.py:591: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df[\"DATA_MOV\"] = pd.to_datetime(\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_27408\\225378347.py:595: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sorted_df = grouped.apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before applying 'imballo' condition:\n",
      "   DESCRIZIONE_ART CODICE_ART MARCA_ART\n",
      "23         IMBALLO    IMBALLO         .\n",
      "After applying 'imballo' condition:\n",
      "   DESCRIZIONE_ART CODICE_ART  MARCA_ART\n",
      "23         IMBALLO  PACKING01  Aziendale\n",
      "Missing 'AGENTE' after filling: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_27408\\225378347.py:591: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df[\"DATA_MOV\"] = pd.to_datetime(\n",
      "C:\\Users\\artor\\AppData\\Local\\Temp\\ipykernel_27408\\225378347.py:595: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sorted_df = grouped.apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning process completed and saved to Z:/My Drive/rcs/business analyst/Year2024-StatsSoFar-cleaned.xlsx\n",
      "Dropped data saved to Z:/My Drive/rcs/business analyst/Year2024-StatsSoFar-dropped.csv\n",
      "JSON file saved to Z:/My Drive/rcs/business analyst/Year2024-StatsSoFar-cleaned.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Define the path pattern for the monthly CSV files\n",
    "file_pattern = (\n",
    "    \"Z:/My Drive/rcs/business analyst/Statistica_del_Venduto_e_Acquistato_2024_*.csv\"\n",
    ")\n",
    "file_paths = glob.glob(file_pattern)\n",
    "\n",
    "# Initialize lists to store dataframes\n",
    "df_list = []\n",
    "dropped_df_list = []\n",
    "\n",
    "# Define the foreign \"RAGIONE SOCIALE\" list\n",
    "foreign_ragione_sociale = [\n",
    "    \"KOSSER JSC\",\n",
    "    \"PRESTOLITE ELECTIRC LIMITED\",\n",
    "    \"SHF TRADE TD LTD\",\n",
    "    \"NEAT AUTOS LTD\",\n",
    "    \"SUPERIORCARUSO SERVICE\",\n",
    "    \"MOTOR & TRANSMISSIONSTEKNIK\",\n",
    "    \"RAIN GROUP TRADING FZE\",\n",
    "    \"LUCAS DIESEL Inyeccion Diesel\",\n",
    "    \"SURIPE_x001a_S - IMPORTA_x001a_O E\",\n",
    "    \"AOI SOLUTIONS GMBH\",\n",
    "    \"MARIO SPITERI PARTS & SERVICE\",\n",
    "    \"HOLSTEIN-HANDEL\",\n",
    "    \"RAY CINI TRANS\",\n",
    "    \"GAMMA TECHNICAL CORPORATION\",\n",
    "    \"GOBITRADE LTD.\",\n",
    "    \"CARPARTS IMPORT EXPORT KFT\",\n",
    "    \"REMANTE GROUP S.R.O.\",\n",
    "    \"PURIC D.O.O.\",\n",
    "    \"FERDINAND BILSTEIN UK LTD\",\n",
    "    \"ATLANTIC SPARE PARTS\",\n",
    "    \"GEZAIRI CO. Attn: Mr. Halo\",\n",
    "    \"RECANPRI, S.L.\",\n",
    "    \"SPH - SPARE PARTS HOUSE LDA\",\n",
    "    \"DELPHI DIESEL AFTERMARKET\",\n",
    "    \"PRESTOLITE ELECTRIC LIMITED\",\n",
    "    \"AVESA S.L.\",\n",
    "    \"TECNODIESEL MURCIA S.L.\",\n",
    "    \"MERLIN DIESEL SYSTEMS LTD\",\n",
    "    \"MERLIN DIESEL SYSTEM LTD\",\n",
    "    \"AMAZON EU S.A.R.L.\",\n",
    "    \"JOHN'S DIESEL SERVICE'S SQAQ\",\n",
    "    \"S.C DIESEL SOF S.R.L. STR\",\n",
    "    \"ARROWTRONIC LTD-EMS\",\n",
    "    \"HORIZON PROCUREMENTS\",\n",
    "    \"SARL PROIETTI FARNESE\",\n",
    "    \"TYRONE DIESEL SYSTEMS\",\n",
    "    \"DIESELWELT BRENNER GMBH\",\n",
    "    \"TURBOTECNIC AUTOMOCIO S.L.\",\n",
    "    \"KUNSHAN POWER ASIA IMPORT &\",\n",
    "    \"ORION PART OTOM.IC VE DIS\",\n",
    "    \"GUANGZHOU BAISITE AUTO PARTS\",\n",
    "    \"IPT OTOMOTIV DIS TICARET\",\n",
    "    \"KONING TECHNISCH BEDRIJF B.V.\",\n",
    "    \"PORTLAOISE DIESEL INJECTION\",\n",
    "    \"CORCORANS ENGINEERING\",\n",
    "    \"ZHENGZHOU LISERON OIL PUMP &\",\n",
    "    \"LKV TEKNIK OTOMOTIV SANAYI IC\",\n",
    "    \"GUANGDONG TOPWAY TECH CO LTD\",\n",
    "    \"UNITED FUEL INJECTION\",\n",
    "    \"LA CASA DEL DIESEL S.L.\",\n",
    "    \"OTT-PUTIAN DIESEL SPARE PARTS\",\n",
    "    \"AUTORAK SP. Z.O.O.\",\n",
    "    \"IB TRADE\",\n",
    "    \"FLOTA SUARDIAZ S.L.U.\",\n",
    "    \"2T + M KFT\",\n",
    "    \"KENDIESEL\",\n",
    "    \"CASTY-ROMER S.L.\",\n",
    "    \"DIESELSERVICE STOKKING B.V.\",\n",
    "    \"SUPER ENGINEERING WORKS\",\n",
    "    \"DIJITAL PARCA PLATFORMU SATIS\",\n",
    "    \"GOODAPP\",\n",
    "    \"INJECCIO DIESEL JORDA, S.L\",\n",
    "    \"GUNEYBAGLILAR OTO YEDEK PAR.\",\n",
    "    \"D. HAUPT GMBH\",\n",
    "    \"ASIST OTO YEDEK PARCA SANAYI\",\n",
    "    \"UHURU MAX LIMITED\",\n",
    "    \"SYDNEY DIESEL CENTRE\",\n",
    "    \"TALLERES RUFRE S.L.\",\n",
    "    \"AB IMPORTS\",\n",
    "    \"FACEWORKS LIMITED\",\n",
    "    \"William Psaila\",\n",
    "    \"EURO PARTS TRUCKS & BUSES\",\n",
    "    \"PORTLAOISE DIESEL LTD\",\n",
    "    \"SUARDIAZ MANAGEMENT SERVICES\",\n",
    "    \"EAST HOI TRADING COMPANY FLAT\",\n",
    "    \"Karadaglar group otomotiv\",\n",
    "    \"KARCOM LTD\",\n",
    "    \"ERB ITALY SRL\",\n",
    "    \"A.F. BAEDER GES.M.B.H.\",\n",
    "    \"DIESEL INJECTOR REPAIR Ltd\",\n",
    "    \"FAHRENHEIT LOGISTICS LIMITED\",\n",
    "    \"S T EARTHMOVING SERVICES\",\n",
    "    \"GOLDFARB and ASSOCIATES\",\n",
    "    \"PANDIESEL AEBE\",\n",
    "    \"ADF DIESEL\",\n",
    "    \"AutoDieselPart LLC\",\n",
    "    \"Goldfarb & Associates, Inc\",\n",
    "]\n",
    "\n",
    "# Loop through each file and process it\n",
    "for file_path in file_paths:\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(file_path, encoding=\"ISO-8859-1\")\n",
    "\n",
    "    # If you're working with the 'LISTA' column before renaming\n",
    "    df[\"LISTA\"] = df[\"LISTA\"].apply(\n",
    "        lambda x: int(x.split(\"/\")[1]) if \"/\" in x else int(x)\n",
    "    )\n",
    "\n",
    "    # Fix warranty, trasporto, imballo, and eco friendly issues\n",
    "    df[\"CODICE_ART\"] = df[\"CODICE_ART\"].str.upper()\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"].str.contains(\"warranty\", case=False, na=False)\n",
    "        & df[\"CODICE_ART\"].isin([\"\", \".\", \"NC\", \"nc\"]),\n",
    "        [\"CODICE_ART\", \"MARCA_ART\"],\n",
    "    ] = [\"WARRANTY\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"].str.contains(\"trasporto|TRASPORTO\", case=False, na=False)\n",
    "        & df[\"CODICE_ART\"].isin([\"\", \".\", \"NC\", \"nc\", \"SPESE\", \"EXTRA\", \"TRASPORTO\"]),\n",
    "        [\"CODICE_ART\", \"MARCA_ART\"],\n",
    "    ] = [\"SHIPREGULAR\", \"Aziendale\"]\n",
    "    # Print rows before applying condition\n",
    "    print(\"Before applying 'imballo' condition:\")\n",
    "    print(\n",
    "        df[\n",
    "            df[\"DESCRIZIONE_ART\"].str.contains(\n",
    "                r\"\\bimballo\\b|\\bIMBALLO\\b\", case=False, na=False\n",
    "            )\n",
    "        ][[\"DESCRIZIONE_ART\", \"CODICE_ART\", \"MARCA_ART\"]]\n",
    "    )\n",
    "\n",
    "    # Apply the 'imballo' condition\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"].str.contains(\n",
    "            r\"\\bimballo\\b|\\bIMBALLO\\b\", case=False, na=False\n",
    "        )\n",
    "        & df[\"CODICE_ART\"].isin([\"\", \".\", \"NC\", \"nc\", \"EXTRA\", \"IMBALLO\"]),\n",
    "        [\"CODICE_ART\", \"MARCA_ART\"],\n",
    "    ] = [\"PACKING01\", \"Aziendale\"]\n",
    "\n",
    "    # Print rows after applying condition\n",
    "    print(\"After applying 'imballo' condition:\")\n",
    "    print(\n",
    "        df[\n",
    "            df[\"DESCRIZIONE_ART\"].str.contains(\n",
    "                r\"\\bimballo\\b|\\bIMBALLO\\b\", case=False, na=False\n",
    "            )\n",
    "        ][[\"DESCRIZIONE_ART\", \"CODICE_ART\", \"MARCA_ART\"]]\n",
    "    )\n",
    "\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"].str.contains(\n",
    "            \"ECO FRIENDLY|ECO-FRIENDLY|INIZIATIVA ECO-FRIENDLY|PROGETTO ECOFRIENDLY - TRASPORTO GR\",\n",
    "            case=False,\n",
    "            na=False,\n",
    "        ),\n",
    "        [\"CODICE_ART\", \"MARCA_ART\", \"DESCRIZIONE_ART\"],\n",
    "    ] = [\"PROMOECO\", \"Aziendale\", \"ECO FRIENDLY\"]\n",
    "\n",
    "    # Fix specific values in DESCRIZIONE_ART\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"].str.contains(\n",
    "            \"PREMIO FATTURATO|premio\", case=False, na=False\n",
    "        ),\n",
    "        [\"CODICE_ART\", \"MARCA_ART\"],\n",
    "    ] = [\"PROMOCONTRACT01\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"].str.contains(\"INSOLUTO|INSOLUTI\", case=False, na=False),\n",
    "        [\"CODICE_ART\", \"MARCA_ART\"],\n",
    "    ] = [\"INSOLUTO\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"accredito per err.applic.penale\",\n",
    "        [\"CODICE_ART\", \"MARCA_ART\"],\n",
    "    ] = [\"ACCERR20240417\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"errato addeb. spese di incasso\",\n",
    "        [\"CODICE_ART\", \"MARCA_ART\"],\n",
    "    ] = [\"EXPSHIPERR20240614\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"MANODOPERA STACCO/RIATT CON PRESSA\",\n",
    "        [\"CODICE_ART\", \"MARCA_ART\"],\n",
    "    ] = [\"ACCMANODOP01\", \"RCS\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"].str.contains(\n",
    "            \"SBLOCCANTE WD40|WD-40 SBLOCCANTE|WD-40 SBLOCCANTE \", case=False, na=False\n",
    "        ),\n",
    "        [\"CODICE_ART\", \"MARCA_ART\"],\n",
    "    ] = [\"490040\", \"WD-40\"]\n",
    "\n",
    "    # Additional condition for SPESE URGENZA\n",
    "    df.loc[\n",
    "        df[\"CODICE_ART\"] == \"SPESE URGENZA\",\n",
    "        [\"CODICE_ART\", \"MARCA_ART\", \"DESCRIZIONE_ART\"],\n",
    "    ] = [\"SHIPURGENT\", \"Aziendale\", \"TRASPORTO URGENTE\"]\n",
    "\n",
    "    # Fix specific values in CODICE_ART\n",
    "    df.loc[\n",
    "        df[\"CODICE_ART\"].isin([\"TBF1214035\", \"TBF1414035\"])\n",
    "        & df[\"MARCA_ART\"].isin([\"NC\", \".\", \"nc\"]),\n",
    "        \"MARCA_ART\",\n",
    "    ] = \"RCS\"\n",
    "    df.loc[\n",
    "        (df[\"CODICE_ART\"] == \"26870\") & df[\"MARCA_ART\"].isin([\"NC\", \".\", \"nc\"]),\n",
    "        \"MARCA_ART\",\n",
    "    ] = \"FEBI\"\n",
    "    df.loc[\n",
    "        (df[\"CODICE_ART\"] == \"3182654192\") & df[\"MARCA_ART\"].isin([\"NC\", \".\", \"nc\"]),\n",
    "        \"MARCA_ART\",\n",
    "    ] = \"SACHS\"\n",
    "\n",
    "    # Fix IMPOSTA OLIO in DESCRIZIONE_ART\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"].str.contains(\"IMPOSTA OLIO\", case=False, na=False),\n",
    "        [\"CODICE_ART\", \"MARCA_ART\"],\n",
    "    ] = [\"OILVAT\", \"TAX\"]\n",
    "\n",
    "    # Additional conditions\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"].str.contains(\n",
    "            \"BONUS15|BONUS_15|BONUS_25|BONUS-15\", case=False, na=False\n",
    "        ),\n",
    "        [\"CODICE_ART\", \"MARCA_ART\"],\n",
    "    ] = [\"PROMOBONUS01\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"].str.contains(\n",
    "            \"ASSICURAZIONE 6% CASCO|ASSICURAZIONE 4033195 6%|ASSICURAZIONE\",\n",
    "            case=False,\n",
    "            na=False,\n",
    "        ),\n",
    "        [\"CODICE_ART\", \"MARCA_ART\"],\n",
    "    ] = [\"ASSICURAZIONE\", \"Aziendale\"]\n",
    "    df.loc[df[\"DESCRIZIONE_ART\"] == \"periodo q4/2023\", [\"CODICE_ART\", \"MARCA_ART\"]] = [\n",
    "        \"BOSCCORPQ42023\",\n",
    "        \"Aziendale\",\n",
    "    ]\n",
    "    df.loc[df[\"DESCRIZIONE_ART\"] == \"PERIODO Q3/2023\", [\"CODICE_ART\", \"MARCA_ART\"]] = [\n",
    "        \"BOSCCORPQ32023\",\n",
    "        \"Aziendale\",\n",
    "    ]\n",
    "    df.loc[df[\"DESCRIZIONE_ART\"] == \"PERIODO Q1/2024\", [\"CODICE_ART\", \"MARCA_ART\"]] = [\n",
    "        \"BOSCORPQ12024\",\n",
    "        \"Aziendale\",\n",
    "    ]\n",
    "\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"Kit tubaz/flessib Mercedes\",\n",
    "        [\"CODICE_ART\", \"MARCA_ART\"],\n",
    "    ] = [\"Unknown\", \"MERCEDES\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"PACKAGING\",\n",
    "        [\"CODICE_ART\", \"DESCRIZIONE_ART\", \"MARCA_ART\"],\n",
    "    ] = [\"PACKING01\", \"IMBALLO\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"Urgenza Bosch\",\n",
    "        [\"CODICE_ART\", \"DESCRIZIONE_ART\", \"MARCA_ART\"],\n",
    "    ] = [\"PACKING01\", \"IMBALLO\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"PROVA AL BANCO INIETT DENSO C.R.\",\n",
    "        [\"CODICE_ART\", \"MARCA_ART\"],\n",
    "    ] = [\"ORDMANO01\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"INIETT 0445110351\", [\"CODICE_ART\", \"MARCA_ART\"]\n",
    "    ] = [\"0445110351\", \"BOSCH\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"INIETT 0445110351 REVIS.\", [\"CODICE_ART\", \"MARCA_ART\"]\n",
    "    ] = [\"0986435204\", \"BOSCH\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"EUROTECH 95006227\", [\"CODICE_ART\", \"MARCA_ART\"]\n",
    "    ] = [\"95006227\", \"EUROTEC\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"SCONTO SU CONTEGGI\", [\"CODICE_ART\", \"MARCA_ART\"]\n",
    "    ] = [\"PROMOCOUNT\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"VENDITA BATTERIE ESAUSTE\", [\"CODICE_ART\", \"MARCA_ART\"]\n",
    "    ] = [\"ORDEXIDE20240510\", \"Aziendale\"]\n",
    "    df.loc[df[\"DESCRIZIONE_ART\"] == \"IMPORTO ADDEBITO\", [\"CODICE_ART\", \"MARCA_ART\"]] = [\n",
    "        \"ACCPEN01\",\n",
    "        \"Aziendale\",\n",
    "    ]\n",
    "\n",
    "    # Fix specific values in DESCRIZIONE_ART for unique conditions\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"ASSEGNO IMPAGATO N 0214310167\",\n",
    "        [\"CODICE_ART\", \"MARCA_ART\"],\n",
    "    ] = [\"ACC0214310167\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"ASSEGNO IMPAG N 100593023 DEL 07/06\",\n",
    "        [\"CODICE_ART\", \"MARCA_ART\"],\n",
    "    ] = [\"ACC100593023\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"accredito per err.applic.penale\",\n",
    "        [\"CODICE_ART\", \"MARCA_ART\"],\n",
    "    ] = [\"ACCERR20240417\", \"Aziendale\"]\n",
    "    df.loc[df[\"DESCRIZIONE_ART\"] == \"INCASSO, VARIE\", [\"CODICE_ART\", \"MARCA_ART\"]] = [\n",
    "        \"ACC20240318\",\n",
    "        \"Aziendale\",\n",
    "    ]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"INDICARE MOTIVAZIONE\", [\"CODICE_ART\", \"MARCA_ART\"]\n",
    "    ] = [\"ACCERRU\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"SCONTO NON INDICATO\", [\"CODICE_ART\", \"MARCA_ART\"]\n",
    "    ] = [\"ACCERR\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"SPEDIZIONI ERRATE\", [\"CODICE_ART\", \"MARCA_ART\"]\n",
    "    ] = [\"ACCERRSHIP\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"errato importo trasporto\", [\"CODICE_ART\", \"MARCA_ART\"]\n",
    "    ] = [\"ACCERRSHIP\", \"Aziendale\"]\n",
    "\n",
    "    # SHIP conditions\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"1/2TRASPORTO INTERCONTIN.\",\n",
    "        [\"CODICE_ART\", \"MARCA_ART\"],\n",
    "    ] = [\"SHIPINTERNATIONAL\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"TRASPORTO URGENTE ENGLAND\",\n",
    "        [\"CODICE_ART\", \"MARCA_ART\"],\n",
    "    ] = [\"SHIPINTERNATIONAL\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"TRASPORTO URGENTE estero\", [\"CODICE_ART\", \"MARCA_ART\"]\n",
    "    ] = [\"SHIPINTERNATIONAL\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"TRASPORTO URGENTE ESTERO\", [\"CODICE_ART\", \"MARCA_ART\"]\n",
    "    ] = [\"SHIPINTERNATIONAL\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"TRASPORTO DA ESTERO\", [\"CODICE_ART\", \"MARCA_ART\"]\n",
    "    ] = [\"SHIPINTERNATIONAL\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"TRASPORTO URGENTE GERMANY\",\n",
    "        [\"CODICE_ART\", \"MARCA_ART\"],\n",
    "    ] = [\"SHIPINTERNATIONAL\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"DA E PER FORNITORE\", [\"CODICE_ART\", \"MARCA_ART\"]\n",
    "    ] = [\"SHIPSUPPLIER\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"TRASPORTO DA FORNITORE\", [\"CODICE_ART\", \"MARCA_ART\"]\n",
    "    ] = [\"SHIPSUPPLIER\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"TRASPORTO da fornitore\", [\"CODICE_ART\", \"MARCA_ART\"]\n",
    "    ] = [\"SHIPSUPPLIER\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"TRASPORTO CON SPONDA\", [\"CODICE_ART\", \"MARCA_ART\"]\n",
    "    ] = [\"SHIPSPONDA\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"accredito per spese trasporto \",\n",
    "        [\"CODICE_ART\", \"MARCA_ART\"],\n",
    "    ] = [\"ACCSHIPERR\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"storno spese di trasporto\",\n",
    "        [\"CODICE_ART\", \"MARCA_ART\"],\n",
    "    ] = [\"ACCSHIPERR\", \"Aziendale\"]\n",
    "    df.loc[df[\"DESCRIZIONE_ART\"] == \"TRASPORTO BRT\", [\"CODICE_ART\", \"MARCA_ART\"]] = [\n",
    "        \"SHIPBRT\",\n",
    "        \"Aziendale\",\n",
    "    ]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"TRASPORTO a destinatario\", [\"CODICE_ART\", \"MARCA_ART\"]\n",
    "    ] = [\"SHIPREGULAR\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"TRASPORTO URGENTE BH\", [\"CODICE_ART\", \"MARCA_ART\"]\n",
    "    ] = [\"SHIPBH\", \"Aziendale\"]\n",
    "    df.loc[df[\"DESCRIZIONE_ART\"] == \"TRASPORTO BH\", [\"CODICE_ART\", \"MARCA_ART\"]] = [\n",
    "        \"SHIPBH\",\n",
    "        \"Aziendale\",\n",
    "    ]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"trasporto  da BH\",\n",
    "        [\"CODICE_ART\", \"MARCA_ART\", \"QUANTITA\", \"PREZZO\", \"VAL_MERCE\", \"ULTIMO_ACQ\"],\n",
    "    ] = [\"SHIPBH\", \"Aziendale\", 1, 25, 25, 20.5]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"TRASPORTO DA E PER FORNITORE\",\n",
    "        [\"CODICE_ART\", \"MARCA_ART\"],\n",
    "    ] = [\"SHIPSUPPLIER\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"trasporto da e per Fornitore\",\n",
    "        [\"CODICE_ART\", \"MARCA_ART\"],\n",
    "    ] = [\"SHIPSUPPLIER\", \"Aziendale\"]\n",
    "    df.loc[df[\"DESCRIZIONE_ART\"] == \"Trasporto delphi\", [\"CODICE_ART\", \"MARCA_ART\"]] = [\n",
    "        \"SHIPSUPPLIER\",\n",
    "        \"Aziendale\",\n",
    "    ]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"TRASPORTO AEREO DA FORNITORE\",\n",
    "        [\"CODICE_ART\", \"MARCA_ART\"],\n",
    "    ] = [\"SHIPSUPPLIERURGENT\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"TRASPORTO 50 EURO \",\n",
    "        [\"CODICE_ART\", \"MARCA_ART\", \"VAL_MERCE\", \"ULTIMO_ACQ\"],\n",
    "    ] = [\"SHIPURGENT\", \"Aziendale\", 50, 45]\n",
    "\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"Servizi Extra Trasporto Vasca racco\",\n",
    "        [\"CODICE_ART\", \"MARCA_ART\"],\n",
    "    ] = [\"SHIPREGULAR\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"TRASFERTA PRESSO VS SEDE\", [\"CODICE_ART\", \"MARCA_ART\"]\n",
    "    ] = [\"SHIPREGULAR\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"TRASP. GLS RIF DDT 14 DEL 08/05/24\",\n",
    "        [\"CODICE_ART\", \"MARCA_ART\", \"QUANTITA\", \"PREZZO\", \"VAL_MERCE\", \"ULTIMO_ACQ\"],\n",
    "    ] = [\"SHIPBH\", \"Aziendale\", 1, 37, 22, 37]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"TRASPORTO URGENTE 3,8%\", [\"CODICE_ART\", \"MARCA_ART\"]\n",
    "    ] = [\"SHIPURGENT\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"TRASPORTO URGENTE 3.8%\", [\"CODICE_ART\", \"MARCA_ART\"]\n",
    "    ] = [\"SHIPURGENT\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"SPESE TRASPORTO URGENZA \", [\"CODICE_ART\", \"MARCA_ART\"]\n",
    "    ] = [\"SHIPURGENT\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"TRASPORTO URGENZA \", [\"CODICE_ART\", \"MARCA_ART\"]\n",
    "    ] = [\"SHIPURGENT\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"SPESE TRASPORTO URGENTE\", [\"CODICE_ART\", \"MARCA_ART\"]\n",
    "    ] = [\"SHIPURGENT\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"TRASPORTO SPESE URGENTI\", [\"CODICE_ART\", \"MARCA_ART\"]\n",
    "    ] = [\"SHIPURGENT\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"SPESE TRASPORTO UREGENTE\", [\"CODICE_ART\", \"MARCA_ART\"]\n",
    "    ] = [\"SHIPURGENT\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"TRASPORTO URGENTE 2359.000\",\n",
    "        [\"CODICE_ART\", \"MARCA_ART\"],\n",
    "    ] = [\"SHIPURGENT\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"trasporto urgente\",\n",
    "        [\"CODICE_ART\", \"MARCA_ART\"],\n",
    "    ] = [\"SHIPURGENT\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"TRASPORTO URGENTE \",\n",
    "        [\"CODICE_ART\", \"MARCA_ART\"],\n",
    "    ] = [\"SHIPURGENT\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"Trasporto da Fornitore\",\n",
    "        [\"CODICE_ART\", \"MARCA_ART\"],\n",
    "    ] = [\"SHIPSUPPLIER\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"Trasporto Urgente\", [\"CODICE_ART\", \"MARCA_ART\"]\n",
    "    ] = [\"SHIPURGENT\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"TRASPORTO URGENTE polv+ centralina\",\n",
    "        [\"CODICE_ART\", \"MARCA_ART\"],\n",
    "    ] = [\"SHIPURGENT\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"TRASPORTO OMAGGIO\", [\"CODICE_ART\", \"MARCA_ART\"]\n",
    "    ] = [\"PROMOSHIP\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"TRASPORTO EXTRA URGENZA\", [\"CODICE_ART\", \"MARCA_ART\"]\n",
    "    ] = [\"SHIPURGENT\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"TRASPORTO URGENTE 4%\", [\"CODICE_ART\", \"MARCA_ART\"]\n",
    "    ] = [\"SHIPURGENT\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"TRASPORTO URGENTE\", [\"CODICE_ART\", \"MARCA_ART\"]\n",
    "    ] = [\"SHIPURGENT\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"Trasporto urgente da Bosch\",\n",
    "        [\"CODICE_ART\", \"MARCA_ART\"],\n",
    "    ] = [\"SHIPSUPPLIERURGENT\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"TRASPORTO URGENTE DA BOSCH\",\n",
    "        [\"CODICE_ART\", \"MARCA_ART\"],\n",
    "    ] = [\"SHIPSUPPLIERURGENT\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"TRASPORTO URGENTE DA FORNITORE\",\n",
    "        [\"CODICE_ART\", \"MARCA_ART\"],\n",
    "    ] = [\"SHIPSUPPLIERURGENT\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"TRASPORTO URGENTE DA ESTERO\",\n",
    "        [\"CODICE_ART\", \"MARCA_ART\"],\n",
    "    ] = [\"SHIPINTERNATIONAL\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"TRASPORTO DA FORNITORE ESTERO\",\n",
    "        [\"CODICE_ART\", \"MARCA_ART\"],\n",
    "    ] = [\"SHIPINTERNATIONAL\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"Trasporto da Fornitore Estero\",\n",
    "        [\"CODICE_ART\", \"MARCA_ART\"],\n",
    "    ] = [\"SHIPINTERNATIONAL\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"SPESE TRASPORTO UREGENTI\", [\"CODICE_ART\", \"MARCA_ART\"]\n",
    "    ] = [\"SHIPURGENT\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"SPESE TRASPORTO URGENTI\", [\"CODICE_ART\", \"MARCA_ART\"]\n",
    "    ] = [\"SHIPURGENT\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"Trasporto da e per Fornitore\",\n",
    "        [\"CODICE_ART\", \"MARCA_ART\"],\n",
    "    ] = [\"SHIPINTERNATIONAL\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"TRASPORTO URGENTE ALBERO DA ESTERO\",\n",
    "        [\"CODICE_ART\", \"MARCA_ART\"],\n",
    "    ] = [\"SHIPINTERNATIONAL\", \"Aziendale\"]\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"].str.contains(\"TRASPORTO DHL\", case=False, na=False),\n",
    "        [\"CODICE_ART\", \"MARCA_ART\"],\n",
    "    ] = [\"SHIPDHL\", \"Aziendale\"]\n",
    "\n",
    "    # More conditions\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"PROVA AL BANCO INIET DENSO\",\n",
    "        [\"CODICE_ART\", \"MARCA_ART\"],\n",
    "    ] = [\"RCSORDSCIAB20243001\", \"Aziendale\"]\n",
    "\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"].isin(\n",
    "            [\n",
    "                \"SPESE TRASORTO URGENTE\",\n",
    "                \"SPESE TRSPORTO URGENTI\",\n",
    "                \"SPESE URGENTI\",\n",
    "                \"SPESE TRSPORTO URGENTE\",\n",
    "            ]\n",
    "        ),\n",
    "        \"CODICE_ART\",\n",
    "    ] = \"SHIPURGENT\"\n",
    "\n",
    "    df.loc[df[\"DESCRIZIONE_ART\"] == \"spese\", [\"CODICE_ART\", \"MARCA_ART\"]] = [\n",
    "        \"PACKING01\",\n",
    "        \"Aziendale\",\n",
    "    ]\n",
    "\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"] == \"STORNO SPESE INCASSO\", [\"CODICE_ART\", \"MARCA_ART\"]\n",
    "    ] = [\"ACC01\", \"Aziendale\"]\n",
    "\n",
    "    # One last logic pass to check for imballo and trasporto remnants\n",
    "\n",
    "    df.loc[\n",
    "        (df[\"CODICE_ART\"] == \"trasporto\") & (df[\"DESCRIZIONE_ART\"] == \"\"),\n",
    "        [\"CODICE_ART\", \"DESCRIZIONE_ART\"],\n",
    "    ] = [\"SHIPREGULAR\", \"TRASPORTO\"]\n",
    "    df.loc[\n",
    "        (df[\"CODICE_ART\"] == \"SPESE\") & (df[\"DESCRIZIONE_ART\"] == \"IMBALLO\"),\n",
    "        [\"CODICE_ART\", \"DESCRIZIONE_ART\"],\n",
    "    ] = [\"PACKING01\", \"IMBALLO\"]\n",
    "\n",
    "    # Set ULTIMO_ACQ to 0 for specific DESCRIZIONE_ART values\n",
    "    df.loc[\n",
    "        df[\"DESCRIZIONE_ART\"].str.contains(\n",
    "            \"WD40|WD-40|WARRANTY|IMPOSTA OLIO|IMBALLO|TRASPORTO|PACKAGING|Urgenza Bosch\",\n",
    "            case=False,\n",
    "            na=False,\n",
    "        ),\n",
    "        \"ULTIMO_ACQ\",\n",
    "    ] = 0\n",
    "\n",
    "    # Assign AGENTE value of 60 for 'CLIENTE DA WEB'\n",
    "    df.loc[df[\"RAGIONE_SOCIALE\"] == \"CLIENTE DA WEB\", \"AGENTE\"] = 60\n",
    "\n",
    "    # Assign placeholder Codice Agente for any remaining missing or zero values\n",
    "    df[\"AGENTE\"] = df[\"AGENTE\"].apply(lambda x: \"100\" if pd.isna(x) or x == 0 else x)\n",
    "\n",
    "    # Assign AG 50 for foreign \"RAGIONE SOCIALE\"\n",
    "    df[\"AGENTE\"] = df.apply(\n",
    "        lambda row: \"50\"\n",
    "        if row[\"RAGIONE_SOCIALE\"] in foreign_ragione_sociale\n",
    "        else row[\"AGENTE\"],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Verify that no 'AGENTE' values are missing\n",
    "    missing_agente_after = df[\"AGENTE\"].isna().sum()\n",
    "    print(f\"Missing 'AGENTE' after filling: {missing_agente_after}\")\n",
    "\n",
    "    # Assign 'MARCA_ART' to 'Aziendale' for specific 'CODICE_ART' values\n",
    "    df.loc[\n",
    "        df[\"CODICE_ART\"].str.contains(\n",
    "            \"EXTRA|ASSICURAZIONE|PENALE|ABBONAM-TRASP-0001|OMAGGIO|SPESE|TRASPORTO|WARRANTY\",\n",
    "            case=False,\n",
    "            na=False,\n",
    "        ),\n",
    "        \"MARCA_ART\",\n",
    "    ] = \"Aziendale\"\n",
    "\n",
    "    # Separate the rows to be dropped\n",
    "    dropped_df = df[df[\"MARCA_ART\"].isin([\"NC\", \".\", \"\", \"nc\"])]\n",
    "    dropped_df_list.append(dropped_df)\n",
    "\n",
    "    # Drop entries where 'MARCA_ART' is 'NC' or '.' or empty\n",
    "    filtered_df = df[~df[\"MARCA_ART\"].isin([\"NC\", \".\", \"\", \"nc\"])]\n",
    "\n",
    "    # Group by 'Ragione Sociale Cliente' and sort each group by 'Data Documento Precedente'\n",
    "    filtered_df[\"DATA_MOV\"] = pd.to_datetime(\n",
    "        filtered_df[\"DATA_MOV\"], format=\"%d/%m/%Y\", errors=\"coerce\"\n",
    "    )\n",
    "    grouped = filtered_df.groupby([\"RAGIONE_SOCIALE\"])\n",
    "    sorted_df = grouped.apply(\n",
    "        lambda x: x.sort_values(by=\"DATA_MOV\", ascending=True)\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # Keep specific columns and rename them\n",
    "    columns_to_keep = {\n",
    "        \"DATA_MOV\": \"Data Documento Precedente\",\n",
    "        \"LISTA\": \"Numero Lista\",\n",
    "        \"MESE\": \"Mese\",\n",
    "        \"ANNO\": \"Anno\",\n",
    "        \"RAGIONE_SOCIALE\": \"Ragione Sociale Cliente\",\n",
    "        \"CLIENTE\": \"Codice Cliente\",\n",
    "        \"AGENTE\": \"Codice Agente\",\n",
    "        \"CODICE_ART\": \"Codice Articolo\",\n",
    "        \"MARCA_ART\": \"Marca Articolo\",\n",
    "        \"DESCRIZIONE_ART\": \"Descrizione Articolo\",\n",
    "        \"QUANTITA\": \"Quantita\",\n",
    "        \"VAL_MERCE\": \"Valore\",\n",
    "        \"ULTIMO_ACQ\": \"Costo\",\n",
    "        \"PREZZO\": \"Prezzo Articolo\",\n",
    "    }\n",
    "    cleaned_df = filtered_df[columns_to_keep.keys()].rename(columns=columns_to_keep)\n",
    "\n",
    "    # Update 'Costo' based on 'Prezzo Articolo'\n",
    "    cleaned_df[\"Costo\"] = cleaned_df.apply(\n",
    "        lambda row: row[\"Prezzo Articolo\"] * 0.9\n",
    "        if pd.isna(row[\"Costo\"]) or row[\"Costo\"] == 0 and row[\"Prezzo Articolo\"] > 0\n",
    "        else 0\n",
    "        if pd.isna(row[\"Costo\"]) or row[\"Costo\"] == 0 and row[\"Prezzo Articolo\"] == 0\n",
    "        else abs(row[\"Prezzo Articolo\"])\n",
    "        if pd.isna(row[\"Costo\"]) or row[\"Costo\"] == 0 and row[\"Prezzo Articolo\"] < 0\n",
    "        else row[\"Costo\"],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Append the cleaned dataframe to the list\n",
    "    df_list.append(cleaned_df)\n",
    "\n",
    "# Concatenate all the dataframes\n",
    "final_df = pd.concat(df_list, ignore_index=True)\n",
    "dropped_final_df = pd.concat(dropped_df_list, ignore_index=True)\n",
    "\n",
    "# Save the cleaned dataset to a new Excel file\n",
    "cleaned_file_path = \"Z:/My Drive/rcs/business analyst/Year2024-StatsSoFar-cleaned.xlsx\"\n",
    "final_df.to_excel(cleaned_file_path, index=False)\n",
    "\n",
    "# Save the dropped dataset to a new CSV file\n",
    "dropped_file_path = \"Z:/My Drive/rcs/business analyst/Year2024-StatsSoFar-dropped.csv\"\n",
    "dropped_final_df.to_csv(dropped_file_path, index=False)\n",
    "\n",
    "# Convert to JSON format\n",
    "json_file_path = \"Z:/My Drive/rcs/business analyst/Year2024-StatsSoFar-cleaned.json\"\n",
    "final_df.to_json(json_file_path, orient=\"records\", date_format=\"iso\")\n",
    "\n",
    "print(\"Cleaning process completed and saved to\", cleaned_file_path)\n",
    "print(\"Dropped data saved to\", dropped_file_path)\n",
    "print(\"JSON file saved to\", json_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
